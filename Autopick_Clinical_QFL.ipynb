{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanikairoshi/QFL_Experiments/blob/main/Autopick_Clinical_QFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG1N_smvDDit"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhNEMwvxDISJ",
        "outputId": "2622e9a9-c31f-4ada-c878-9907664a7d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit\n",
            "  Downloading qiskit-2.1.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting qiskit-machine-learning\n",
            "  Downloading qiskit_machine_learning-0.8.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from qiskit) (1.16.1)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit) (0.3.8)\n",
            "Collecting stevedore>=3.0.0 (from qiskit)\n",
            "  Downloading stevedore-5.5.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit) (4.15.0)\n",
            "Collecting qiskit\n",
            "  Downloading qiskit-1.4.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: setuptools>=40.1 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit) (1.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit) (2.9.0.post0)\n",
            "Collecting symengine<0.14,>=0.11 (from qiskit)\n",
            "  Downloading symengine-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit) (1.3.0)\n",
            "Downloading qiskit_machine_learning-0.8.3-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit-1.4.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: symengine, stevedore, rustworkx, qiskit, qiskit-machine-learning\n",
            "Successfully installed qiskit-1.4.4 qiskit-machine-learning-0.8.3 rustworkx-0.17.1 stevedore-5.5.0 symengine-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install qiskit qiskit-machine-learning scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit-algorithms"
      ],
      "metadata": {
        "id": "3C_xg0yIib8s",
        "outputId": "045cea38-2a59-4d58-b3c5-f71fd710cea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit-algorithms\n",
            "  Downloading qiskit_algorithms-0.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: qiskit>=1.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-algorithms) (1.4.4)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.12/dist-packages (from qiskit-algorithms) (1.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from qiskit-algorithms) (2.0.2)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (0.17.1)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (1.13.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (2.9.0.post0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (5.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (4.15.0)\n",
            "Requirement already satisfied: symengine<0.14,>=0.11 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.0->qiskit-algorithms) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit>=1.0->qiskit-algorithms) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.3->qiskit>=1.0->qiskit-algorithms) (1.3.0)\n",
            "Downloading qiskit_algorithms-0.4.0-py3-none-any.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: qiskit-algorithms\n",
            "Successfully installed qiskit-algorithms-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# qfl_qiskit_builtin.py\n",
        "# Quantum Federated Learning with in-built Qiskit ML (EstimatorQNN + NeuralNetworkClassifier).\n",
        "# Saves per-round train/test accuracy, loss, and time to CSV in Drive (or HOME fallback).\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ------------------------\n",
        "# Qiskit ML (built-ins)\n",
        "# ------------------------\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.quantum_info import SparsePauliOp, Statevector\n",
        "from qiskit.primitives import StatevectorEstimator\n",
        "\n",
        "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
        "try:\n",
        "    # Newer path\n",
        "    from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "except Exception:\n",
        "    # Older path\n",
        "    from qiskit_machine_learning.algorithms import NeuralNetworkClassifier\n",
        "\n",
        "# optional gradient (silence \"creating a gradient function\" warning if available)\n",
        "try:\n",
        "    from qiskit_algorithms.gradients import ParamShiftEstimatorGradient\n",
        "except Exception:\n",
        "    try:\n",
        "        from qiskit.algorithms.gradients import ParamShiftEstimatorGradient\n",
        "    except Exception:\n",
        "        ParamShiftEstimatorGradient = None\n",
        "\n",
        "# ------------------------\n",
        "# Optimizer (robust import)\n",
        "# ------------------------\n",
        "try:\n",
        "    from qiskit_algorithms.optimizers import COBYLA\n",
        "except Exception:\n",
        "    try:\n",
        "        from qiskit.algorithms.optimizers import COBYLA\n",
        "    except Exception:\n",
        "        COBYLA = None\n",
        "try:\n",
        "    from qiskit_algorithms.optimizers import SPSA\n",
        "except Exception:\n",
        "    try:\n",
        "        from qiskit.algorithms.optimizers import SPSA\n",
        "    except Exception:\n",
        "        SPSA = None\n",
        "\n",
        "# ------------------------\n",
        "# Sklearn\n",
        "# ------------------------\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Drive helper\n",
        "# ============================================================\n",
        "def get_outdir_in_drive(subdir: str) -> Path:\n",
        "    \"\"\"\n",
        "    If running in Colab, mount Drive and save under /content/drive/MyDrive/<subdir>.\n",
        "    Otherwise, fallback to ~/ (HOME)/<subdir>.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        base = Path(\"/content/drive/MyDrive\")\n",
        "    except Exception:\n",
        "        base = Path.home()\n",
        "    out = base / subdir\n",
        "    out.mkdir(parents=True, exist_ok=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Data loading\n",
        "# ============================================================\n",
        "def load_breast_cancer(pca_k: Optional[int] = 4, test_size: float = 0.30, seed: int = 42\n",
        "                       ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    data = datasets.load_breast_cancer()\n",
        "    X, y = data.data.astype(float), data.target.astype(int)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(X_train)\n",
        "    X_test = sc.transform(X_test)\n",
        "    if pca_k is not None:\n",
        "        p = PCA(n_components=int(pca_k), random_state=seed)\n",
        "        X_train = p.fit_transform(X_train)\n",
        "        X_test = p.transform(X_test)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def load_clinical_csv(csv_path: str,\n",
        "                      test_size: float = 0.45,\n",
        "                      pca_k: int = 2,\n",
        "                      seed: int = 42\n",
        "                      ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Minimal, robust preprocessing for BrEaST-Lesions-USG-Clinical.csv\n",
        "    → PCA to 2D (2 qubits by default).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    selected_features = [\"Age\", \"Shape\", \"Echogenicity\",\n",
        "                         \"Posterior_features\", \"Calcifications\", \"Classification\"]\n",
        "    df = df[selected_features]\n",
        "\n",
        "    # keep clean rows\n",
        "    df = df[\n",
        "        (df[\"Age\"] != \"not available\") &\n",
        "        (~df[\"Shape\"].isin([\"not applicable\"])) &\n",
        "        (~df[\"Echogenicity\"].isin([\"not applicable\"])) &\n",
        "        (~df[\"Posterior_features\"].isin([\"not applicable\"])) &\n",
        "        (~df[\"Calcifications\"].isin([\"not applicable\", \"indefinable\"])) &\n",
        "        (df[\"Classification\"].isin([\"benign\", \"malignant\"]))\n",
        "    ].copy()\n",
        "\n",
        "    df[\"Age\"] = pd.to_numeric(df[\"Age\"])\n",
        "\n",
        "    for col in [\"Shape\", \"Echogenicity\", \"Posterior_features\", \"Calcifications\"]:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "    df[\"Label\"] = df[\"Classification\"].map({\"benign\": 0, \"malignant\": 1})\n",
        "    df.drop(columns=[\"Classification\"], inplace=True)\n",
        "\n",
        "    X = df.drop(columns=[\"Label\"]).values.astype(float)\n",
        "    y = df[\"Label\"].values.astype(int)\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    X = sc.fit_transform(X)\n",
        "\n",
        "    p = PCA(n_components=int(pca_k), random_state=seed)\n",
        "    X = p.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Partition helpers (IID + simple label skew)\n",
        "# ============================================================\n",
        "def shard_iid(X: np.ndarray, y: np.ndarray, num_clients: int, seed: int = 7):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.permutation(len(y))\n",
        "    chunks = np.array_split(idx, num_clients)\n",
        "    return [(X[c], y[c]) for c in chunks]\n",
        "\n",
        "\n",
        "def shard_label_skew(X: np.ndarray, y: np.ndarray,\n",
        "                     num_clients: int, min_per_client: int = 60, seed: int = 7):\n",
        "    \"\"\"\n",
        "    Each client gets mostly a single label (binary); draws with replacement as needed.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    X0, y0 = X[y == 0], y[y == 0]\n",
        "    X1, y1 = X[y == 1], y[y == 1]\n",
        "    shards = []\n",
        "    for cid in range(num_clients):\n",
        "        maj = 0 if (cid % 2 == 0) else 1\n",
        "        k_major = int(min_per_client * 0.8)\n",
        "        k_minor = max(1, min_per_client - k_major)\n",
        "        if maj == 0:\n",
        "            idx0 = rng.integers(0, len(X0), size=k_major)\n",
        "            idx1 = rng.integers(0, len(X1), size=k_minor)\n",
        "            Xi = np.vstack([X0[idx0], X1[idx1]])\n",
        "            yi = np.hstack([y0[idx0], y1[idx1]])\n",
        "        else:\n",
        "            idx0 = rng.integers(0, len(X0), size=k_minor)\n",
        "            idx1 = rng.integers(0, len(X1), size=k_major)\n",
        "            Xi = np.vstack([X0[idx0], X1[idx1]])\n",
        "            yi = np.hstack([y0[idx0], y1[idx1]])\n",
        "        p = rng.permutation(len(yi))\n",
        "        shards.append((Xi[p], yi[p]))\n",
        "    return shards\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QNN builder (in-built classes only)\n",
        "# ============================================================\n",
        "def build_estimator_qnn(num_features: int,\n",
        "                        fm_reps: int = 1,\n",
        "                        an_reps: int = 2,\n",
        "                        x_scale: float = np.pi):\n",
        "    \"\"\"\n",
        "    EstimatorQNN with ZZFeatureMap + RealAmplitudes and observable Z on qubit 0.\n",
        "    x_scale multiplies classical inputs before feeding to the feature map.\n",
        "    \"\"\"\n",
        "    fm = ZZFeatureMap(feature_dimension=num_features, reps=fm_reps)\n",
        "    an = RealAmplitudes(num_qubits=num_features, reps=an_reps, entanglement=\"linear\")\n",
        "    obs = SparsePauliOp.from_list([(\"Z\" + \"I\" * (num_features - 1), 1.0)])\n",
        "    est = StatevectorEstimator()\n",
        "    grad = None\n",
        "    if ParamShiftEstimatorGradient is not None:\n",
        "        try:\n",
        "            grad = ParamShiftEstimatorGradient(est)\n",
        "        except Exception:\n",
        "            grad = None\n",
        "\n",
        "    qnn = EstimatorQNN(\n",
        "        circuit=fm.compose(an),\n",
        "        observables=obs,\n",
        "        input_params=sorted(list(fm.parameters), key=lambda p: p.name),\n",
        "        weight_params=list(an.parameters),\n",
        "        estimator=est,\n",
        "        gradient=grad,  # None is OK; Qiskit will auto-create a gradient if needed\n",
        "    )\n",
        "    return qnn, x_scale\n",
        "\n",
        "\n",
        "def make_classifier(num_features: int,\n",
        "                    initial_point: Optional[np.ndarray],\n",
        "                    maxiter: int = 60,\n",
        "                    optimizer: str = \"SPSA\",\n",
        "                    opt_kwargs: Optional[dict] = None):\n",
        "    qnn, _ = build_estimator_qnn(num_features)\n",
        "    opt_kwargs = {} if opt_kwargs is None else dict(opt_kwargs)\n",
        "\n",
        "    if optimizer.upper() == \"SPSA\":\n",
        "        if SPSA is None:\n",
        "            raise RuntimeError(\"SPSA not available; install/update qiskit-algorithms.\")\n",
        "        # Prefer to pass only maxiter (always supported); try extras if available\n",
        "        try:\n",
        "            opt = SPSA(maxiter=maxiter, **opt_kwargs)\n",
        "        except TypeError:\n",
        "            opt = SPSA(maxiter=maxiter)  # fallback if your version lacks those kwargs\n",
        "    elif optimizer.upper() == \"COBYLA\":\n",
        "        if COBYLA is None:\n",
        "            raise RuntimeError(\"COBYLA not available; install/update qiskit-algorithms.\")\n",
        "        opt = COBYLA(maxiter=maxiter, **opt_kwargs) if opt_kwargs else COBYLA(maxiter=maxiter)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer '{optimizer}'. Use 'SPSA' or 'COBYLA'.\")\n",
        "\n",
        "    clf = NeuralNetworkClassifier(\n",
        "        neural_network=qnn,\n",
        "        optimizer=opt,\n",
        "        initial_point=initial_point,\n",
        "        loss=\"cross_entropy\",\n",
        "        one_hot=False,  # pass 1-D labels\n",
        "    )\n",
        "    return clf\n",
        "\n",
        "\n",
        "def extract_weights(clf: NeuralNetworkClassifier) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Robustly grab the trained weights from NeuralNetworkClassifier across versions.\n",
        "    \"\"\"\n",
        "    # Common containers\n",
        "    for attr in (\"fit_result_\", \"fit_result\"):\n",
        "        if hasattr(clf, attr):\n",
        "            res = getattr(clf, attr)\n",
        "            if hasattr(res, \"x\"):\n",
        "                return np.asarray(res.x, float).copy()\n",
        "            if isinstance(res, dict) and \"x\" in res:\n",
        "                return np.asarray(res[\"x\"], float).copy()\n",
        "    # Some versions expose weights_ directly\n",
        "    if hasattr(clf, \"weights_\"):\n",
        "        w = getattr(clf, \"weights_\")\n",
        "        return np.asarray(w, float).copy()\n",
        "    raise RuntimeError(\"Could not extract weights from classifier; check qiskit-machine-learning version.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Aggregation helpers\n",
        "# ============================================================\n",
        "def _wrap_to_pi(theta: np.ndarray) -> np.ndarray:\n",
        "    return (theta + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "def agg_fedavg(local_ws: List[np.ndarray], shard_sizes: List[int]) -> np.ndarray:\n",
        "    return np.mean(np.stack(local_ws, axis=0), axis=0)\n",
        "\n",
        "def agg_fedavg_weighted(local_ws: List[np.ndarray], shard_sizes: List[int]) -> np.ndarray:\n",
        "    W = np.asarray(shard_sizes, float)\n",
        "    W = W / W.sum()\n",
        "    Wstack = np.stack(local_ws, axis=0)  # (K, D)\n",
        "    return (W[:, None] * Wstack).sum(axis=0)\n",
        "\n",
        "def agg_circular_weighted(local_ws: List[np.ndarray], shard_sizes: List[int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Angle-aware weighted mean (good for rotation parameters).\n",
        "    \"\"\"\n",
        "    W = np.asarray(shard_sizes, float); W = W / W.sum()\n",
        "    Theta = np.stack([_wrap_to_pi(w) for w in local_ws], axis=0)  # (K, D)\n",
        "    S = np.sum((W[:, None]) * np.sin(Theta), axis=0)\n",
        "    C = np.sum((W[:, None]) * np.cos(Theta), axis=0)\n",
        "    return np.arctan2(S, C)\n",
        "\n",
        "AGG_MAP = {\n",
        "    \"fedavg\": agg_fedavg,\n",
        "    \"fedavg_weighted\": agg_fedavg_weighted,\n",
        "    \"circular_weighted\": agg_circular_weighted,\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Evaluation with QNN directly (no classifier needed)\n",
        "# ============================================================\n",
        "def qnn_predict_proba(qnn: EstimatorQNN, X: np.ndarray, w: np.ndarray, x_scale: float = np.pi) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert EstimatorQNN expectation <Z0> to class probabilities.\n",
        "    \"\"\"\n",
        "    Xs = np.asarray(X, float) * x_scale\n",
        "    exp = qnn.forward(Xs, w)  # shape (N,)\n",
        "    exp = np.asarray(exp, float).reshape(-1)\n",
        "    p1 = (1.0 - exp) / 2.0\n",
        "    probs = np.vstack([1.0 - p1, p1]).T\n",
        "    return probs\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Federated loop (built-ins only)\n",
        "# ============================================================\n",
        "@dataclass\n",
        "class FLConfig:\n",
        "    num_clients: int = 10\n",
        "    rounds: int = 100\n",
        "    seed: int = 2025\n",
        "    partition: str = \"label_skew\"   # \"iid\" or \"label_skew\"\n",
        "    agg_mode: str = \"circular_weighted\"  # \"fedavg\" | \"fedavg_weighted\" | \"circular_weighted\" | \"auto_pick\"\n",
        "    maxiter_local: int = 60          # optimizer maxiter for each client\n",
        "    x_scale: float = np.pi           # multiply classical inputs before FM\n",
        "    optimizer: str = \"COBYLA\"        # \"SPSA\" | \"COBYLA\"\n",
        "    opt_kwargs: dict = None          # optional hyperparameters\n",
        "\n",
        "# ---------- Angle diagnostics helpers ----------\n",
        "\n",
        "def _unwrap_to_ref(A: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Unwrap angles client-wise to be closest to the first client's angles (reference).\n",
        "    A: shape (K, D) with values in (-pi, pi].\n",
        "    Returns an unwrapped copy in R: each row i adjusted by +/- 2π so A[i]-A[0] ∈ (-π, π].\n",
        "    \"\"\"\n",
        "    K, D = A.shape\n",
        "    out = A.copy()\n",
        "    ref = out[0]\n",
        "    for i in range(1, K):\n",
        "        delta = out[i] - ref\n",
        "        out[i] = ref + ((delta + np.pi) % (2*np.pi) - np.pi)\n",
        "    return out\n",
        "\n",
        "def _min_covering_arc_length(angles_1d: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Minimal arc length (in [0, 2π]) that covers all given angles on the circle.\n",
        "    = 2π - max_gap, where max_gap is the largest gap between consecutive sorted angles (including wrap gap).\n",
        "    \"\"\"\n",
        "    a = np.sort(angles_1d)\n",
        "    gaps = np.diff(a, append=a[0] + 2*np.pi)\n",
        "    max_gap = np.max(gaps)\n",
        "    return float(2*np.pi - max_gap)\n",
        "\n",
        "def _geodesic_sse(angles: np.ndarray, center: np.ndarray, W: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Weighted sum of squared geodesic distances on the circle, computed coordinate-wise then summed.\n",
        "    angles: (K, D) in (-π, π]; center: (D,) in (-π, π]; W: (K,) nonnegative, sum to 1.\n",
        "    \"\"\"\n",
        "    # geodesic difference per client i, coord j is wrap(angles[i,j] - center[j])\n",
        "    diff = _wrap_to_pi(angles - center)              # (K, D)\n",
        "    sse_per_client = np.sum(diff**2, axis=1)         # (K,)\n",
        "    return float(np.sum(W * sse_per_client))\n",
        "\n",
        "def angle_diagnostics(local_ws: List[np.ndarray], shard_sizes: List[int]) -> dict:\n",
        "    \"\"\"\n",
        "    Compute circular statistics and linear-vs-circular intrinsic error gap for this round.\n",
        "    Returns a dict of scalar diagnostics.\n",
        "    \"\"\"\n",
        "    A = np.stack(local_ws, axis=0)                   # (K, D), angles in (-π, π] if callers keep them wrapped\n",
        "    K, D = A.shape\n",
        "\n",
        "    # weights\n",
        "    W = np.asarray(shard_sizes, float)\n",
        "    W = W / W.sum()\n",
        "\n",
        "    # Circular resultant per coordinate\n",
        "    C = np.sum(W[:, None] * np.cos(A), axis=0)       # (D,)\n",
        "    S = np.sum(W[:, None] * np.sin(A), axis=0)       # (D,)\n",
        "    R = np.sqrt(C**2 + S**2)                         # (D,)\n",
        "    R_mean = float(np.mean(R))\n",
        "    R_min  = float(np.min(R))\n",
        "\n",
        "    # Circular (weighted) mean vector\n",
        "    mu_circ = np.arctan2(S, C)                       # (D,)\n",
        "\n",
        "    # \"Linear\" mean via unwrap relative to reference then re-wrap\n",
        "    A_unwrap = _unwrap_to_ref(A)                     # (K, D) real line\n",
        "    mu_lin_unwrapped = np.sum(W[:, None] * A_unwrap, axis=0)  # (D,)\n",
        "    mu_lin = _wrap_to_pi(mu_lin_unwrapped)\n",
        "\n",
        "    # Intrinsic (geodesic) SSE for each center\n",
        "    sse_circ = _geodesic_sse(A, mu_circ, W)\n",
        "    sse_lin  = _geodesic_sse(A, mu_lin,  W)\n",
        "    sse_gap  = float(sse_lin - sse_circ)             # >0 ⇒ circular has lower true torus error\n",
        "\n",
        "    # Semicircle coverage / boundary straddle fraction by coordinate\n",
        "    cover_lengths = np.array([_min_covering_arc_length(A[:, j]) for j in range(D)])  # (D,)\n",
        "    semicircle_ok = cover_lengths <= np.pi\n",
        "    straddle_frac = float(1.0 - np.mean(semicircle_ok))  # fraction NOT contained in any semicircle\n",
        "\n",
        "    return {\n",
        "        \"R_mean\": R_mean,\n",
        "        \"R_min\": R_min,\n",
        "        \"straddle_frac\": straddle_frac,\n",
        "        \"sse_geo_gap\": sse_gap,      # global intrinsic SSE(linear) - SSE(circular)\n",
        "        \"mu_circ\": mu_circ,          # vectors, if you want to inspect\n",
        "        \"mu_lin\":  mu_lin,\n",
        "    }\n",
        "\n",
        "def run_federated_qnn_builtins(X_train, y_train, X_test, y_test, cfg: FLConfig\n",
        "                               ) -> Dict[str, Any]:\n",
        "    rng = np.random.default_rng(cfg.seed)\n",
        "\n",
        "    # accept auto_pick as special mode\n",
        "    if cfg.agg_mode != \"auto_pick\" and cfg.agg_mode not in AGG_MAP:\n",
        "        raise ValueError(\n",
        "            f\"Unknown agg_mode={cfg.agg_mode}. Choose from {list(AGG_MAP.keys())} + ['auto_pick']\"\n",
        "        )\n",
        "    aggregator = AGG_MAP.get(cfg.agg_mode, None)  # None when 'auto_pick'\n",
        "\n",
        "    # client shards\n",
        "    if cfg.partition == \"iid\":\n",
        "        shards = shard_iid(X_train, y_train, cfg.num_clients, seed=cfg.seed)\n",
        "    else:\n",
        "        shards = shard_label_skew(\n",
        "            X_train, y_train, cfg.num_clients,\n",
        "            min_per_client=max(40, len(X_train)//cfg.num_clients),\n",
        "            seed=cfg.seed\n",
        "        )\n",
        "\n",
        "    num_features = X_train.shape[1]\n",
        "\n",
        "    # reference QNN for evaluation\n",
        "    qnn_ref, _ = build_estimator_qnn(num_features, fm_reps=1, an_reps=2, x_scale=cfg.x_scale)\n",
        "    D = len(qnn_ref.weight_params)\n",
        "\n",
        "    # initialize global weights\n",
        "    w_global = 0.10 * rng.standard_normal(D)\n",
        "\n",
        "    # histories\n",
        "    rounds_csv_rows: List[Dict[str, Any]] = []\n",
        "    client_csv_rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    print(f\"=== QFL using '{cfg.agg_mode}' | partition='{cfg.partition}' | clients={cfg.num_clients} rounds={cfg.rounds} ===\")\n",
        "    for r in range(1, cfg.rounds + 1):\n",
        "        t0 = time.perf_counter()\n",
        "\n",
        "        local_ws, shard_sizes = [], []\n",
        "\n",
        "        # iterate clients\n",
        "        for cid, (Xi, yi) in enumerate(shards):\n",
        "            clf = make_classifier(\n",
        "                num_features,\n",
        "                initial_point=w_global,\n",
        "                maxiter=cfg.maxiter_local,\n",
        "                optimizer=cfg.optimizer,\n",
        "                opt_kwargs=cfg.opt_kwargs,\n",
        "            )\n",
        "\n",
        "            # IMPORTANT: scale inputs for training consistently with evaluation\n",
        "            t_c0 = time.perf_counter()\n",
        "            clf.fit(Xi * cfg.x_scale, yi)   # one_hot=False in make_classifier, so 1-D labels are correct\n",
        "            dt_c = time.perf_counter() - t_c0\n",
        "\n",
        "            w_local = extract_weights(clf)\n",
        "            if len(w_local) != D:\n",
        "                raise RuntimeError(f\"[Round {r}] Client {cid}: weight dim mismatch ({len(w_local)} != {D})\")\n",
        "            local_ws.append(w_local)\n",
        "            shard_sizes.append(len(yi))\n",
        "\n",
        "            # --- client eval (optional, for analysis) ---\n",
        "            probs_tr_loc = qnn_predict_proba(qnn_ref, Xi,      w_local, x_scale=cfg.x_scale)\n",
        "            probs_te_loc = qnn_predict_proba(qnn_ref, X_test,  w_local, x_scale=cfg.x_scale)\n",
        "            yhat_tr_loc = np.argmax(probs_tr_loc, axis=1)\n",
        "            yhat_te_loc = np.argmax(probs_te_loc, axis=1)\n",
        "\n",
        "            client_csv_rows.append({\n",
        "                \"round\": r,\n",
        "                \"client_id\": cid,\n",
        "                \"shard_size\": int(len(yi)),\n",
        "                \"train_acc_local\": float(accuracy_score(yi, yhat_tr_loc)),\n",
        "                \"test_acc_local\":  float(accuracy_score(y_test, yhat_te_loc)),\n",
        "                \"train_loss_local\": float(log_loss(yi,     probs_tr_loc, labels=[0,1])),\n",
        "                \"test_loss_local\":  float(log_loss(y_test, probs_te_loc, labels=[0,1])),\n",
        "                \"time_sec_local\":   float(dt_c),\n",
        "            })\n",
        "\n",
        "\n",
        "                # ==== DIAGNOSTICS (compute before aggregation) ====\n",
        "        diag = angle_diagnostics(local_ws, shard_sizes)\n",
        "        # Pretty console print\n",
        "        print(f\"    [diag] R̄={diag['R_mean']:.3f} | Rmin={diag['R_min']:.3f} | \"\n",
        "              f\"straddle_frac={diag['straddle_frac']:.2f} | ΔSSE_geo={diag['sse_geo_gap']:.4f}\")\n",
        "\n",
        "\n",
        "        # -------- aggregate to new global --------\n",
        "        test_loss_lin = None\n",
        "        test_loss_circ = None\n",
        "        picked = None\n",
        "\n",
        "        if cfg.agg_mode == \"auto_pick\":\n",
        "            # try both\n",
        "            w_lin  = agg_fedavg_weighted(local_ws, shard_sizes)\n",
        "            w_circ = agg_circular_weighted(local_ws, shard_sizes)\n",
        "\n",
        "            ## evaluate both on X_test (or use a separate val set)\n",
        "            probs_lin  = qnn_predict_proba(qnn_ref, X_test, w_lin,  x_scale=cfg.x_scale)\n",
        "            probs_circ = qnn_predict_proba(qnn_ref, X_test, w_circ, x_scale=cfg.x_scale)\n",
        "            test_loss_lin  = log_loss(y_test, probs_lin,  labels=[0, 1])\n",
        "            test_loss_circ = log_loss(y_test, probs_circ, labels=[0, 1])\n",
        "\n",
        "            # pick lower-loss aggregate\n",
        "            use_circ = test_loss_circ <= test_loss_lin\n",
        "            w_global = w_circ if use_circ else w_lin\n",
        "            picked = \"circular\" if use_circ else \"linear\"\n",
        "            # print the decision\n",
        "            print(f\"    ↳ auto_pick chose: {picked} \"\n",
        "              f\"(loss_lin={test_loss_lin:.4f}, loss_circ={test_loss_circ:.4f})\")\n",
        "        else:\n",
        "            w_global = aggregator(local_ws, shard_sizes)\n",
        "            # print which fixed aggregator we used\n",
        "            print(f\"    ↳ agg used: {cfg.agg_mode}\")\n",
        "\n",
        "        # -------- global evaluation --------\n",
        "        probs_tr = qnn_predict_proba(qnn_ref, X_train, w_global, x_scale=cfg.x_scale)\n",
        "        probs_te = qnn_predict_proba(qnn_ref, X_test,  w_global, x_scale=cfg.x_scale)\n",
        "        ytr_pred = np.argmax(probs_tr, axis=1)\n",
        "        yte_pred = np.argmax(probs_te, axis=1)\n",
        "\n",
        "        train_acc = accuracy_score(y_train, ytr_pred)\n",
        "        test_acc  = accuracy_score(y_test,  yte_pred)\n",
        "        train_loss = log_loss(y_train, probs_tr, labels=[0,1])\n",
        "        test_loss  = log_loss(y_test,  probs_te, labels=[0,1])\n",
        "\n",
        "        dt = time.perf_counter() - t0\n",
        "        # just before the big print\n",
        "        if cfg.agg_mode == \"auto_pick\":\n",
        "          suffix = f\" | picked={picked}\"\n",
        "        else:\n",
        "          suffix = f\" | agg={cfg.agg_mode}\"\n",
        "\n",
        "        print(f\"[Round {r:02d}] TrainAcc={train_acc:.3f} | TestAcc={test_acc:.3f} | \"\n",
        "            f\"TrainLoss={train_loss:.4f} | TestLoss={test_loss:.4f} | Time={dt:.2f}s{suffix}\")\n",
        "\n",
        "\n",
        "        row = {\n",
        "            \"round\": r,\n",
        "            \"train_acc\": float(train_acc),\n",
        "            \"test_acc\":  float(test_acc),\n",
        "            \"train_loss\": float(train_loss),\n",
        "            \"test_loss\":  float(test_loss),\n",
        "            \"time_sec\": float(dt),\n",
        "\n",
        "            # diagnostics\n",
        "            \"resultant_mean\": float(diag[\"R_mean\"]),\n",
        "            \"resultant_min\":  float(diag[\"R_min\"]),\n",
        "            \"straddle_frac\":  float(diag[\"straddle_frac\"]),\n",
        "            \"delta_SSE_geo\":  float(diag[\"sse_geo_gap\"]),\n",
        "        }\n",
        "        if cfg.agg_mode == \"auto_pick\":\n",
        "            row.update({\n",
        "                \"test_loss_lin\":  None if test_loss_lin  is None else float(test_loss_lin),\n",
        "                \"test_loss_circ\": None if test_loss_circ is None else float(test_loss_circ),\n",
        "                \"picked\": picked,\n",
        "            })\n",
        "            print(f\"    [pick] chose={picked}  (loss_lin={test_loss_lin:.4f}, loss_circ={test_loss_circ:.4f})\")\n",
        "        else:\n",
        "            row[\"picked\"] = cfg.agg_mode\n",
        "        rounds_csv_rows.append(row)\n",
        "\n",
        "    return {\n",
        "        \"w_global\": w_global,\n",
        "        \"rows\": rounds_csv_rows,\n",
        "        \"client_rows\": client_csv_rows,\n",
        "        \"config\": cfg.__dict__,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Save CSV + metadata\n",
        "# ============================================================\n",
        "def save_rounds_csv(rows: List[Dict[str, Any]], path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df = pd.DataFrame(rows)  # accept any extra columns\n",
        "    base = [\"round\", \"train_acc\", \"test_acc\", \"train_loss\", \"test_loss\", \"time_sec\"]\n",
        "    extras = [c for c in df.columns if c not in base]\n",
        "    df = df[base + extras]\n",
        "    df.to_csv(path, index=False)\n",
        "\n",
        "def save_meta(meta: Dict[str, Any], path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "def save_client_rounds_csv(rows: List[Dict[str, Any]], path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    cols = [\n",
        "        \"round\",\"client_id\",\"shard_size\",\n",
        "        \"train_acc_local\",\"test_acc_local\",\n",
        "        \"train_loss_local\",\"test_loss_local\",\n",
        "        \"time_sec_local\"\n",
        "    ]\n",
        "    df = pd.DataFrame(rows, columns=cols)\n",
        "    df.to_csv(path, index=False)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main (run both datasets)\n",
        "# ============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # ---------------- CONFIG ----------------\n",
        "    # Choose aggregator: \"fedavg\", \"fedavg_weighted\", \"circular_weighted\", or \"auto_pick\"\n",
        "    AGGREGATOR = \"auto_pick\"   # compare circular vs weighted-fedavg each round and pick best\n",
        "    NUM_CLIENTS = 10\n",
        "    ROUNDS = 50\n",
        "    MAXITER_LOCAL = 60\n",
        "    X_SCALE = np.pi\n",
        "\n",
        "    # Clinical CSV path (edit if needed)\n",
        "    CLINICAL_CSV = \"/content/drive/MyDrive/data/BrEaST-Lesions-USG-Clinical.csv\"\n",
        "\n",
        "    # Drive output folders\n",
        "    OUTDIR_BC = get_outdir_in_drive(f\"QFL_folder/ZZ_breast_cancer_{AGGREGATOR}\")\n",
        "    OUTDIR_CL = get_outdir_in_drive(f\"QFL_folder/ZZ_clinical_{AGGREGATOR}\")\n",
        "\n",
        "    # ------------- Breast Cancer (sklearn) -------------\n",
        "    Xtr, Xte, ytr, yte = load_breast_cancer(pca_k=5, test_size=0.30, seed=42)\n",
        "    cfg_bc = FLConfig(num_clients=NUM_CLIENTS, rounds=ROUNDS, seed=2025,\n",
        "                      partition=\"label_skew\", agg_mode=AGGREGATOR,\n",
        "                      maxiter_local=MAXITER_LOCAL, x_scale=X_SCALE,\n",
        "                      optimizer=\"SPSA\", opt_kwargs={\"learning_rate\": 0.05, \"perturbation\": 0.1})\n",
        "    print(\"\\n=== Breast Cancer (sklearn) ===\")\n",
        "    out_bc = run_federated_qnn_builtins(Xtr, ytr, Xte, yte, cfg_bc)\n",
        "    csv_bc = OUTDIR_BC / f\"round_metrics_bc_clients{NUM_CLIENTS}_rounds{ROUNDS}.csv\"\n",
        "    save_rounds_csv(out_bc[\"rows\"], csv_bc)\n",
        "\n",
        "    # (optional) per-client CSV\n",
        "    csv_bc_clients = OUTDIR_BC / f\"round_client_metrics_bc_clients{NUM_CLIENTS}_rounds{ROUNDS}.csv\"\n",
        "    save_client_rounds_csv(out_bc[\"client_rows\"], csv_bc_clients)\n",
        "\n",
        "    save_meta({\"dataset\": \"sklearn_breast_cancer\", **out_bc[\"config\"]}, OUTDIR_BC / \"run_metadata.json\")\n",
        "    np.save(OUTDIR_BC / \"w_global.npy\", out_bc[\"w_global\"])\n",
        "    print(f\"Saved breast-cancer CSV to: {csv_bc}\")\n",
        "\n",
        "    # ------------- Clinical CSV -------------\n",
        "    try:\n",
        "        Xtr, Xte, ytr, yte = load_clinical_csv(CLINICAL_CSV, test_size=0.45, pca_k=5, seed=42)\n",
        "        cfg_cl = FLConfig(num_clients=NUM_CLIENTS, rounds=ROUNDS, seed=2025,\n",
        "                          partition=\"label_skew\", agg_mode=AGGREGATOR,\n",
        "                          maxiter_local=MAXITER_LOCAL, x_scale=X_SCALE,\n",
        "                          optimizer=\"COBYLA\")  # COBYLA is often stable for small circuits\n",
        "        print(\"\\n=== Clinical CSV (PCA→5D) ===\")\n",
        "        out_cl = run_federated_qnn_builtins(Xtr, ytr, Xte, yte, cfg_cl)\n",
        "        csv_cl = OUTDIR_CL / f\"round_metrics_clinical_clients{NUM_CLIENTS}_rounds{ROUNDS}.csv\"\n",
        "        save_rounds_csv(out_cl[\"rows\"], csv_cl)\n",
        "\n",
        "        csv_cl_clients = OUTDIR_CL / f\"round_client_metrics_clinical_clients{NUM_CLIENTS}_rounds{ROUNDS}.csv\"\n",
        "        save_client_rounds_csv(out_cl[\"client_rows\"], csv_cl_clients)\n",
        "\n",
        "        save_meta({\"dataset\": \"clinical_ultrasound_csv\", \"csv_path\": CLINICAL_CSV, **out_cl[\"config\"]},\n",
        "                  OUTDIR_CL / \"run_metadata.json\")\n",
        "        np.save(OUTDIR_CL / \"w_global.npy\", out_cl[\"w_global\"])\n",
        "        print(f\"Saved clinical CSV to: {csv_cl}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"[WARN] Clinical CSV not found at: {CLINICAL_CSV}. Skipping that run.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa7Q1R30DGi1",
        "outputId": "5d3914b3-3524-4fec-d60a-d085b06cd118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "=== Breast Cancer (sklearn) ===\n",
            "=== QFL using 'auto_pick' | partition='label_skew' | clients=10 rounds=50 ===\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7308, loss_circ=0.7301)\n",
            "[Round 01] TrainAcc=0.497 | TestAcc=0.456 | TrainLoss=0.7145 | TestLoss=0.7278 | Time=252.19s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7167, loss_circ=0.7254)\n",
            "[Round 02] TrainAcc=0.487 | TestAcc=0.462 | TrainLoss=0.6998 | TestLoss=0.7192 | Time=247.59s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6994, loss_circ=0.7053)\n",
            "[Round 03] TrainAcc=0.500 | TestAcc=0.579 | TrainLoss=0.7147 | TestLoss=0.6988 | Time=244.61s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7059, loss_circ=0.7089)\n",
            "[Round 04] TrainAcc=0.528 | TestAcc=0.526 | TrainLoss=0.6971 | TestLoss=0.7047 | Time=246.00s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7035, loss_circ=0.7037)\n",
            "[Round 05] TrainAcc=0.472 | TestAcc=0.468 | TrainLoss=0.7074 | TestLoss=0.7048 | Time=245.64s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6990, loss_circ=0.7164)\n",
            "[Round 06] TrainAcc=0.495 | TestAcc=0.480 | TrainLoss=0.7048 | TestLoss=0.7003 | Time=244.62s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7096, loss_circ=0.6901)\n",
            "[Round 07] TrainAcc=0.485 | TestAcc=0.515 | TrainLoss=0.7136 | TestLoss=0.6899 | Time=246.94s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7134, loss_circ=0.7074)\n",
            "[Round 08] TrainAcc=0.520 | TestAcc=0.480 | TrainLoss=0.6966 | TestLoss=0.7062 | Time=243.85s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7132, loss_circ=0.7109)\n",
            "[Round 09] TrainAcc=0.497 | TestAcc=0.497 | TrainLoss=0.7050 | TestLoss=0.7135 | Time=243.02s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7199, loss_circ=0.7101)\n",
            "[Round 10] TrainAcc=0.480 | TestAcc=0.485 | TrainLoss=0.7161 | TestLoss=0.7125 | Time=245.24s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6884, loss_circ=0.7035)\n",
            "[Round 11] TrainAcc=0.492 | TestAcc=0.532 | TrainLoss=0.7101 | TestLoss=0.6936 | Time=245.98s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6948, loss_circ=0.7070)\n",
            "[Round 12] TrainAcc=0.467 | TestAcc=0.526 | TrainLoss=0.6981 | TestLoss=0.6927 | Time=244.33s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6849, loss_circ=0.6984)\n",
            "[Round 13] TrainAcc=0.465 | TestAcc=0.532 | TrainLoss=0.7110 | TestLoss=0.6878 | Time=245.65s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.6970, loss_circ=0.6938)\n",
            "[Round 14] TrainAcc=0.475 | TestAcc=0.532 | TrainLoss=0.7057 | TestLoss=0.6936 | Time=247.50s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6897, loss_circ=0.7197)\n",
            "[Round 15] TrainAcc=0.457 | TestAcc=0.550 | TrainLoss=0.7136 | TestLoss=0.6883 | Time=245.61s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6999, loss_circ=0.7042)\n",
            "[Round 16] TrainAcc=0.508 | TestAcc=0.480 | TrainLoss=0.7017 | TestLoss=0.6986 | Time=247.34s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6917, loss_circ=0.6941)\n",
            "[Round 17] TrainAcc=0.523 | TestAcc=0.491 | TrainLoss=0.6886 | TestLoss=0.6932 | Time=249.56s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6825, loss_circ=0.6984)\n",
            "[Round 18] TrainAcc=0.477 | TestAcc=0.532 | TrainLoss=0.7014 | TestLoss=0.6861 | Time=251.52s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6742, loss_circ=0.6856)\n",
            "[Round 19] TrainAcc=0.477 | TestAcc=0.591 | TrainLoss=0.7112 | TestLoss=0.6757 | Time=260.77s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6976, loss_circ=0.7131)\n",
            "[Round 20] TrainAcc=0.485 | TestAcc=0.509 | TrainLoss=0.7067 | TestLoss=0.6990 | Time=255.50s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7206, loss_circ=0.7078)\n",
            "[Round 21] TrainAcc=0.500 | TestAcc=0.468 | TrainLoss=0.7087 | TestLoss=0.7132 | Time=257.05s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7329, loss_circ=0.7084)\n",
            "[Round 22] TrainAcc=0.510 | TestAcc=0.474 | TrainLoss=0.7033 | TestLoss=0.7095 | Time=267.20s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7155, loss_circ=0.7258)\n",
            "[Round 23] TrainAcc=0.510 | TestAcc=0.456 | TrainLoss=0.7031 | TestLoss=0.7172 | Time=250.16s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6830, loss_circ=0.7311)\n",
            "[Round 24] TrainAcc=0.525 | TestAcc=0.567 | TrainLoss=0.7022 | TestLoss=0.6839 | Time=246.09s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7024, loss_circ=0.7024)\n",
            "[Round 25] TrainAcc=0.467 | TestAcc=0.520 | TrainLoss=0.7182 | TestLoss=0.7008 | Time=250.09s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6905, loss_circ=0.7096)\n",
            "[Round 26] TrainAcc=0.503 | TestAcc=0.579 | TrainLoss=0.7165 | TestLoss=0.6918 | Time=246.15s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6879, loss_circ=0.7085)\n",
            "[Round 27] TrainAcc=0.492 | TestAcc=0.573 | TrainLoss=0.7116 | TestLoss=0.6896 | Time=245.65s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7346, loss_circ=0.6982)\n",
            "[Round 28] TrainAcc=0.528 | TestAcc=0.526 | TrainLoss=0.6957 | TestLoss=0.6980 | Time=256.36s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7137, loss_circ=0.7124)\n",
            "[Round 29] TrainAcc=0.538 | TestAcc=0.503 | TrainLoss=0.6868 | TestLoss=0.7132 | Time=244.92s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7040, loss_circ=0.7099)\n",
            "[Round 30] TrainAcc=0.485 | TestAcc=0.491 | TrainLoss=0.7133 | TestLoss=0.7039 | Time=248.31s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7077, loss_circ=0.6965)\n",
            "[Round 31] TrainAcc=0.455 | TestAcc=0.538 | TrainLoss=0.7248 | TestLoss=0.6971 | Time=245.16s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6945, loss_circ=0.7007)\n",
            "[Round 32] TrainAcc=0.500 | TestAcc=0.485 | TrainLoss=0.6954 | TestLoss=0.6947 | Time=244.72s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.6972, loss_circ=0.6940)\n",
            "[Round 33] TrainAcc=0.492 | TestAcc=0.526 | TrainLoss=0.7001 | TestLoss=0.6918 | Time=247.25s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6913, loss_circ=0.7110)\n",
            "[Round 34] TrainAcc=0.533 | TestAcc=0.526 | TrainLoss=0.6913 | TestLoss=0.6895 | Time=248.24s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7124, loss_circ=0.7015)\n",
            "[Round 35] TrainAcc=0.535 | TestAcc=0.561 | TrainLoss=0.6972 | TestLoss=0.7000 | Time=247.65s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7001, loss_circ=0.6929)\n",
            "[Round 36] TrainAcc=0.497 | TestAcc=0.544 | TrainLoss=0.7017 | TestLoss=0.6917 | Time=244.98s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6981, loss_circ=0.7134)\n",
            "[Round 37] TrainAcc=0.482 | TestAcc=0.456 | TrainLoss=0.6952 | TestLoss=0.7014 | Time=245.03s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7055, loss_circ=0.7161)\n",
            "[Round 38] TrainAcc=0.510 | TestAcc=0.550 | TrainLoss=0.6960 | TestLoss=0.7067 | Time=246.48s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7338, loss_circ=0.7195)\n",
            "[Round 39] TrainAcc=0.525 | TestAcc=0.439 | TrainLoss=0.7125 | TestLoss=0.7199 | Time=256.68s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6838, loss_circ=0.7120)\n",
            "[Round 40] TrainAcc=0.525 | TestAcc=0.556 | TrainLoss=0.6912 | TestLoss=0.6852 | Time=244.42s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.6997, loss_circ=0.6991)\n",
            "[Round 41] TrainAcc=0.535 | TestAcc=0.538 | TrainLoss=0.6991 | TestLoss=0.6978 | Time=244.53s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7152, loss_circ=0.6906)\n",
            "[Round 42] TrainAcc=0.492 | TestAcc=0.503 | TrainLoss=0.7182 | TestLoss=0.6928 | Time=244.04s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7059, loss_circ=0.7080)\n",
            "[Round 43] TrainAcc=0.508 | TestAcc=0.485 | TrainLoss=0.7070 | TestLoss=0.7059 | Time=266.14s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6940, loss_circ=0.7239)\n",
            "[Round 44] TrainAcc=0.482 | TestAcc=0.509 | TrainLoss=0.7036 | TestLoss=0.6961 | Time=259.93s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6987, loss_circ=0.7022)\n",
            "[Round 45] TrainAcc=0.490 | TestAcc=0.515 | TrainLoss=0.7005 | TestLoss=0.6973 | Time=253.50s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7098, loss_circ=0.7266)\n",
            "[Round 46] TrainAcc=0.500 | TestAcc=0.491 | TrainLoss=0.7024 | TestLoss=0.7111 | Time=252.96s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.6992, loss_circ=0.6905)\n",
            "[Round 47] TrainAcc=0.535 | TestAcc=0.550 | TrainLoss=0.7013 | TestLoss=0.6898 | Time=255.89s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7061, loss_circ=0.6999)\n",
            "[Round 48] TrainAcc=0.503 | TestAcc=0.520 | TrainLoss=0.7123 | TestLoss=0.7005 | Time=252.01s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6885, loss_circ=0.7005)\n",
            "[Round 49] TrainAcc=0.487 | TestAcc=0.561 | TrainLoss=0.7134 | TestLoss=0.6887 | Time=253.07s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7020, loss_circ=0.6999)\n",
            "[Round 50] TrainAcc=0.503 | TestAcc=0.497 | TrainLoss=0.6991 | TestLoss=0.7000 | Time=254.53s | picked=circular\n",
            "Saved breast-cancer CSV to: /content/drive/MyDrive/QFL_folder/ZZ_breast_cancer_auto_pick/round_metrics_bc_clients10_rounds50.csv\n",
            "\n",
            "=== Clinical CSV (PCA→5D) ===\n",
            "=== QFL using 'auto_pick' | partition='label_skew' | clients=10 rounds=50 ===\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.6978, loss_circ=0.6964)\n",
            "[Round 01] TrainAcc=0.544 | TestAcc=0.484 | TrainLoss=0.7023 | TestLoss=0.6989 | Time=129.21s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.6915, loss_circ=0.6888)\n",
            "[Round 02] TrainAcc=0.544 | TestAcc=0.547 | TrainLoss=0.7016 | TestLoss=0.6896 | Time=128.21s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6824, loss_circ=0.6827)\n",
            "[Round 03] TrainAcc=0.482 | TestAcc=0.558 | TrainLoss=0.7068 | TestLoss=0.6794 | Time=126.29s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7043, loss_circ=0.7029)\n",
            "[Round 04] TrainAcc=0.412 | TestAcc=0.505 | TrainLoss=0.7221 | TestLoss=0.7064 | Time=126.27s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7144, loss_circ=0.7168)\n",
            "[Round 05] TrainAcc=0.412 | TestAcc=0.484 | TrainLoss=0.7296 | TestLoss=0.7130 | Time=126.57s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7140, loss_circ=0.7177)\n",
            "[Round 06] TrainAcc=0.412 | TestAcc=0.442 | TrainLoss=0.7446 | TestLoss=0.7174 | Time=126.47s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7044, loss_circ=0.7073)\n",
            "[Round 07] TrainAcc=0.447 | TestAcc=0.484 | TrainLoss=0.7435 | TestLoss=0.7081 | Time=127.47s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7080, loss_circ=0.7064)\n",
            "[Round 08] TrainAcc=0.430 | TestAcc=0.495 | TrainLoss=0.7287 | TestLoss=0.7061 | Time=127.59s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7202, loss_circ=0.7221)\n",
            "[Round 09] TrainAcc=0.430 | TestAcc=0.411 | TrainLoss=0.7242 | TestLoss=0.7196 | Time=128.04s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7186, loss_circ=0.7165)\n",
            "[Round 10] TrainAcc=0.430 | TestAcc=0.463 | TrainLoss=0.7219 | TestLoss=0.7141 | Time=127.29s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7137, loss_circ=0.7156)\n",
            "[Round 11] TrainAcc=0.439 | TestAcc=0.474 | TrainLoss=0.7247 | TestLoss=0.7138 | Time=126.95s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7144, loss_circ=0.7145)\n",
            "[Round 12] TrainAcc=0.404 | TestAcc=0.463 | TrainLoss=0.7287 | TestLoss=0.7140 | Time=128.42s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7187, loss_circ=0.7199)\n",
            "[Round 13] TrainAcc=0.430 | TestAcc=0.474 | TrainLoss=0.7289 | TestLoss=0.7188 | Time=128.38s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7122, loss_circ=0.7144)\n",
            "[Round 14] TrainAcc=0.430 | TestAcc=0.442 | TrainLoss=0.7282 | TestLoss=0.7160 | Time=129.82s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7094, loss_circ=0.7071)\n",
            "[Round 15] TrainAcc=0.447 | TestAcc=0.474 | TrainLoss=0.7245 | TestLoss=0.7083 | Time=129.64s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7064, loss_circ=0.7089)\n",
            "[Round 16] TrainAcc=0.474 | TestAcc=0.453 | TrainLoss=0.7127 | TestLoss=0.7079 | Time=127.60s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7138, loss_circ=0.7097)\n",
            "[Round 17] TrainAcc=0.526 | TestAcc=0.495 | TrainLoss=0.7075 | TestLoss=0.7126 | Time=129.75s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7128, loss_circ=0.7134)\n",
            "[Round 18] TrainAcc=0.439 | TestAcc=0.495 | TrainLoss=0.7188 | TestLoss=0.7111 | Time=127.73s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7082, loss_circ=0.7074)\n",
            "[Round 19] TrainAcc=0.447 | TestAcc=0.484 | TrainLoss=0.7242 | TestLoss=0.7095 | Time=126.82s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7100, loss_circ=0.7122)\n",
            "[Round 20] TrainAcc=0.465 | TestAcc=0.474 | TrainLoss=0.7323 | TestLoss=0.7104 | Time=127.01s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7027, loss_circ=0.7042)\n",
            "[Round 21] TrainAcc=0.447 | TestAcc=0.537 | TrainLoss=0.7414 | TestLoss=0.7036 | Time=127.42s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7044, loss_circ=0.7073)\n",
            "[Round 22] TrainAcc=0.447 | TestAcc=0.537 | TrainLoss=0.7485 | TestLoss=0.7021 | Time=127.22s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7046, loss_circ=0.7009)\n",
            "[Round 23] TrainAcc=0.439 | TestAcc=0.505 | TrainLoss=0.7480 | TestLoss=0.7020 | Time=128.05s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7079, loss_circ=0.7081)\n",
            "[Round 24] TrainAcc=0.439 | TestAcc=0.505 | TrainLoss=0.7436 | TestLoss=0.7046 | Time=129.04s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7137, loss_circ=0.7125)\n",
            "[Round 25] TrainAcc=0.395 | TestAcc=0.463 | TrainLoss=0.7487 | TestLoss=0.7101 | Time=128.86s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7194, loss_circ=0.7175)\n",
            "[Round 26] TrainAcc=0.395 | TestAcc=0.463 | TrainLoss=0.7493 | TestLoss=0.7199 | Time=128.01s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7165, loss_circ=0.7130)\n",
            "[Round 27] TrainAcc=0.421 | TestAcc=0.453 | TrainLoss=0.7519 | TestLoss=0.7183 | Time=127.89s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7220, loss_circ=0.7214)\n",
            "[Round 28] TrainAcc=0.421 | TestAcc=0.484 | TrainLoss=0.7592 | TestLoss=0.7162 | Time=129.37s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7213, loss_circ=0.7224)\n",
            "[Round 29] TrainAcc=0.404 | TestAcc=0.484 | TrainLoss=0.7694 | TestLoss=0.7199 | Time=127.81s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7149, loss_circ=0.7174)\n",
            "[Round 30] TrainAcc=0.412 | TestAcc=0.516 | TrainLoss=0.7716 | TestLoss=0.7128 | Time=128.48s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7214, loss_circ=0.7189)\n",
            "[Round 31] TrainAcc=0.377 | TestAcc=0.516 | TrainLoss=0.7708 | TestLoss=0.7193 | Time=127.63s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7228, loss_circ=0.7234)\n",
            "[Round 32] TrainAcc=0.351 | TestAcc=0.495 | TrainLoss=0.7764 | TestLoss=0.7234 | Time=128.61s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7232, loss_circ=0.7236)\n",
            "[Round 33] TrainAcc=0.377 | TestAcc=0.474 | TrainLoss=0.7805 | TestLoss=0.7248 | Time=130.76s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7271, loss_circ=0.7258)\n",
            "[Round 34] TrainAcc=0.368 | TestAcc=0.495 | TrainLoss=0.7749 | TestLoss=0.7284 | Time=126.66s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7368, loss_circ=0.7359)\n",
            "[Round 35] TrainAcc=0.325 | TestAcc=0.400 | TrainLoss=0.7621 | TestLoss=0.7353 | Time=126.79s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7329, loss_circ=0.7322)\n",
            "[Round 36] TrainAcc=0.333 | TestAcc=0.411 | TrainLoss=0.7611 | TestLoss=0.7310 | Time=126.62s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7336, loss_circ=0.7288)\n",
            "[Round 37] TrainAcc=0.307 | TestAcc=0.442 | TrainLoss=0.7454 | TestLoss=0.7305 | Time=127.53s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7326, loss_circ=0.7305)\n",
            "[Round 38] TrainAcc=0.412 | TestAcc=0.463 | TrainLoss=0.7408 | TestLoss=0.7356 | Time=128.18s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7221, loss_circ=0.7235)\n",
            "[Round 39] TrainAcc=0.360 | TestAcc=0.432 | TrainLoss=0.7405 | TestLoss=0.7244 | Time=127.66s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7335, loss_circ=0.7332)\n",
            "[Round 40] TrainAcc=0.395 | TestAcc=0.442 | TrainLoss=0.7484 | TestLoss=0.7311 | Time=126.79s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7200, loss_circ=0.7168)\n",
            "[Round 41] TrainAcc=0.404 | TestAcc=0.484 | TrainLoss=0.7409 | TestLoss=0.7198 | Time=127.13s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.7049, loss_circ=0.7080)\n",
            "[Round 42] TrainAcc=0.421 | TestAcc=0.505 | TrainLoss=0.7339 | TestLoss=0.7053 | Time=127.08s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7002, loss_circ=0.6982)\n",
            "[Round 43] TrainAcc=0.412 | TestAcc=0.547 | TrainLoss=0.7251 | TestLoss=0.7009 | Time=126.42s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.6845, loss_circ=0.6844)\n",
            "[Round 44] TrainAcc=0.386 | TestAcc=0.568 | TrainLoss=0.7430 | TestLoss=0.6904 | Time=127.77s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.6934, loss_circ=0.6883)\n",
            "[Round 45] TrainAcc=0.439 | TestAcc=0.547 | TrainLoss=0.7354 | TestLoss=0.6877 | Time=127.73s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6861, loss_circ=0.6885)\n",
            "[Round 46] TrainAcc=0.430 | TestAcc=0.505 | TrainLoss=0.7327 | TestLoss=0.6859 | Time=127.69s | picked=linear\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.6939, loss_circ=0.6933)\n",
            "[Round 47] TrainAcc=0.439 | TestAcc=0.495 | TrainLoss=0.7350 | TestLoss=0.6943 | Time=127.37s | picked=circular\n",
            "    ↳ auto_pick chose: circular (loss_lin=0.7073, loss_circ=0.7058)\n",
            "[Round 48] TrainAcc=0.368 | TestAcc=0.484 | TrainLoss=0.7407 | TestLoss=0.7066 | Time=127.04s | picked=circular\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6955, loss_circ=0.6981)\n",
            "[Round 49] TrainAcc=0.386 | TestAcc=0.484 | TrainLoss=0.7314 | TestLoss=0.6981 | Time=127.46s | picked=linear\n",
            "    ↳ auto_pick chose: linear (loss_lin=0.6964, loss_circ=0.6976)\n",
            "[Round 50] TrainAcc=0.474 | TestAcc=0.516 | TrainLoss=0.7216 | TestLoss=0.6962 | Time=128.04s | picked=linear\n",
            "Saved clinical CSV to: /content/drive/MyDrive/QFL_folder/ZZ_clinical_auto_pick/round_metrics_clinical_clients10_rounds50.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "220b1fb9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === QFL Round Metrics Plotter (columns: und, train_acc, test_acc, train_loss, test_loss, time_sec) ===\n",
        "# Instructions:\n",
        "#  1) Mount Drive in Colab.\n",
        "#  2) Set BC_PATH and CL_PATH below.\n",
        "#  3) Run to generate and save figures.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Configure paths\n",
        "# ------------------------------------------------------------\n",
        "BC_PATH = \"/content/drive/MyDrive/QFL_folder/breast_cancer_circular_weighted/round_metrics_bc_clients10_rounds20.csv\"\n",
        "CL_PATH = \"/content/drive/MyDrive/QFL_folder/clinical_circular_weighted/round_metrics_clinical_clients10_rounds20.csv\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Loader for your schema (robust to TSV/CSV and stray spaces)\n",
        "# ------------------------------------------------------------\n",
        "def load_metrics(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Expect columns: und, train_acc, test_acc, train_loss, test_loss, time_sec\n",
        "    Returns standardized: Round, TrainAcc, TestAcc, TrainLoss, TestLoss, (optional) TimeSec\n",
        "    \"\"\"\n",
        "    p = Path(csv_path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
        "\n",
        "    # Let pandas sniff delimiter (works for CSV or TSV); keep default engine='c' if you prefer.\n",
        "    df = pd.read_csv(p, sep=None, engine=\"python\")\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # Minimal schema check\n",
        "    required = {\"round\", \"train_acc\", \"test_acc\", \"train_loss\", \"test_loss\"}\n",
        "    if not required.issubset(set(df.columns)):\n",
        "        missing = sorted(required - set(df.columns))\n",
        "        raise ValueError(f\"Missing columns in {csv_path}: {missing}\")\n",
        "\n",
        "    # Construct rename map safely (no None target)\n",
        "    rename_map = {\n",
        "        \"round\": \"Round\",\n",
        "        \"train_acc\": \"TrainAcc\",\n",
        "        \"test_acc\": \"TestAcc\",\n",
        "        \"train_loss\": \"TrainLoss\",\n",
        "        \"test_loss\": \"TestLoss\",\n",
        "    }\n",
        "    if \"time_sec\" in df.columns:\n",
        "        rename_map[\"time_sec\"] = \"TimeSec\"\n",
        "\n",
        "    df = df.rename(columns=rename_map)\n",
        "\n",
        "    # Coerce numerics\n",
        "    for k in [\"Round\", \"TrainAcc\", \"TestAcc\", \"TrainLoss\", \"TestLoss\"]:\n",
        "        df[k] = pd.to_numeric(df[k], errors=\"coerce\")\n",
        "\n",
        "    # Drop incomplete rows and sort by round (just in case)\n",
        "    df = (\n",
        "        df.dropna(subset=[\"Round\", \"TrainAcc\", \"TestAcc\", \"TrainLoss\", \"TestLoss\"])\n",
        "          .sort_values(\"Round\")\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    keep = [\"Round\", \"TrainAcc\", \"TestAcc\", \"TrainLoss\", \"TestLoss\"]\n",
        "    if \"TimeSec\" in df.columns:\n",
        "        keep.append(\"TimeSec\")\n",
        "    return df[keep]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Plotting utilities\n",
        "# ------------------------------------------------------------\n",
        "def plot_acc_loss(df: pd.DataFrame, title_prefix: str, ax_acc, ax_loss):\n",
        "    ax_acc.plot(df[\"Round\"], df[\"TrainAcc\"], label=\"TrainAcc\")\n",
        "    ax_acc.plot(df[\"Round\"], df[\"TestAcc\"],  label=\"TestAcc\")\n",
        "    ax_acc.set_title(f\"{title_prefix} – Accuracy\")\n",
        "    ax_acc.set_xlabel(\"Federated Round\")\n",
        "    ax_acc.set_ylabel(\"Accuracy\")\n",
        "    ax_acc.grid(True, alpha=0.3)\n",
        "    ax_acc.legend()\n",
        "\n",
        "    ax_loss.plot(df[\"Round\"], df[\"TrainLoss\"], label=\"TrainLoss\")\n",
        "    ax_loss.plot(df[\"Round\"], df[\"TestLoss\"],  label=\"TestLoss\")\n",
        "    ax_loss.set_title(f\"{title_prefix} – Loss\")\n",
        "    ax_loss.set_xlabel(\"Federated Round\")\n",
        "    ax_loss.set_ylabel(\"Loss\")\n",
        "    ax_loss.grid(True, alpha=0.3)\n",
        "    ax_loss.legend()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Load, plot 2×2 panel, save\n",
        "# ------------------------------------------------------------\n",
        "bc_df = load_metrics(BC_PATH)\n",
        "cl_df = load_metrics(CL_PATH)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "plot_acc_loss(bc_df, \"Breast Cancer (sklearn)\", axes[0, 0], axes[0, 1])\n",
        "plot_acc_loss(cl_df, \"Clinical (PCA→5D)\",     axes[1, 0], axes[1, 1])\n",
        "plt.tight_layout()\n",
        "\n",
        "OUT_PANEL = \"/content/drive/MyDrive/QFL_folder/qfl_round_metrics_panel.png\"\n",
        "plt.savefig(OUT_PANEL, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"Saved 2×2 panel to: {OUT_PANEL}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Optional overlay comparisons (toggle as needed)\n",
        "# ------------------------------------------------------------\n",
        "def moving_average(s, k=3):\n",
        "    return s.rolling(window=k, center=True, min_periods=1).mean()\n",
        "\n",
        "SMOOTH = False\n",
        "K = 3\n",
        "def maybe(x): return moving_average(x, K) if SMOOTH else x\n",
        "\n",
        "fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Accuracy overlay\n",
        "ax1.plot(bc_df[\"Round\"], maybe(bc_df[\"TrainAcc\"]), label=\"BC TrainAcc\")\n",
        "ax1.plot(bc_df[\"Round\"], maybe(bc_df[\"TestAcc\"]),  label=\"BC TestAcc\")\n",
        "ax1.plot(cl_df[\"Round\"], maybe(cl_df[\"TrainAcc\"]), label=\"Clinical TrainAcc\")\n",
        "ax1.plot(cl_df[\"Round\"], maybe(cl_df[\"TestAcc\"]),  label=\"Clinical TestAcc\")\n",
        "ax1.set_title(\"Accuracy Overlay\")\n",
        "ax1.set_xlabel(\"Federated Round\"); ax1.set_ylabel(\"Accuracy\"); ax1.grid(True, alpha=0.3); ax1.legend()\n",
        "\n",
        "# Loss overlay\n",
        "ax2.plot(bc_df[\"Round\"], maybe(bc_df[\"TrainLoss\"]), label=\"BC TrainLoss\")\n",
        "ax2.plot(bc_df[\"Round\"], maybe(bc_df[\"TestLoss\"]),  label=\"BC TestLoss\")\n",
        "ax2.plot(cl_df[\"Round\"], maybe(cl_df[\"TrainLoss\"]), label=\"Clinical TrainLoss\")\n",
        "ax2.plot(cl_df[\"Round\"], maybe(cl_df[\"TestLoss\"]),  label=\"Clinical TestLoss\")\n",
        "ax2.set_title(\"Loss Overlay\")\n",
        "ax2.set_xlabel(\"Federated Round\"); ax2.set_ylabel(\"Loss\"); ax2.grid(True, alpha=0.3); ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "OUT_OVERLAY = \"/content/drive/MyDrive/QFL_folder/qfl_round_metrics_overlay.png\"\n",
        "plt.savefig(OUT_OVERLAY, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"Saved overlay figure to: {OUT_OVERLAY}\")\n"
      ],
      "metadata": {
        "id": "aCl_f_E-iPAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxbEq7fYyC_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Compare COBYLA (solid) vs SPSA (dotted) on BC & Clinical ===\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Paths (edit if necessary)\n",
        "# -----------------------------\n",
        "BC_COBYLA_PATH = \"/content/drive/MyDrive/QFL_folder/breast_cancer_circular_weighted/round_metrics_bc_clients10_rounds20.csv\"\n",
        "CL_COBYLA_PATH = \"/content/drive/MyDrive/QFL_folder/clinical_circular_weighted/round_metrics_clinical_clients10_rounds20.csv\"\n",
        "\n",
        "BC_SPSA_PATH   = \"/content/drive/MyDrive/QFL_folder/ZZ_SPSA_breast_cancer_circular_weighted/round_metrics_bc_clients10_rounds20.csv\"\n",
        "CL_SPSA_PATH   = \"/content/drive/MyDrive/QFL_folder/ZZ_SPSA_clinical_circular_weighted/round_metrics_clinical_clients10_rounds20.csv\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Minimal robust CSV loader\n",
        "# -----------------------------\n",
        "def load_metrics(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # Normalize round column\n",
        "    for cand in [\"round\", \"Round\", \"Federated Round\", \"und\"]:\n",
        "        if cand in df.columns:\n",
        "            df = df.rename(columns={cand: \"Round\"})\n",
        "            break\n",
        "    if \"Round\" not in df.columns:\n",
        "        df[\"Round\"] = range(1, len(df) + 1)\n",
        "\n",
        "    # Normalize metric columns (case-insensitive)\n",
        "    def ren_ci(src, dst):\n",
        "        for c in list(df.columns):\n",
        "            if c.lower() == src.lower():\n",
        "                df.rename(columns={c: dst}, inplace=True)\n",
        "                return\n",
        "    for src, dst in [\n",
        "        (\"train_acc\", \"TrainAcc\"),\n",
        "        (\"test_acc\",  \"TestAcc\"),\n",
        "        (\"train_loss\",\"TrainLoss\"),\n",
        "        (\"test_loss\", \"TestLoss\"),\n",
        "    ]:\n",
        "        ren_ci(src, dst)\n",
        "\n",
        "    keep = [\"Round\", \"TrainAcc\", \"TestAcc\", \"TrainLoss\", \"TestLoss\"]\n",
        "    for k in keep:\n",
        "        df[k] = pd.to_numeric(df[k], errors=\"coerce\")\n",
        "    return df[keep].dropna().sort_values(\"Round\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Load all four datasets\n",
        "# -----------------------------\n",
        "bc_cob = load_metrics(BC_COBYLA_PATH)\n",
        "cl_cob = load_metrics(CL_COBYLA_PATH)\n",
        "bc_sps = load_metrics(BC_SPSA_PATH)\n",
        "cl_sps = load_metrics(CL_SPSA_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Plot: solid=COBYLA, dotted=SPSA\n",
        "# -----------------------------\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# (A) Breast Cancer – Accuracy\n",
        "ax = axes[0, 0]\n",
        "ax.plot(bc_cob[\"Round\"], bc_cob[\"TrainAcc\"], linestyle=\"-\", label=\"COBYLA TrainAcc\")\n",
        "ax.plot(bc_cob[\"Round\"], bc_cob[\"TestAcc\"],  linestyle=\"-\", label=\"COBYLA TestAcc\")\n",
        "ax.plot(bc_sps[\"Round\"], bc_sps[\"TrainAcc\"], linestyle=\":\", label=\"SPSA TrainAcc\")\n",
        "ax.plot(bc_sps[\"Round\"], bc_sps[\"TestAcc\"],  linestyle=\":\", label=\"SPSA TestAcc\")\n",
        "ax.set_title(\"Breast Cancer — Accuracy\"); ax.set_xlabel(\"Round\"); ax.set_ylabel(\"Accuracy\")\n",
        "ax.grid(True, alpha=0.3); ax.legend()\n",
        "\n",
        "# (B) Breast Cancer – Loss\n",
        "ax = axes[0, 1]\n",
        "ax.plot(bc_cob[\"Round\"], bc_cob[\"TrainLoss\"], linestyle=\"-\", label=\"COBYLA TrainLoss\")\n",
        "ax.plot(bc_cob[\"Round\"], bc_cob[\"TestLoss\"],  linestyle=\"-\", label=\"COBYLA TestLoss\")\n",
        "ax.plot(bc_sps[\"Round\"], bc_sps[\"TrainLoss\"], linestyle=\":\", label=\"SPSA TrainLoss\")\n",
        "ax.plot(bc_sps[\"Round\"], bc_sps[\"TestLoss\"],  linestyle=\":\", label=\"SPSA TestLoss\")\n",
        "ax.set_title(\"Breast Cancer — Loss\"); ax.set_xlabel(\"Round\"); ax.set_ylabel(\"Loss\")\n",
        "ax.grid(True, alpha=0.3); ax.legend()\n",
        "\n",
        "# (C) Clinical — Accuracy\n",
        "ax = axes[1, 0]\n",
        "ax.plot(cl_cob[\"Round\"], cl_cob[\"TrainAcc\"], linestyle=\"-\", label=\"COBYLA TrainAcc\")\n",
        "ax.plot(cl_cob[\"Round\"], cl_cob[\"TestAcc\"],  linestyle=\"-\", label=\"COBYLA TestAcc\")\n",
        "ax.plot(cl_sps[\"Round\"], cl_sps[\"TrainAcc\"], linestyle=\":\", label=\"SPSA TrainAcc\")\n",
        "ax.plot(cl_sps[\"Round\"], cl_sps[\"TestAcc\"],  linestyle=\":\", label=\"SPSA TestAcc\")\n",
        "ax.set_title(\"Clinical — Accuracy\"); ax.set_xlabel(\"Round\"); ax.set_ylabel(\"Accuracy\")\n",
        "ax.grid(True, alpha=0.3); ax.legend()\n",
        "\n",
        "# (D) Clinical — Loss\n",
        "ax = axes[1, 1]\n",
        "ax.plot(cl_cob[\"Round\"], cl_cob[\"TrainLoss\"], linestyle=\"-\", label=\"COBYLA TrainLoss\")\n",
        "ax.plot(cl_cob[\"Round\"], cl_cob[\"TestLoss\"],  linestyle=\"-\", label=\"COBYLA TestLoss\")\n",
        "ax.plot(cl_sps[\"Round\"], cl_sps[\"TrainLoss\"], linestyle=\":\", label=\"SPSA TrainLoss\")\n",
        "ax.plot(cl_sps[\"Round\"], cl_sps[\"TestLoss\"],  linestyle=\":\", label=\"SPSA TestLoss\")\n",
        "ax.set_title(\"Clinical — Loss\"); ax.set_xlabel(\"Round\"); ax.set_ylabel(\"Loss\")\n",
        "ax.grid(True, alpha=0.3); ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "OUT = \"/content/drive/MyDrive/QFL_folder/qfl_compare_cobyla_vs_spsa_bc_clinical.png\"\n",
        "plt.savefig(OUT, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"Saved figure to: {OUT}\")\n"
      ],
      "metadata": {
        "id": "XFh_q9GPDXZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Compare Aggregators: circular_weighted (solid) vs fedavg_weighted (dashed) ===\n",
        "# - Uses round-level metrics where available.\n",
        "# - For BC fedavg_weighted, if only client-level CSV is found, it plots client-mean approximations.\n",
        "\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Paths you provided\n",
        "# -----------------------------\n",
        "# circular_weighted (round-level)\n",
        "BC_CIRC_PATH = \"/content/drive/MyDrive/QFL_folder/breast_cancer_circular_weighted/round_metrics_bc_clients10_rounds20.csv\"\n",
        "CL_CIRC_PATH = \"/content/drive/MyDrive/QFL_folder/clinical_circular_weighted/round_metrics_clinical_clients10_rounds20.csv\"\n",
        "\n",
        "# fedavg_weighted\n",
        "# - Breast Cancer: you shared client-level file\n",
        "BC_FEDAVG_CLIENT_PATH = \"/content/drive/MyDrive/QFL_folder/ZZ_SPSA_breast_cancer_fedavg_weighted/round_client_metrics_bc_clients10_rounds20.csv\"\n",
        "# We'll try to auto-locate a round-level file in the same directory:\n",
        "BC_FEDAVG_DIR = str(Path(BC_FEDAVG_CLIENT_PATH).parent)\n",
        "\n",
        "# - Clinical: you shared round-level file\n",
        "CL_FEDAVG_ROUND_PATH = \"/content/drive/MyDrive/QFL_folder/ZZ_SPSA_clinical_fedavg_weighted/round_metrics_clinical_clients10_rounds20.csv\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Robust CSV loaders\n",
        "# -----------------------------\n",
        "def load_round_metrics(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load round-level CSV -> columns: Round, TrainAcc, TestAcc, TrainLoss, TestLoss.\"\"\"\n",
        "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # Normalize round column\n",
        "    for cand in [\"round\", \"Round\", \"Federated Round\", \"und\"]:\n",
        "        if cand in df.columns:\n",
        "            df = df.rename(columns={cand: \"Round\"})\n",
        "            break\n",
        "    if \"Round\" not in df.columns:\n",
        "        df[\"Round\"] = range(1, len(df) + 1)\n",
        "\n",
        "    # Normalize metrics (case-insensitive)\n",
        "    def ren_ci(src, dst):\n",
        "        for c in list(df.columns):\n",
        "            if c.lower() == src.lower():\n",
        "                df.rename(columns={c: dst}, inplace=True)\n",
        "                return\n",
        "    for src, dst in [(\"train_acc\",\"TrainAcc\"), (\"test_acc\",\"TestAcc\"),\n",
        "                     (\"train_loss\",\"TrainLoss\"), (\"test_loss\",\"TestLoss\")]:\n",
        "        ren_ci(src, dst)\n",
        "\n",
        "    keep = [\"Round\",\"TrainAcc\",\"TestAcc\",\"TrainLoss\",\"TestLoss\"]\n",
        "    for k in keep:\n",
        "        df[k] = pd.to_numeric(df[k], errors=\"coerce\")\n",
        "    return df[keep].dropna().sort_values(\"Round\").reset_index(drop=True)\n",
        "\n",
        "def load_client_metrics(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load client-level CSV and produce per-round MEANS:\n",
        "    returns columns: Round, TrainAcc, TestAcc, TrainLoss, TestLoss (means across clients).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # Normalize columns\n",
        "    col_map = {\n",
        "        \"round\": \"Round\",\n",
        "        \"client_id\": \"client_id\",\n",
        "        \"train_acc_local\": \"TrainAcc\",\n",
        "        \"test_acc_local\": \"TestAcc\",\n",
        "        \"train_loss_local\": \"TrainLoss\",\n",
        "        \"test_loss_local\": \"TestLoss\",\n",
        "    }\n",
        "    # tolerate case differences\n",
        "    for k in list(col_map.keys()):\n",
        "        if k not in df.columns:\n",
        "            for c in df.columns:\n",
        "                if c.lower() == k.lower():\n",
        "                    col_map[c] = col_map.pop(k)\n",
        "                    break\n",
        "\n",
        "    df = df.rename(columns=col_map)\n",
        "    required = {\"Round\",\"TrainAcc\",\"TestAcc\",\"TrainLoss\",\"TestLoss\"}\n",
        "    if not required.issubset(set(df.columns)):\n",
        "        missing = sorted(required - set(df.columns))\n",
        "        raise ValueError(f\"Client CSV missing required columns: {missing}\")\n",
        "\n",
        "    # numeric + group by round\n",
        "    for k in [\"Round\",\"TrainAcc\",\"TestAcc\",\"TrainLoss\",\"TestLoss\"]:\n",
        "        df[k] = pd.to_numeric(df[k], errors=\"coerce\")\n",
        "    grp = (df.dropna(subset=list(required))\n",
        "             .groupby(\"Round\", as_index=False)[[\"TrainAcc\",\"TestAcc\",\"TrainLoss\",\"TestLoss\"]]\n",
        "             .mean())\n",
        "    return grp.sort_values(\"Round\").reset_index(drop=True)\n",
        "\n",
        "def find_round_metrics_in_dir(dir_path: str, pattern=\"round_metrics_bc_clients*_rounds*.csv\"):\n",
        "    paths = glob.glob(os.path.join(dir_path, pattern))\n",
        "    if not paths:\n",
        "        return None\n",
        "    # most recent\n",
        "    paths.sort(key=os.path.getmtime, reverse=True)\n",
        "    return paths[0]\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Resolve fedavg(BC) round-level or fallback to client-mean\n",
        "# -----------------------------\n",
        "BC_FEDAVG_ROUND_PATH = find_round_metrics_in_dir(BC_FEDAVG_DIR)\n",
        "bc_fedavg_from_client_mean = False\n",
        "\n",
        "if BC_FEDAVG_ROUND_PATH and Path(BC_FEDAVG_ROUND_PATH).exists():\n",
        "    bc_fedavg_df = load_round_metrics(BC_FEDAVG_ROUND_PATH)\n",
        "else:\n",
        "    # Fallback: compute client-mean approximation per round\n",
        "    bc_fedavg_df = load_client_metrics(BC_FEDAVG_CLIENT_PATH)\n",
        "    bc_fedavg_from_client_mean = True\n",
        "    print(\"[INFO] BC fedavg_weighted round-level metrics not found; using client-mean approximation.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Load the other three datasets\n",
        "# -----------------------------\n",
        "bc_circ_df = load_round_metrics(BC_CIRC_PATH)\n",
        "cl_circ_df = load_round_metrics(CL_CIRC_PATH)\n",
        "cl_fedavg_df = load_round_metrics(CL_FEDAVG_ROUND_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Plot: circular(solid) vs fedavg(dashed)\n",
        "# -----------------------------\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# (A) Breast Cancer — Accuracy\n",
        "ax = axes[0,0]\n",
        "ax.plot(bc_circ_df[\"Round\"], bc_circ_df[\"TrainAcc\"], linestyle=\"-\", label=\"circular TrainAcc\")\n",
        "ax.plot(bc_circ_df[\"Round\"], bc_circ_df[\"TestAcc\"],  linestyle=\"-\", label=\"circular TestAcc\")\n",
        "label_suffix = \" (client-mean approx)\" if bc_fedavg_from_client_mean else \"\"\n",
        "ax.plot(bc_fedavg_df[\"Round\"], bc_fedavg_df[\"TrainAcc\"], linestyle=\"--\", label=f\"fedavg TrainAcc{label_suffix}\")\n",
        "ax.plot(bc_fedavg_df[\"Round\"], bc_fedavg_df[\"TestAcc\"],  linestyle=\"--\", label=f\"fedavg TestAcc{label_suffix}\")\n",
        "ax.set_title(\"Breast Cancer — Accuracy\"); ax.set_xlabel(\"Round\"); ax.set_ylabel(\"Accuracy\")\n",
        "ax.grid(True, alpha=0.3); ax.legend()\n",
        "\n",
        "# (B) Breast Cancer — Loss\n",
        "ax = axes[0,1]\n",
        "ax.plot(bc_circ_df[\"Round\"], bc_circ_df[\"TrainLoss\"], linestyle=\"-\", label=\"circular TrainLoss\")\n",
        "ax.plot(bc_circ_df[\"Round\"], bc_circ_df[\"TestLoss\"],  linestyle=\"-\", label=\"circular TestLoss\")\n",
        "ax.plot(bc_fedavg_df[\"Round\"], bc_fedavg_df[\"TrainLoss\"], linestyle=\"--\", label=f\"fedavg TrainLoss{label_suffix}\")\n",
        "ax.plot(bc_fedavg_df[\"Round\"], bc_fedavg_df[\"TestLoss\"],  linestyle=\"--\", label=f\"fedavg TestLoss{label_suffix}\")\n",
        "ax.set_title(\"Breast Cancer — Loss\"); ax.set_xlabel(\"Round\"); ax.set_ylabel(\"Loss\")\n",
        "ax.grid(True, alpha=0.3); ax.legend()\n",
        "\n",
        "# (C) Clinical — Accuracy\n",
        "ax = axes[1,0]\n",
        "ax.plot(cl_circ_df[\"Round\"], cl_circ_df[\"TrainAcc\"], linestyle=\"-\", label=\"circular TrainAcc\")\n",
        "ax.plot(cl_circ_df[\"Round\"], cl_circ_df[\"TestAcc\"],  linestyle=\"-\", label=\"circular TestAcc\")\n",
        "ax.plot(cl_fedavg_df[\"Round\"], cl_fedavg_df[\"TrainAcc\"], linestyle=\"--\", label=\"fedavg TrainAcc\")\n",
        "ax.plot(cl_fedavg_df[\"Round\"], cl_fedavg_df[\"TestAcc\"],  linestyle=\"--\", label=\"fedavg TestAcc\")\n",
        "ax.set_title(\"Clinical — Accuracy\"); ax.set_xlabel(\"Round\"); ax.set_ylabel(\"Accuracy\")\n",
        "ax.grid(True, alpha=0.3); ax.legend()\n",
        "\n",
        "# (D) Clinical — Loss\n",
        "ax = axes[1,1]\n",
        "ax.plot(cl_circ_df[\"Round\"], cl_circ_df[\"TrainLoss\"], linestyle=\"-\", label=\"circular TrainLoss\")\n",
        "ax.plot(cl_circ_df[\"Round\"], cl_circ_df[\"TestLoss\"],  linestyle=\"-\", label=\"circular TestLoss\")\n",
        "ax.plot(cl_fedavg_df[\"Round\"], cl_fedavg_df[\"TrainLoss\"], linestyle=\"--\", label=\"fedavg TrainLoss\")\n",
        "ax.plot(cl_fedavg_df[\"Round\"], cl_fedavg_df[\"TestLoss\"],  linestyle=\"--\", label=\"fedavg TestLoss\")\n",
        "ax.set_title(\"Clinical — Loss\"); ax.set_xlabel(\"Round\"); ax.set_ylabel(\"Loss\")\n",
        "ax.grid(True, alpha=0.3); ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "OUT = \"/content/drive/MyDrive/QFL_folder/qfl_compare_circular_vs_fedavg_bc_clinical.png\"\n",
        "plt.savefig(OUT, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"Saved comparison figure to: {OUT}\")\n"
      ],
      "metadata": {
        "id": "lj5-fuImyEDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1JZEEOzW1BcCFj85aXJl2eruwXe2NQIkO",
      "authorship_tag": "ABX9TyMMTL1LlBOjq31YkPEjj3qc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}