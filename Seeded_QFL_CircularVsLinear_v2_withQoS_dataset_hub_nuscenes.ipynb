{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanikairoshi/QFL_Experiments/blob/main/Seeded_QFL_CircularVsLinear_v2_withQoS_dataset_hub_nuscenes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b22c42e",
      "metadata": {
        "id": "4b22c42e"
      },
      "source": [
        "# QoS-weighted QFL aggregation (Linear vs Circular) — Notebook\n",
        "\n",
        "This notebook wraps the fixed script `pasted_qos_updated_fixed.py`.\n",
        "\n",
        "The previous error (`AttributeError: 'FLConfig' object has no attribute 'use_qos_weights'`)\n",
        "occurred because QoS fields were outside the `FLConfig` dataclass. This notebook fixes that,\n",
        "so you can toggle QoS-weighted aggregation ablations.\n",
        "\n",
        "## Ablations\n",
        "- Baseline: `use_qos_weights=False`\n",
        "- Fidelity only: `use_qos_weights=True, qos_alpha=1, qos_gamma=0, qos_delta=0`\n",
        "- Latency only: `use_qos_weights=True, qos_alpha=0, qos_gamma=1, qos_delta=0`\n",
        "- Instability only: `use_qos_weights=True, qos_alpha=0, qos_gamma=0, qos_delta=1`\n",
        "- All signals: `use_qos_weights=True, qos_alpha=1, qos_gamma=1, qos_delta=1`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"qiskit>=1.1\" qiskit-aer qiskit-algorithms \"qiskit-machine-learning>=0.7\" qiskit-ibm-runtime \\\n",
        "                 scikit-learn pandas matplotlib numpy torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3fqbUpKjyb2",
        "outputId": "729559b3-e70e-49c2-b027-06375b7401c8"
      },
      "id": "q3fqbUpKjyb2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit>=1.1\n",
            "  Downloading qiskit-2.2.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (12 kB)\n",
            "Collecting qiskit-aer\n",
            "  Downloading qiskit_aer-0.17.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Collecting qiskit-algorithms\n",
            "  Downloading qiskit_algorithms-0.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting qiskit-machine-learning>=0.7\n",
            "  Downloading qiskit_machine_learning-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting qiskit-ibm-runtime\n",
            "  Downloading qiskit_ibm_runtime-0.44.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit>=1.1)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1) (1.16.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1) (0.3.8)\n",
            "Collecting stevedore>=3.0.0 (from qiskit>=1.1)\n",
            "  Downloading stevedore-5.6.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from qiskit>=1.1) (4.15.0)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-aer) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools>=40.1 in /usr/local/lib/python3.12/dist-packages (from qiskit-machine-learning>=0.7) (75.2.0)\n",
            "Requirement already satisfied: requests!=2.32.2,>=2.19 in /usr/local/lib/python3.12/dist-packages (from qiskit-ibm-runtime) (2.32.4)\n",
            "Collecting requests-ntlm>=1.1.0 (from qiskit-ibm-runtime)\n",
            "  Downloading requests_ntlm-1.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from qiskit-ibm-runtime) (2.5.0)\n",
            "Collecting ibm-platform-services>=0.22.6 (from qiskit-ibm-runtime)\n",
            "  Downloading ibm_platform_services-0.72.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from qiskit-ibm-runtime) (2.12.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from qiskit-ibm-runtime) (25.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting ibm_cloud_sdk_core<4.0.0,>=3.24.2 (from ibm-platform-services>=0.22.6->qiskit-ibm-runtime)\n",
            "  Downloading ibm_cloud_sdk_core-3.24.2-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.5.0->qiskit-ibm-runtime) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.5.0->qiskit-ibm-runtime) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.5.0->qiskit-ibm-runtime) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.0->qiskit-aer) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests!=2.32.2,>=2.19->qiskit-ibm-runtime) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests!=2.32.2,>=2.19->qiskit-ibm-runtime) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests!=2.32.2,>=2.19->qiskit-ibm-runtime) (2025.11.12)\n",
            "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.12/dist-packages (from requests-ntlm>=1.1.0->qiskit-ibm-runtime) (43.0.3)\n",
            "Collecting pyspnego>=0.4.0 (from requests-ntlm>=1.1.0->qiskit-ibm-runtime)\n",
            "  Downloading pyspnego-0.12.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime) (2.0.0)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from ibm_cloud_sdk_core<4.0.0,>=3.24.2->ibm-platform-services>=0.22.6->qiskit-ibm-runtime) (2.10.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-runtime) (2.23)\n",
            "Downloading qiskit-2.2.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_aer-0.17.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_algorithms-0.4.0-py3-none-any.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_machine_learning-0.9.0-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.1/263.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qiskit_ibm_runtime-0.44.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ibm_platform_services-0.72.0-py3-none-any.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.5/378.5 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_ntlm-1.3.0-py3-none-any.whl (6.6 kB)\n",
            "Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.6.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ibm_cloud_sdk_core-3.24.2-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspnego-0.12.0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stevedore, rustworkx, qiskit, ibm_cloud_sdk_core, qiskit-machine-learning, qiskit-algorithms, qiskit-aer, pyspnego, ibm-platform-services, requests-ntlm, qiskit-ibm-runtime\n",
            "Successfully installed ibm-platform-services-0.72.0 ibm_cloud_sdk_core-3.24.2 pyspnego-0.12.0 qiskit-2.2.3 qiskit-aer-0.17.2 qiskit-algorithms-0.4.0 qiskit-ibm-runtime-0.44.0 qiskit-machine-learning-0.9.0 requests-ntlm-1.3.0 rustworkx-0.17.1 stevedore-5.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d90d9273",
      "metadata": {
        "id": "d90d9273"
      },
      "outputs": [],
      "source": [
        "# (Optional) quick sanity check that FLConfig has QoS fields\n",
        "# Run this cell after executing the main script cell below if you want to confirm.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # suppress all warnings\n",
        "\n",
        "\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ------------------------\n",
        "# Qiskit ML (built-ins)\n",
        "# ------------------------\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.quantum_info import SparsePauliOp, Statevector\n",
        "from qiskit.primitives import StatevectorEstimator\n",
        "\n",
        "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
        "try:\n",
        "    from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "except Exception:\n",
        "    from qiskit_machine_learning.algorithms import NeuralNetworkClassifier\n",
        "\n",
        "# optional gradient\n",
        "try:\n",
        "    from qiskit_algorithms.gradients import ParamShiftEstimatorGradient\n",
        "except Exception:\n",
        "    try:\n",
        "        from qiskit.algorithms.gradients import ParamShiftEstimatorGradient\n",
        "    except Exception:\n",
        "        ParamShiftEstimatorGradient = None\n",
        "\n",
        "# ------------------------\n",
        "# Optimizers (robust import)\n",
        "# ------------------------\n",
        "try:\n",
        "    from qiskit_algorithms.optimizers import COBYLA\n",
        "except Exception:\n",
        "    try:\n",
        "        from qiskit.algorithms.optimizers import COBYLA\n",
        "    except Exception:\n",
        "        COBYLA = None\n",
        "\n",
        "try:\n",
        "    from qiskit_algorithms.optimizers import SPSA\n",
        "except Exception:\n",
        "    try:\n",
        "        from qiskit.algorithms.optimizers import SPSA\n",
        "    except Exception:\n",
        "        SPSA = None\n",
        "\n",
        "# ------------------------\n",
        "# Sklearn\n",
        "# ------------------------\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Drive helper\n",
        "# ============================================================\n",
        "def get_outdir_in_drive(subdir: str) -> Path:\n",
        "    \"\"\"\n",
        "    If running in Colab, mount Drive and save under /content/drive/MyDrive/<subdir>.\n",
        "    Otherwise, fallback to ~/ (HOME)/<subdir>.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        base = Path(\"/content/drive/MyDrive/AutopickV2\")\n",
        "    except Exception:\n",
        "        base = Path.home()\n",
        "    out = base / subdir\n",
        "    out.mkdir(parents=True, exist_ok=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Data loading\n",
        "# ============================================================\n",
        "def load_breast_cancer(\n",
        "    pca_k: Optional[int] = 4,\n",
        "    test_size: float = 0.30,\n",
        "    seed: int = 42\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    data = datasets.load_breast_cancer()\n",
        "    X, y = data.data.astype(float), data.target.astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(X_train)\n",
        "    X_test = sc.transform(X_test)\n",
        "\n",
        "    if pca_k is not None:\n",
        "        p = PCA(n_components=int(pca_k), random_state=seed)\n",
        "        X_train = p.fit_transform(X_train)\n",
        "        X_test = p.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def load_clinical_csv(\n",
        "    csv_path: str,\n",
        "    test_size: float = 0.25,\n",
        "    pca_k: int = 2,\n",
        "    seed: int = 42\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Minimal preprocessing for BrEaST-Lesions-USG-Clinical.csv.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    selected_features = [\"Age\", \"Shape\", \"Echogenicity\",\n",
        "                         \"Posterior_features\", \"Calcifications\", \"Classification\"]\n",
        "    df = df[selected_features]\n",
        "\n",
        "    df = df[\n",
        "        (df[\"Age\"] != \"not available\") &\n",
        "        (~df[\"Shape\"].isin([\"not applicable\"])) &\n",
        "        (~df[\"Echogenicity\"].isin([\"not applicable\"])) &\n",
        "        (~df[\"Posterior_features\"].isin([\"not applicable\"])) &\n",
        "        (~df[\"Calcifications\"].isin([\"not applicable\", \"indefinable\"])) &\n",
        "        (df[\"Classification\"].isin([\"benign\", \"malignant\"]))\n",
        "    ].copy()\n",
        "\n",
        "    df[\"Age\"] = pd.to_numeric(df[\"Age\"])\n",
        "\n",
        "    for col in [\"Shape\", \"Echogenicity\", \"Posterior_features\", \"Calcifications\"]:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "    df[\"Label\"] = df[\"Classification\"].map({\"benign\": 0, \"malignant\": 1})\n",
        "    df.drop(columns=[\"Classification\"], inplace=True)\n",
        "\n",
        "    X = df.drop(columns=[\"Label\"]).values.astype(float)\n",
        "    y = df[\"Label\"].values.astype(int)\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    X = sc.fit_transform(X)\n",
        "\n",
        "    p = PCA(n_components=int(pca_k), random_state=seed)\n",
        "    X = p.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Validation split helper\n",
        "# ============================================================\n",
        "def split_train_val(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    val_size: float = 0.15,\n",
        "    seed: int = 42\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Create a validation set from the training data only.\n",
        "    \"\"\"\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=val_size, random_state=seed, stratify=y_train\n",
        "    )\n",
        "    return X_tr, X_val, y_tr, y_val\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Partition helpers\n",
        "# ============================================================\n",
        "def shard_iid(X: np.ndarray, y: np.ndarray, num_clients: int, seed: int = 7):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.permutation(len(y))\n",
        "    chunks = np.array_split(idx, num_clients)\n",
        "    return [(X[c], y[c]) for c in chunks]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# OpenML loaders (CPU-friendly) for biomarker / tabular datasets\n",
        "# ============================================================\n",
        "def load_openml_binary(\n",
        "    data_id: int,\n",
        "    *,\n",
        "    pos_label: Optional[str] = None,\n",
        "    pca_k: Optional[int] = 4,\n",
        "    test_size: float = 0.30,\n",
        "    seed: int = 42,\n",
        "    max_samples: Optional[int] = 600,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Fetch a dataset from OpenML and return (X_train, X_test, y_train, y_test) for binary classification.\n",
        "    - If the dataset is binary, we map the *second* class (sorted) to 1 by default.\n",
        "    - If multi-class, we keep only the top-2 most frequent classes (binary reduction).\n",
        "    - max_samples caps the total samples (before train/test split) to keep CPU runtime practical.\n",
        "    \"\"\"\n",
        "    X, y = fetch_openml(data_id=data_id, as_frame=True, return_X_y=True)\n",
        "    X_df = X.copy() if hasattr(X, \"copy\") else pd.DataFrame(X)\n",
        "    y_s = pd.Series(y).astype(str)\n",
        "\n",
        "    # Numeric matrix + median imputation\n",
        "    X_df = X_df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    X_df = X_df.fillna(X_df.median(numeric_only=True))\n",
        "\n",
        "    # Reduce labels to binary (if needed)\n",
        "    classes = list(pd.unique(y_s))\n",
        "    if pos_label is not None:\n",
        "        y_bin = (y_s == str(pos_label)).astype(int)\n",
        "    else:\n",
        "        if len(classes) == 2:\n",
        "            cls_sorted = sorted(map(str, classes))\n",
        "            y_bin = (y_s == cls_sorted[1]).astype(int)\n",
        "        else:\n",
        "            top2 = y_s.value_counts().index[:2].tolist()\n",
        "            mask = y_s.isin(top2)\n",
        "            X_df = X_df.loc[mask].reset_index(drop=True)\n",
        "            y_s = y_s.loc[mask].reset_index(drop=True)\n",
        "            y_bin = (y_s == top2[1]).astype(int)\n",
        "\n",
        "    # Optional subsample BEFORE split (stratified)\n",
        "    if max_samples is not None and len(X_df) > max_samples:\n",
        "        X_df, _, y_bin, _ = train_test_split(\n",
        "            X_df, y_bin, train_size=max_samples, random_state=seed, stratify=y_bin\n",
        "        )\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_df.to_numpy(dtype=float),\n",
        "        np.asarray(y_bin, dtype=int),\n",
        "        test_size=test_size,\n",
        "        random_state=seed,\n",
        "        stratify=y_bin\n",
        "    )\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(X_train)\n",
        "    X_test = sc.transform(X_test)\n",
        "\n",
        "    if pca_k is not None:\n",
        "        pca = PCA(n_components=pca_k, random_state=seed, svd_solver=\"randomized\")\n",
        "        X_train = pca.fit_transform(X_train)\n",
        "        X_test = pca.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def load_biomarker_leukemia(\n",
        "    *,\n",
        "    pca_k: Optional[int] = 4,\n",
        "    test_size: float = 0.30,\n",
        "    seed: int = 42,\n",
        "    max_samples: Optional[int] = 400,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"OpenML id=1104 (leukemia; ALL vs AML style binary).\"\"\"\n",
        "    return load_openml_binary(\n",
        "        1104, pca_k=pca_k, test_size=test_size, seed=seed, max_samples=max_samples\n",
        "    )\n",
        "\n",
        "def load_biomarker_arcene(\n",
        "    *,\n",
        "    pca_k: Optional[int] = 4,\n",
        "    test_size: float = 0.30,\n",
        "    seed: int = 42,\n",
        "    max_samples: Optional[int] = 600,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"OpenML id=1458 (ARCENE; cancer vs normal; high-dimensional).\"\"\"\n",
        "    return load_openml_binary(\n",
        "        1458, pca_k=pca_k, test_size=test_size, seed=seed, max_samples=max_samples\n",
        "    )\n",
        "\n",
        "\n",
        "def build_nuscenes_turning_dataset(\n",
        "    dataroot: str,\n",
        "    version: str = \"v1.0-mini\",\n",
        "    category_prefix: str = \"vehicle\",\n",
        "    history_sec: float = 2.0,\n",
        "    horizon_sec: float = 2.0,\n",
        "    yaw_thresh_deg: float = 15.0,\n",
        "    max_samples: Optional[int] = 400,\n",
        "    seed: int = 42,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Build a small CPU-friendly motion dataset from nuScenes annotations.\n",
        "\n",
        "    We construct fixed-length windows from annotated object tracks (default: any 'vehicle*').\n",
        "    Binary label: whether the agent will 'turn' within the horizon (yaw change > threshold).\n",
        "\n",
        "    Requires:\n",
        "      pip install nuscenes-devkit\n",
        "      and nuScenes data downloaded under `dataroot` with `v1.0-mini/` available.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from nuscenes.nuscenes import NuScenes\n",
        "        from pyquaternion import Quaternion\n",
        "    except Exception as e:\n",
        "        raise ImportError(\n",
        "            \"nuScenes support requires `nuscenes-devkit` (and its deps). \"\n",
        "            \"Install with: pip install nuscenes-devkit\"\n",
        "        ) from e\n",
        "\n",
        "    dt = 0.5  # nuScenes keyframes are at 2Hz (every 0.5s) in the standard release.\n",
        "    H = max(1, int(round(history_sec / dt)))\n",
        "    F = max(1, int(round(horizon_sec / dt)))\n",
        "    yaw_thresh = np.deg2rad(float(yaw_thresh_deg))\n",
        "\n",
        "    def wrap_pi(a: np.ndarray) -> np.ndarray:\n",
        "        return (a + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "    nusc = NuScenes(version=version, dataroot=dataroot, verbose=False)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    X_list: List[List[float]] = []\n",
        "    y_list: List[int] = []\n",
        "\n",
        "    for inst in nusc.instance:\n",
        "        # Build the annotation chain for this instance.\n",
        "        ann_token = inst.get(\"first_annotation_token\", \"\")\n",
        "        if not ann_token:\n",
        "            continue\n",
        "\n",
        "        track = []\n",
        "        while ann_token:\n",
        "            ann = nusc.get(\"sample_annotation\", ann_token)\n",
        "            if str(ann.get(\"category_name\", \"\")).startswith(category_prefix):\n",
        "                track.append(ann)\n",
        "            ann_token = ann.get(\"next\", \"\") or \"\"\n",
        "\n",
        "        if len(track) < (H + F + 2):\n",
        "            continue\n",
        "\n",
        "        xs = np.asarray([a[\"translation\"][0] for a in track], dtype=float)\n",
        "        ys = np.asarray([a[\"translation\"][1] for a in track], dtype=float)\n",
        "\n",
        "        # yaw from quaternion [w, x, y, z]\n",
        "        yaws = np.asarray([Quaternion(a[\"rotation\"]).yaw_pitch_roll[0] for a in track], dtype=float)\n",
        "\n",
        "        # speed estimate (finite difference on positions)\n",
        "        dx = np.diff(xs)\n",
        "        dy = np.diff(ys)\n",
        "        speeds = np.sqrt(dx**2 + dy**2) / dt\n",
        "        speeds = np.concatenate([[speeds[0]], speeds])  # align to track length\n",
        "\n",
        "        for t in range(H, len(track) - F - 1):\n",
        "            dxH = xs[t] - xs[t - H]\n",
        "            dyH = ys[t] - ys[t - H]\n",
        "\n",
        "            v_hist = speeds[t - H:t]\n",
        "            v_mean = float(np.mean(v_hist))\n",
        "            v_last = float(speeds[t])\n",
        "            v_prev = float(speeds[t - 1]) if t - 1 >= 0 else v_last\n",
        "            a_last = (v_last - v_prev) / dt\n",
        "\n",
        "            yaw_now = float(yaws[t])\n",
        "            yaw_past = float(yaws[t - H])\n",
        "            yaw_rate = float(wrap_pi(np.array([yaw_now - yaw_past]))[0] / (H * dt))\n",
        "\n",
        "            yaw_future = float(yaws[t + F])\n",
        "            turn = int(abs(wrap_pi(np.array([yaw_future - yaw_now]))[0]) > yaw_thresh)\n",
        "\n",
        "            # Feature vector (compact + stable):\n",
        "            # displacement, speed stats, accel, yaw_rate, heading embedding\n",
        "            X_list.append([\n",
        "                float(dxH), float(dyH),\n",
        "                v_mean, v_last,\n",
        "                float(np.std(v_hist, ddof=0)),\n",
        "                float(a_last),\n",
        "                float(yaw_rate),\n",
        "                float(np.sin(yaw_now)), float(np.cos(yaw_now)),\n",
        "            ])\n",
        "            y_list.append(turn)\n",
        "\n",
        "            if max_samples is not None and len(y_list) >= int(max_samples):\n",
        "                break\n",
        "\n",
        "        if max_samples is not None and len(y_list) >= int(max_samples):\n",
        "            break\n",
        "\n",
        "    if len(y_list) < 20:\n",
        "        raise RuntimeError(\n",
        "            \"nuScenes motion extraction produced too few windows. \"\n",
        "            \"Check that `dataroot` points to a valid nuScenes root containing the requested version \"\n",
        "            f\"(e.g., {dataroot}/{version}).\"\n",
        "        )\n",
        "\n",
        "    X = np.asarray(X_list, dtype=float)\n",
        "    y = np.asarray(y_list, dtype=int)\n",
        "\n",
        "    idx = rng.permutation(len(y))\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "\n",
        "def load_motion_nuscenes_mini(\n",
        "    *,\n",
        "    dataroot: str,\n",
        "    version: str = \"v1.0-mini\",\n",
        "    category_prefix: str = \"vehicle\",\n",
        "    pca_k: Optional[int] = 4,\n",
        "    test_size: float = 0.30,\n",
        "    seed: int = 42,\n",
        "    max_samples: Optional[int] = 400,\n",
        "    history_sec: float = 2.0,\n",
        "    horizon_sec: float = 2.0,\n",
        "    yaw_thresh_deg: float = 15.0,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"nuScenes v1.0-mini → CPU-friendly motion windows (binary turning label).\"\"\"\n",
        "    X, y = build_nuscenes_turning_dataset(\n",
        "        dataroot=dataroot,\n",
        "        version=version,\n",
        "        category_prefix=category_prefix,\n",
        "        history_sec=history_sec,\n",
        "        horizon_sec=horizon_sec,\n",
        "        yaw_thresh_deg=yaw_thresh_deg,\n",
        "        max_samples=max_samples,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        random_state=seed,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(X_train)\n",
        "    X_test = sc.transform(X_test)\n",
        "\n",
        "    if pca_k is not None:\n",
        "        pca = PCA(n_components=pca_k, random_state=seed, svd_solver=\"randomized\")\n",
        "        X_train = pca.fit_transform(X_train)\n",
        "        X_test = pca.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def shard_label_skew(\n",
        "    X: np.ndarray, y: np.ndarray,\n",
        "    num_clients: int, min_per_client: int = 60, seed: int = 7\n",
        "):\n",
        "    \"\"\"\n",
        "    Each client gets mostly a single label (binary); draws with replacement as needed.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    X0, y0 = X[y == 0], y[y == 0]\n",
        "    X1, y1 = X[y == 1], y[y == 1]\n",
        "    shards = []\n",
        "    for cid in range(num_clients):\n",
        "        maj = 0 if (cid % 2 == 0) else 1\n",
        "        k_major = int(min_per_client * 0.8)\n",
        "        k_minor = max(1, min_per_client - k_major)\n",
        "\n",
        "        if maj == 0:\n",
        "            idx0 = rng.integers(0, len(X0), size=k_major)\n",
        "            idx1 = rng.integers(0, len(X1), size=k_minor)\n",
        "            Xi = np.vstack([X0[idx0], X1[idx1]])\n",
        "            yi = np.hstack([y0[idx0], y1[idx1]])\n",
        "        else:\n",
        "            idx0 = rng.integers(0, len(X0), size=k_minor)\n",
        "            idx1 = rng.integers(0, len(X1), size=k_major)\n",
        "            Xi = np.vstack([X0[idx0], X1[idx1]])\n",
        "            yi = np.hstack([y0[idx0], y1[idx1]])\n",
        "\n",
        "        p = rng.permutation(len(yi))\n",
        "        shards.append((Xi[p], yi[p]))\n",
        "\n",
        "    return shards\n",
        "\n",
        "\n",
        "def shard_dirichlet(\n",
        "    X: np.ndarray, y: np.ndarray,\n",
        "    num_clients: int, alpha: float = 0.3, seed: int = 7,\n",
        "    min_samples_per_class_per_client: int = 1\n",
        "):\n",
        "    \"\"\"\n",
        "    Dirichlet label distribution partition.\n",
        "    Smaller alpha => stronger non-IID.\n",
        "    Ensures each client has at least min_samples_per_class_per_client for each class.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    classes = np.unique(y)\n",
        "    if len(classes) != 2:\n",
        "        raise ValueError(\"shard_dirichlet expects binary classification labels (0 and 1).\")\n",
        "\n",
        "    client_shards_data = [[] for _ in range(num_clients)]\n",
        "    client_shards_labels = [[] for _ in range(num_clients)]\n",
        "\n",
        "    for c in classes:\n",
        "        idx_c = np.where(y == c)[0]\n",
        "        rng.shuffle(idx_c)\n",
        "\n",
        "        if len(idx_c) < num_clients * min_samples_per_class_per_client:\n",
        "            raise ValueError(f\"Not enough samples for class {c} to satisfy \"\n",
        "                             f\"min_samples_per_class_per_client={min_samples_per_class_per_client} \"\n",
        "                             f\"for {num_clients} clients.\")\n",
        "\n",
        "        # 1. Distribute minimum required samples for current class 'c'\n",
        "        remaining_idx_c = list(idx_c)\n",
        "        for i in range(num_clients):\n",
        "            client_shards_data[i].extend(X[remaining_idx_c[:min_samples_per_class_per_client]].tolist())\n",
        "            client_shards_labels[i].extend(y[remaining_idx_c[:min_samples_per_class_per_client]].tolist())\n",
        "            remaining_idx_c = remaining_idx_c[min_samples_per_class_per_client:]\n",
        "\n",
        "        # 2. Distribute the rest using Dirichlet\n",
        "        props = rng.dirichlet([alpha] * num_clients)\n",
        "        counts = (props * len(remaining_idx_c)).astype(int)\n",
        "\n",
        "        # Fix rounding drift\n",
        "        while counts.sum() < len(remaining_idx_c):\n",
        "            counts[rng.integers(0, num_clients)] += 1\n",
        "        while counts.sum() > len(remaining_idx_c):\n",
        "            j = rng.integers(0, num_clients)\n",
        "            if counts[j] > 0:\n",
        "                counts[j] -= 1\n",
        "\n",
        "        start = 0\n",
        "        for i in range(num_clients):\n",
        "            take = counts[i]\n",
        "            if take > 0:\n",
        "                client_shards_data[i].extend(X[remaining_idx_c[start:start+take]].tolist())\n",
        "                client_shards_labels[i].extend(y[remaining_idx_c[start:start+take]].tolist())\n",
        "            start += take\n",
        "\n",
        "    shards = []\n",
        "    for i in range(num_clients):\n",
        "        Xi = np.array(client_shards_data[i])\n",
        "        yi = np.array(client_shards_labels[i])\n",
        "        p = rng.permutation(len(yi))\n",
        "        shards.append((Xi[p], yi[p]))\n",
        "\n",
        "    return shards\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QNN builder (built-ins only)\n",
        "# ============================================================\n",
        "def build_estimator_qnn(\n",
        "    num_features: int,\n",
        "    fm_reps: int = 1,\n",
        "    an_reps: int = 2\n",
        "):\n",
        "    fm = ZZFeatureMap(feature_dimension=num_features, reps=fm_reps)\n",
        "    an = RealAmplitudes(num_qubits=num_features, reps=an_reps, entanglement=\"linear\")\n",
        "\n",
        "    obs = SparsePauliOp.from_list([(\"Z\" + \"I\" * (num_features - 1), 1.0)])\n",
        "    est = StatevectorEstimator()\n",
        "\n",
        "    grad = None\n",
        "    if ParamShiftEstimatorGradient is not None:\n",
        "        try:\n",
        "            grad = ParamShiftEstimatorGradient(est)\n",
        "        except Exception:\n",
        "            grad = None\n",
        "\n",
        "    qnn = EstimatorQNN(\n",
        "        circuit=fm.compose(an),\n",
        "        observables=obs,\n",
        "        input_params=sorted(list(fm.parameters), key=lambda p: p.name),\n",
        "        weight_params=list(an.parameters),\n",
        "        estimator=est,\n",
        "        gradient=grad,\n",
        "    )\n",
        "    return qnn\n",
        "\n",
        "\n",
        "def make_classifier(\n",
        "    num_features: int,\n",
        "    initial_point: Optional[np.ndarray],\n",
        "    maxiter: int = 10,\n",
        "    optimizer: str = \"SPSA\",\n",
        "    opt_kwargs: Optional[dict] = None\n",
        "):\n",
        "    qnn = build_estimator_qnn(num_features)\n",
        "    opt_kwargs = {} if opt_kwargs is None else dict(opt_kwargs)\n",
        "\n",
        "    if optimizer.upper() == \"SPSA\":\n",
        "        if SPSA is None:\n",
        "            raise RuntimeError(\"SPSA not available; install/update qiskit-algorithms.\")\n",
        "        try:\n",
        "            opt = SPSA(maxiter=maxiter, **opt_kwargs)\n",
        "        except TypeError:\n",
        "            opt = SPSA(maxiter=maxiter)\n",
        "    elif optimizer.upper() == \"COBYLA\":\n",
        "        if COBYLA is None:\n",
        "            raise RuntimeError(\"COBYLA not available; install/update qiskit-algorithms.\")\n",
        "        opt = COBYLA(maxiter=maxiter, **opt_kwargs) if opt_kwargs else COBYLA(maxiter=maxiter)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer '{optimizer}'.\")\n",
        "\n",
        "    clf = NeuralNetworkClassifier(\n",
        "        neural_network=qnn,\n",
        "        optimizer=opt,\n",
        "        initial_point=initial_point,\n",
        "        loss=\"cross_entropy\",\n",
        "        one_hot=False,\n",
        "    )\n",
        "    return clf\n",
        "\n",
        "\n",
        "def extract_weights(clf: NeuralNetworkClassifier) -> np.ndarray:\n",
        "    for attr in (\"fit_result_\", \"fit_result\"):\n",
        "        if hasattr(clf, attr):\n",
        "            res = getattr(clf, attr)\n",
        "            if hasattr(res, \"x\"):\n",
        "                return np.asarray(res.x, float).copy()\n",
        "            if isinstance(res, dict) and \"x\" in res:\n",
        "                return np.asarray(res[\"x\"], float).copy()\n",
        "\n",
        "    if hasattr(clf, \"weights_\"):\n",
        "        return np.asarray(getattr(clf, \"weights_\"), float).copy()\n",
        "\n",
        "    raise RuntimeError(\"Could not extract weights; check qiskit-machine-learning version.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Circular helpers\n",
        "# ============================================================\n",
        "def wrap_to_pi(theta: np.ndarray) -> np.ndarray:\n",
        "    return (theta + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QoS signals for QFL aggregation (fidelity, latency, instability)\n",
        "# ============================================================\n",
        "def angular_instability_sigma2(w_local: np.ndarray, w_global: np.ndarray) -> float:\n",
        "    \"\"\"A simple angular instability proxy: mean squared wrapped update magnitude.\"\"\"\n",
        "    diff = wrap_to_pi(np.asarray(w_local, float) - np.asarray(w_global, float))\n",
        "    return float(np.mean(diff * diff))\n",
        "\n",
        "def _bind_qnn_params(qnn_ref: EstimatorQNN, x_scaled: np.ndarray, w: np.ndarray) -> Dict:\n",
        "    \"\"\"Build a parameter binding dict for qnn_ref.circuit.\"\"\"\n",
        "    bind = {}\n",
        "    for p, val in zip(qnn_ref.input_params, np.asarray(x_scaled, float).reshape(-1)):\n",
        "        bind[p] = float(val)\n",
        "    for p, val in zip(qnn_ref.weight_params, np.asarray(w, float).reshape(-1)):\n",
        "        bind[p] = float(val)\n",
        "    return bind\n",
        "\n",
        "def avg_state_fidelity(\n",
        "    qnn_ref: EstimatorQNN,\n",
        "    X_ref: np.ndarray,\n",
        "    w_a: np.ndarray,\n",
        "    w_b: np.ndarray,\n",
        "    x_scale: float = np.pi,\n",
        ") -> float:\n",
        "    \"\"\"Average pure-state fidelity between |psi(x; w_a)> and |psi(x; w_b)> over a small reference set.\n",
        "\n",
        "    Notes:\n",
        "      - This uses Statevector simulation; on hardware you would typically estimate fidelity via tomography/shadow\n",
        "        methods or proxy channel metrics.\n",
        "      - In this codebase, it is primarily a *relative* QoS indicator for ablations.\n",
        "    \"\"\"\n",
        "    if X_ref is None or len(X_ref) == 0:\n",
        "        return 1.0\n",
        "\n",
        "    circ = qnn_ref.circuit\n",
        "    vals = []\n",
        "    for x in np.asarray(X_ref, float):\n",
        "        xa = x * x_scale\n",
        "        psi_a = Statevector.from_instruction(circ.assign_parameters(_bind_qnn_params(qnn_ref, xa, w_a), inplace=False))\n",
        "        psi_b = Statevector.from_instruction(circ.assign_parameters(_bind_qnn_params(qnn_ref, xa, w_b), inplace=False))\n",
        "        ov = np.vdot(psi_a.data, psi_b.data)\n",
        "        vals.append(float(np.abs(ov) ** 2))\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "def compute_qos_scores(\n",
        "    local_ws: List[np.ndarray],\n",
        "    w_global: np.ndarray,\n",
        "    local_times: List[float],\n",
        "    qnn_ref: EstimatorQNN,\n",
        "    X_ref: Optional[np.ndarray],\n",
        "    cfg: \"FLConfig\",\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Compute per-client QoS scores q_i,t from fidelity F, latency tau, and instability sigma^2.\n",
        "\n",
        "    q_i,t = F_i,t^alpha / ((tau_i,t + eps)^gamma * (sigma_i,t^2 + eps)^delta)\n",
        "    \"\"\"\n",
        "    K = len(local_ws)\n",
        "    tau = np.asarray(local_times, float).reshape(K)\n",
        "    sig2 = np.asarray([angular_instability_sigma2(wi, w_global) for wi in local_ws], float)\n",
        "    F = np.asarray([avg_state_fidelity(qnn_ref, X_ref, w_global, wi, x_scale=cfg.x_scale) for wi in local_ws], float)\n",
        "\n",
        "    q = (F ** float(cfg.qos_alpha)) / (\n",
        "        (tau + float(cfg.qos_eps)) ** float(cfg.qos_gamma) *\n",
        "        (sig2 + float(cfg.qos_eps)) ** float(cfg.qos_delta)\n",
        "    )\n",
        "    q = np.clip(q, float(cfg.qos_clip_min), float(cfg.qos_clip_max))\n",
        "    return {\"F\": F, \"tau\": tau, \"sig2\": sig2, \"q\": q}\n",
        "\n",
        "# ============================================================\n",
        "# Aggregation helpers\n",
        "# ============================================================\n",
        "def agg_fedavg(local_ws: List[np.ndarray], shard_sizes: List[int]) -> np.ndarray:\n",
        "    return np.mean(np.stack(local_ws, axis=0), axis=0)\n",
        "\n",
        "\n",
        "def agg_fedavg_weighted(local_ws: List[np.ndarray], shard_sizes: List[int]) -> np.ndarray:\n",
        "    W = np.asarray(shard_sizes, float)\n",
        "    W = W / (W.sum() + 1e-12)\n",
        "    A = np.stack(local_ws, axis=0)\n",
        "    return (W[:, None] * A).sum(axis=0)\n",
        "\n",
        "\n",
        "def agg_circular_weighted(local_ws: List[np.ndarray], shard_sizes: List[int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Review simplification:\n",
        "      Treat all ansatz weights as angular parameters.\n",
        "      For RealAmplitudes, weights correspond to rotation angles.\n",
        "    \"\"\"\n",
        "    W = np.asarray(shard_sizes, float)\n",
        "    W = W / (W.sum() + 1e-12)\n",
        "    Theta = np.stack([wrap_to_pi(w) for w in local_ws], axis=0)\n",
        "    S = np.sum(W[:, None] * np.sin(Theta), axis=0)\n",
        "    C = np.sum(W[:, None] * np.cos(Theta), axis=0)\n",
        "    return np.arctan2(S, C)\n",
        "\n",
        "\n",
        "AGG_MAP = {\n",
        "    \"fedavg\": agg_fedavg,\n",
        "    \"fedavg_weighted\": agg_fedavg_weighted,\n",
        "    \"linear_weighted\": agg_fedavg_weighted,      # alias for clarity in plots/papers\n",
        "    \"circular_weighted\": agg_circular_weighted,\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QNN evaluation\n",
        "# ============================================================\n",
        "def qnn_predict_proba(qnn: EstimatorQNN, X: np.ndarray, w: np.ndarray, x_scale: float = np.pi) -> np.ndarray:\n",
        "    Xs = np.asarray(X, float) * x_scale\n",
        "    exp = qnn.forward(Xs, w)\n",
        "    exp = np.asarray(exp, float).reshape(-1)\n",
        "    p1 = (1.0 - exp) / 2.0\n",
        "    return np.vstack([1.0 - p1, p1]).T\n",
        "\n",
        "\n",
        "def eval_candidate(qnn_ref: EstimatorQNN, X_eval, y_eval, w, x_scale):\n",
        "    probs = qnn_predict_proba(qnn_ref, X_eval, w, x_scale=x_scale)\n",
        "    yhat = np.argmax(probs, axis=1)\n",
        "    acc = float(accuracy_score(y_eval, yhat))\n",
        "    loss = float(log_loss(y_eval, probs, labels=[0, 1]))\n",
        "    return acc, loss\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Geometry diagnostics + risk + rho\n",
        "# ============================================================\n",
        "def _unwrap_to_ref(A: np.ndarray) -> np.ndarray:\n",
        "    K, D = A.shape\n",
        "    out = A.copy()\n",
        "    ref = out[0]\n",
        "    for i in range(1, K):\n",
        "        delta = out[i] - ref\n",
        "        out[i] = ref + ((delta + np.pi) % (2*np.pi) - np.pi)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _min_covering_arc_length(angles_1d: np.ndarray) -> float:\n",
        "    a = np.sort(angles_1d)\n",
        "    gaps = np.diff(a, append=a[0] + 2*np.pi)\n",
        "    max_gap = np.max(gaps)\n",
        "    return float(2*np.pi - max_gap)\n",
        "\n",
        "\n",
        "def _geodesic_sse(angles: np.ndarray, center: np.ndarray, W: np.ndarray) -> float:\n",
        "    diff = wrap_to_pi(angles - center)\n",
        "    sse_per_client = np.sum(diff**2, axis=1)\n",
        "    return float(np.sum(W * sse_per_client))\n",
        "\n",
        "\n",
        "def angle_diagnostics(local_ws: List[np.ndarray], shard_sizes: List[int]) -> dict:\n",
        "    A_raw = np.stack(local_ws, axis=0)\n",
        "    A = wrap_to_pi(A_raw)\n",
        "\n",
        "    W = np.asarray(shard_sizes, float)\n",
        "    W = W / (W.sum() + 1e-12)\n",
        "\n",
        "    C = np.sum(W[:, None] * np.cos(A), axis=0)\n",
        "    S = np.sum(W[:, None] * np.sin(A), axis=0)\n",
        "    R = np.sqrt(C**2 + S**2)\n",
        "\n",
        "    R_mean = float(np.mean(R))\n",
        "    R_min  = float(np.min(R))\n",
        "\n",
        "    mu_circ = np.arctan2(S, C)\n",
        "\n",
        "    A_unwrap = _unwrap_to_ref(A)\n",
        "    mu_lin_unwrapped = np.sum(W[:, None] * A_unwrap, axis=0)\n",
        "    mu_lin = wrap_to_pi(mu_lin_unwrapped)\n",
        "\n",
        "    sse_circ = _geodesic_sse(A, mu_circ, W)\n",
        "    sse_lin  = _geodesic_sse(A, mu_lin,  W)\n",
        "    sse_gap  = float(sse_lin - sse_circ)\n",
        "\n",
        "    cover_lengths = np.array([_min_covering_arc_length(A[:, j]) for j in range(A.shape[1])])\n",
        "    straddle_frac = float(np.mean(cover_lengths > np.pi))\n",
        "\n",
        "    gap = np.abs(wrap_to_pi(mu_lin - mu_circ))\n",
        "    disagreement_rad = float(np.mean(gap))\n",
        "    disagreement_deg = float(np.rad2deg(disagreement_rad))\n",
        "\n",
        "    return {\n",
        "        \"R_mean\": R_mean,\n",
        "        \"R_min\": R_min,\n",
        "        \"straddle_frac\": straddle_frac,\n",
        "        \"sse_geo_gap\": sse_gap,\n",
        "        \"disagreement_rad\": disagreement_rad,\n",
        "        \"disagreement_deg\": disagreement_deg,\n",
        "        \"mu_circ\": mu_circ,\n",
        "        \"mu_lin\":  mu_lin,\n",
        "    }\n",
        "\n",
        "\n",
        "def geometry_risk_from_diag(diag: dict, a: float = 0.50, b: float = 0.30, c: float = 0.20) -> float:\n",
        "    R_mean = float(diag.get(\"R_mean\", 0.0))\n",
        "    straddle = float(diag.get(\"straddle_frac\", 0.0))\n",
        "    gap = float(diag.get(\"sse_geo_gap\", 0.0))\n",
        "\n",
        "    gap_pos = max(0.0, gap)\n",
        "    gap_term = gap_pos / (gap_pos + 1.0)\n",
        "\n",
        "    risk = a * (1.0 - R_mean) + b * straddle + c * gap_term\n",
        "    return float(np.clip(risk, 0.0, 1.0))\n",
        "\n",
        "\n",
        "def rho_from_risk(risk: float, lam: float = 8.0, center: float = 0.5) -> float:\n",
        "    return float(1.0 / (1.0 + np.exp(-lam * (risk - center))))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Streaming CSV helpers\n",
        "# ============================================================\n",
        "def append_row_csv(row: Dict[str, Any], csv_path: Path):\n",
        "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df = pd.DataFrame([row])\n",
        "    header = not csv_path.exists()\n",
        "    df.to_csv(csv_path, mode=\"a\", header=header, index=False)\n",
        "\n",
        "\n",
        "def append_rows_csv(rows: List[Dict[str, Any]], csv_path: Path):\n",
        "    if not rows:\n",
        "        return\n",
        "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df = pd.DataFrame(rows)\n",
        "    header = not csv_path.exists()\n",
        "    df.to_csv(csv_path, mode=\"a\", header=header, index=False)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Federated loop config\n",
        "# ============================================================\n",
        "@dataclass\n",
        "class FLConfig:\n",
        "    num_clients: int = 10\n",
        "    rounds: int = 50\n",
        "    seed: int = 2025\n",
        "\n",
        "    # partition: \"iid\", \"label_skew\", \"dirichlet\"\n",
        "    partition: str = \"label_skew\"\n",
        "    dirichlet_alpha: float = 0.3\n",
        "\n",
        "    # agg: \"linear_weighted\", \"circular_weighted\", \"auto_pick\"\n",
        "    agg_mode: str = \"circular_weighted\"\n",
        "\n",
        "    maxiter_local: int = 10\n",
        "    x_scale: float = np.pi\n",
        "    optimizer: str = \"SPSA\"\n",
        "    opt_kwargs: Optional[dict] = None\n",
        "\n",
        "    # Validation split inside training\n",
        "    val_size: float = 0.15\n",
        "    val_seed: int = 42\n",
        "\n",
        "    # AutoPick decision policy\n",
        "    autopick_policy: str = \"loss\"  # \"loss\" or \"geometry\"\n",
        "    rho_threshold: float = 0.5\n",
        "\n",
        "    # Regime amplifiers\n",
        "    local_init_noise: float = 0.0  # try 0.05–0.2\n",
        "\n",
        "    angle_stress: bool = False\n",
        "    angle_stress_shift: float = 2.8  # radians (~160°)\n",
        "\n",
        "    # New parameter to ensure clients have both classes in Dirichlet partitioning\n",
        "    min_samples_per_class_per_client: int = 1\n",
        "\n",
        "    # Streaming save options\n",
        "    stream_save: bool = True\n",
        "    save_clients_each_round: bool = False\n",
        "\n",
        "\n",
        "\n",
        "    # QoS-aware aggregation (for ablations): weights from fidelity, latency, and instability\n",
        "    use_qos_weights: bool = False\n",
        "    qos_alpha: float = 1.0     # fidelity exponent\n",
        "    qos_gamma: float = 1.0     # latency exponent\n",
        "    qos_delta: float = 1.0     # instability exponent\n",
        "    qos_eps: float = 1e-6\n",
        "    qos_ref_samples: int = 5   # number of reference inputs for fidelity (kept small for speed)\n",
        "    qos_combine_with_data: bool = True  # multiply QoS score by shard_size weights\n",
        "    qos_clip_min: float = 1e-6\n",
        "    qos_clip_max: float = 1e6\n",
        "\n",
        "# ============================================================\n",
        "# Federated training loop\n",
        "# ============================================================\n",
        "def run_federated_qnn_builtins(\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    cfg: FLConfig,\n",
        "    rounds_csv_path: Optional[Path] = None,\n",
        "    clients_csv_path: Optional[Path] = None,\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    rng = np.random.default_rng(cfg.seed)\n",
        "\n",
        "    if cfg.agg_mode != \"auto_pick\" and cfg.agg_mode not in AGG_MAP:\n",
        "        raise ValueError(\n",
        "            f\"Unknown agg_mode={cfg.agg_mode}. Choose from {list(AGG_MAP.keys())} + ['auto_pick']\"\n",
        "        )\n",
        "    aggregator = AGG_MAP.get(cfg.agg_mode, None)\n",
        "\n",
        "    # ---- Create validation set from training ----\n",
        "    X_tr, X_val, y_tr, y_val = split_train_val(\n",
        "        X_train, y_train, val_size=cfg.val_size, seed=cfg.val_seed\n",
        "    )\n",
        "\n",
        "\n",
        "    # ---- Reference inputs for fidelity (fixed across rounds for comparability) ----\n",
        "    X_ref = None\n",
        "    if cfg.use_qos_weights:\n",
        "        m_ref = int(min(max(1, cfg.qos_ref_samples), len(X_val)))\n",
        "        ref_idx = rng.choice(len(X_val), size=m_ref, replace=False)\n",
        "        X_ref = X_val[ref_idx]\n",
        "\n",
        "    # ---- Client shards are built from TRAIN-ONLY subset ----\n",
        "    if cfg.partition == \"iid\":\n",
        "        shards = shard_iid(X_tr, y_tr, cfg.num_clients, seed=cfg.seed)\n",
        "    elif cfg.partition == \"dirichlet\":\n",
        "        shards = shard_dirichlet(\n",
        "            X_tr, y_tr, cfg.num_clients,\n",
        "            alpha=cfg.dirichlet_alpha, seed=cfg.seed,\n",
        "            min_samples_per_class_per_client=cfg.min_samples_per_class_per_client\n",
        "        )\n",
        "    else:\n",
        "        shards = shard_label_skew(\n",
        "            X_tr, y_tr, cfg.num_clients,\n",
        "            min_per_client=max(40, len(X_tr)//cfg.num_clients),\n",
        "            seed=cfg.seed\n",
        "        )\n",
        "\n",
        "    num_features = X_train.shape[1]\n",
        "\n",
        "    qnn_ref = build_estimator_qnn(num_features, fm_reps=1, an_reps=2)\n",
        "    D = len(qnn_ref.weight_params)\n",
        "\n",
        "    # Initialize global weights\n",
        "    w_global = 0.10 * rng.standard_normal(D)\n",
        "\n",
        "    rounds_rows: List[Dict[str, Any]] = []\n",
        "    client_rows_all: List[Dict[str, Any]] = []\n",
        "\n",
        "    print(\n",
        "        f\"=== QFL | agg='{cfg.agg_mode}' | policy='{cfg.autopick_policy}' | \"\n",
        "        f\"partition='{cfg.partition}' (α={cfg.dirichlet_alpha}) | \"\n",
        "        f\"clients={cfg.num_clients} rounds={cfg.rounds} | \"\n",
        "        f\"init_noise={cfg.local_init_noise} | stress={cfg.angle_stress} ===\"\n",
        "    )\n",
        "\n",
        "    for r in range(1, cfg.rounds + 1):\n",
        "        t0 = time.perf_counter()\n",
        "\n",
        "        local_ws: List[np.ndarray] = []\n",
        "        shard_sizes: List[int] = []\n",
        "        local_times: List[float] = []\n",
        "\n",
        "        client_rows_this_round: List[Dict[str, Any]] = []\n",
        "\n",
        "        # ---------------------------\n",
        "        # Local training per client\n",
        "        # ---------------------------\n",
        "        for cid, (Xi, yi) in enumerate(shards):\n",
        "\n",
        "            # client-specific init noise to encourage divergence\n",
        "            w0 = w_global.copy()\n",
        "            if cfg.local_init_noise > 0:\n",
        "                w0 = w0 + cfg.local_init_noise * rng.standard_normal(len(w0))\n",
        "\n",
        "            clf = make_classifier(\n",
        "                num_features=num_features,\n",
        "                initial_point=w0,\n",
        "                maxiter=cfg.maxiter_local,\n",
        "                optimizer=cfg.optimizer,\n",
        "                opt_kwargs=cfg.opt_kwargs,\n",
        "            )\n",
        "\n",
        "            t_c0 = time.perf_counter()\n",
        "            clf.fit(Xi * cfg.x_scale, yi)\n",
        "            dt_c = time.perf_counter() - t_c0\n",
        "            local_times.append(float(dt_c))\n",
        "\n",
        "            w_local = extract_weights(clf)\n",
        "            if len(w_local) != D:\n",
        "                raise RuntimeError(f\"[Round {r}] Client {cid}: weight dim mismatch ({len(w_local)} != {D})\")\n",
        "\n",
        "            # controlled geometry stress-test (mechanism demo)\n",
        "            if cfg.angle_stress:\n",
        "                direction = +1.0 if (cid % 2 == 0) else -1.0\n",
        "                w_local = wrap_to_pi(w_local + direction * cfg.angle_stress_shift)\n",
        "\n",
        "            local_ws.append(w_local)\n",
        "            shard_sizes.append(len(yi))\n",
        "\n",
        "            # --- optional client eval ---\n",
        "            probs_tr_loc = qnn_predict_proba(qnn_ref, Xi,     w_local, x_scale=cfg.x_scale)\n",
        "            probs_te_loc = qnn_predict_proba(qnn_ref, X_test, w_local, x_scale=cfg.x_scale)\n",
        "            yhat_tr_loc = np.argmax(probs_tr_loc, axis=1)\n",
        "            yhat_te_loc = np.argmax(probs_te_loc, axis=1)\n",
        "\n",
        "            c_row = {\n",
        "                \"round\": r,\n",
        "                \"client_id\": cid,\n",
        "                \"shard_size\": int(len(yi)),\n",
        "                \"train_acc_local\": float(accuracy_score(yi, yhat_tr_loc)),\n",
        "                \"test_acc_local\":  float(accuracy_score(y_test, yhat_te_loc)),\n",
        "                \"train_loss_local\": float(log_loss(yi,     probs_tr_loc, labels=[0, 1])),\n",
        "                \"test_loss_local\":  float(log_loss(y_test, probs_te_loc, labels=[0, 1])),\n",
        "                \"time_sec_local\":   float(dt_c),\n",
        "            }\n",
        "            client_rows_this_round.append(c_row)\n",
        "\n",
        "\n",
        "        # ---------------------------\n",
        "        # QoS weighting (optional)\n",
        "        # ---------------------------\n",
        "        weights_for_agg = np.asarray(shard_sizes, float)\n",
        "\n",
        "        qos = None\n",
        "        if cfg.use_qos_weights:\n",
        "            qos = compute_qos_scores(\n",
        "                local_ws=local_ws,\n",
        "                w_global=w_global,\n",
        "                local_times=local_times,\n",
        "                qnn_ref=qnn_ref,\n",
        "                X_ref=X_ref,\n",
        "                cfg=cfg,\n",
        "            )\n",
        "            if cfg.qos_combine_with_data:\n",
        "                weights_for_agg = weights_for_agg * qos[\"q\"]\n",
        "            else:\n",
        "                weights_for_agg = qos[\"q\"].copy()\n",
        "\n",
        "            # normalize client weights for logging\n",
        "            wnorm = weights_for_agg / (weights_for_agg.sum() + 1e-12)\n",
        "            for i, crow in enumerate(client_rows_this_round):\n",
        "                crow.update({\n",
        "                    \"fidelity\": float(qos[\"F\"][i]),\n",
        "                    \"instability_sigma2\": float(qos[\"sig2\"][i]),\n",
        "                    \"qos_score\": float(qos[\"q\"][i]),\n",
        "                    \"agg_weight\": float(wnorm[i]),\n",
        "                })\n",
        "\n",
        "        # ---------------------------\n",
        "        # Diagnostics (before aggregation)\n",
        "        # ---------------------------\n",
        "        diag = angle_diagnostics(local_ws, weights_for_agg.tolist())\n",
        "        r_t = geometry_risk_from_diag(diag)\n",
        "        rho_t = rho_from_risk(r_t)\n",
        "\n",
        "        print(\n",
        "            f\"    [diag] R̄={diag['R_mean']:.3f} | Rmin={diag['R_min']:.3f} | \"\n",
        "            f\"straddle_frac={diag['straddle_frac']:.2f} | ΔSSE_geo={diag['sse_geo_gap']:.4f} | \"\n",
        "            f\"geom_err={diag['disagreement_deg']:.2f}° | r_t={r_t:.3f} | ρ_t={rho_t:.3f}\"\n",
        "        )\n",
        "\n",
        "        # ---------------------------\n",
        "        # Aggregate to new global\n",
        "        # ---------------------------\n",
        "        picked = None\n",
        "\n",
        "        # Candidate stats (for logging)\n",
        "        val_acc_lin = val_loss_lin = None\n",
        "        val_acc_circ = val_loss_circ = None\n",
        "        test_acc_lin = test_loss_lin = None\n",
        "        test_acc_circ = test_loss_circ = None\n",
        "\n",
        "        if cfg.agg_mode == \"auto_pick\":\n",
        "            # candidates\n",
        "            w_lin  = agg_fedavg_weighted(local_ws, weights_for_agg.tolist())\n",
        "            w_circ = agg_circular_weighted(local_ws, weights_for_agg.tolist())\n",
        "\n",
        "            # evaluate on validation for selection logging\n",
        "            val_acc_lin,  val_loss_lin  = eval_candidate(qnn_ref, X_val, y_val, w_lin,  cfg.x_scale)\n",
        "            val_acc_circ, val_loss_circ = eval_candidate(qnn_ref, X_val, y_val, w_circ, cfg.x_scale)\n",
        "\n",
        "            # also evaluate on test for reporting\n",
        "            test_acc_lin,  test_loss_lin  = eval_candidate(qnn_ref, X_test, y_test, w_lin,  cfg.x_scale)\n",
        "            test_acc_circ, test_loss_circ = eval_candidate(qnn_ref, X_test, y_test, w_circ, cfg.x_scale)\n",
        "\n",
        "            policy = str(cfg.autopick_policy).lower().strip()\n",
        "\n",
        "            if policy == \"loss\":\n",
        "                use_circ = (val_loss_circ <= val_loss_lin)\n",
        "                rule_used = \"loss(val)\"\n",
        "            elif policy == \"geometry\":\n",
        "                use_circ = (rho_t >= cfg.rho_threshold)\n",
        "                rule_used = \"geometry(ρ)\"\n",
        "            else:\n",
        "                raise ValueError(\"autopick_policy must be 'loss' or 'geometry'\")\n",
        "\n",
        "            w_global = w_circ if use_circ else w_lin\n",
        "            picked = \"circular\" if use_circ else \"linear\"\n",
        "\n",
        "            print(\n",
        "                f\"    ↳ AutoPick-{rule_used} chose: {picked} | \"\n",
        "                f\"val_loss_lin={val_loss_lin:.4f}, val_loss_circ={val_loss_circ:.4f} | \"\n",
        "                f\"ρ_t={rho_t:.3f}\"\n",
        "            )\n",
        "        else:\n",
        "            w_global = aggregator(local_ws, weights_for_agg.tolist())  # type: ignore\n",
        "            picked = cfg.agg_mode\n",
        "            print(f\"    ↳ agg used: {cfg.agg_mode}\")\n",
        "\n",
        "        # ---------------------------\n",
        "        # Global evaluation\n",
        "        # ---------------------------\n",
        "        probs_tr = qnn_predict_proba(qnn_ref, X_tr,   w_global, x_scale=cfg.x_scale)\n",
        "        probs_va = qnn_predict_proba(qnn_ref, X_val,  w_global, x_scale=cfg.x_scale)\n",
        "        probs_te = qnn_predict_proba(qnn_ref, X_test, w_global, x_scale=cfg.x_scale)\n",
        "\n",
        "        ytr_pred = np.argmax(probs_tr, axis=1)\n",
        "        yva_pred = np.argmax(probs_va, axis=1)\n",
        "        yte_pred = np.argmax(probs_te, axis=1)\n",
        "\n",
        "        train_acc  = float(accuracy_score(y_tr,   ytr_pred))\n",
        "        val_acc    = float(accuracy_score(y_val,  yva_pred))\n",
        "        test_acc   = float(accuracy_score(y_test, yte_pred))\n",
        "\n",
        "        train_loss = float(log_loss(y_tr,   probs_tr, labels=[0, 1]))\n",
        "        val_loss   = float(log_loss(y_val,  probs_va, labels=[0, 1]))\n",
        "        test_loss  = float(log_loss(y_test, probs_te, labels=[0, 1]))\n",
        "\n",
        "        dt = time.perf_counter() - t0\n",
        "\n",
        "        print(\n",
        "            f\"[Round {r:02d}] TrainAcc={train_acc:.3f} | ValAcc={val_acc:.3f} | TestAcc={test_acc:.3f} | \"\n",
        "            f\"TrainLoss={train_loss:.4f} | ValLoss={val_loss:.4f} | TestLoss={test_loss:.4f} | \"\n",
        "            f\"Time={dt:.2f}s | picked={picked}\"\n",
        "        )\n",
        "\n",
        "        # ---------------------------\n",
        "        # Per-round row\n",
        "        # ---------------------------\n",
        "        row = {\n",
        "            \"round\": r,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"test_acc\": test_acc,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"test_loss\": test_loss,\n",
        "            \"time_sec\": float(dt),\n",
        "\n",
        "            # QoS summaries (when enabled)\n",
        "            \"qos_enabled\": bool(cfg.use_qos_weights),\n",
        "            \"qos_alpha\": float(cfg.qos_alpha),\n",
        "            \"qos_gamma\": float(cfg.qos_gamma),\n",
        "            \"qos_delta\": float(cfg.qos_delta),\n",
        "            \"mean_fidelity\": None,\n",
        "            \"mean_latency\": None,\n",
        "            \"mean_instability\": None,\n",
        "\n",
        "            \"agg_mode\": cfg.agg_mode,\n",
        "            \"autopick_policy\": cfg.autopick_policy if cfg.agg_mode == \"auto_pick\" else \"fixed\",\n",
        "            \"picked\": picked,\n",
        "\n",
        "            # partition + regime knobs for traceability\n",
        "            \"partition\": cfg.partition,\n",
        "            \"dirichlet_alpha\": float(cfg.dirichlet_alpha),\n",
        "            \"local_init_noise\": float(cfg.local_init_noise),\n",
        "            \"angle_stress\": bool(cfg.angle_stress),\n",
        "            \"angle_stress_shift\": float(cfg.angle_stress_shift),\n",
        "\n",
        "            # geometry diagnostics\n",
        "            \"resultant_mean\": float(diag[\"R_mean\"]),\n",
        "            \"resultant_min\":  float(diag[\"R_min\"]),\n",
        "            \"straddle_frac\":  float(diag[\"straddle_frac\"]),\n",
        "            \"delta_SSE_geo\":  float(diag[\"sse_geo_gap\"]),\n",
        "            \"geom_err_deg\":   float(diag[\"disagreement_deg\"]),\n",
        "            \"geom_err_rad\":   float(diag[\"disagreement_rad\"]),\n",
        "            \"risk_t\": float(r_t),\n",
        "            \"rho_t\":  float(rho_t),\n",
        "\n",
        "            # selection threshold (for traceability)\n",
        "            \"rho_threshold\": float(cfg.rho_threshold),\n",
        "            \"val_size\": float(cfg.val_size),\n",
        "            \"min_samples_per_class_per_client\": float(cfg.min_samples_per_class_per_client)\n",
        "        }\n",
        "\n",
        "        # candidate logs (useful for plotting why choices differ)\n",
        "        if cfg.agg_mode == \"auto_pick\":\n",
        "            row.update({\n",
        "                \"val_loss_lin\":  None if val_loss_lin  is None else float(val_loss_lin),\n",
        "                \"val_loss_circ\": None if val_loss_circ is None else float(val_loss_circ),\n",
        "                \"val_acc_lin\":   None if val_acc_lin   is None else float(val_acc_lin),\n",
        "                \"val_acc_circ\":  None if val_acc_circ  is None else float(val_acc_circ),\n",
        "\n",
        "                \"test_loss_lin\":  None if test_loss_lin  is None else float(test_loss_lin),\n",
        "                \"test_loss_circ\": None if test_loss_circ is None else float(test_loss_circ),\n",
        "                \"test_acc_lin\":   None if test_acc_lin   is None else float(test_acc_lin),\n",
        "                \"test_acc_circ\":  None if test_acc_circ  is None else float(test_acc_circ),\n",
        "            })\n",
        "\n",
        "\n",
        "        # Fill QoS summary statistics\n",
        "        if cfg.use_qos_weights and qos is not None:\n",
        "            row[\"mean_fidelity\"] = float(np.mean(qos[\"F\"])) # Removed one ')'\n",
        "            row[\"mean_latency\"] = float(np.mean(qos[\"tau\"])) # Removed one ')'\n",
        "            row[\"mean_instability\"] = float(np.mean(qos[\"sig2\"])) # Removed one ')'\n",
        "\n",
        "        rounds_rows.append(row)\n",
        "\n",
        "        # ---------------------------\n",
        "        # Streaming save\n",
        "        # ---------------------------\n",
        "        if cfg.stream_save and rounds_csv_path is not None:\n",
        "            append_row_csv(row, rounds_csv_path)\n",
        "\n",
        "        if cfg.save_clients_each_round and clients_csv_path is not None:\n",
        "            append_rows_csv(client_rows_this_round, clients_csv_path)\n",
        "\n",
        "        client_rows_all.extend(client_rows_this_round)\n",
        "\n",
        "    return {\n",
        "        \"w_global\": w_global,\n",
        "        \"rows\": rounds_rows,\n",
        "        \"client_rows\": client_rows_all,\n",
        "        \"config\": cfg.__dict__,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Save metadata (once)\n",
        "# ============================================================\n",
        "def save_meta(meta: Dict[str, Any], path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main (configuration-driven)\n",
        "# ============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    import random\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # ---------------- USER CHOICES ----------------\n",
        "    FAST_CPU_MODE = True\n",
        "\n",
        "    # Multi-seed protocol (5 is a standard minimum; use 10 for stronger evidence)\n",
        "    SEEDS = [1, 2, 3, 4]\n",
        "\n",
        "\n",
        "    # Choose dataset:\n",
        "    DATASET_CHOICE = \"biomarker_leukemia\"\n",
        "    # options: \"breast_cancer\", \"clinical\", \"both\", \"biomarker_leukemia\", \"biomarker_arcene\", \"motion_nuscenes_mini\", or \"openml:<id>\"\n",
        "\n",
        "    # Feature dimension (= number of qubits). 4 is a good CPU default.\n",
        "    PCA_K = 4\n",
        "    # Cap samples to keep Qiskit simulation practical (set None to disable).\n",
        "    MAX_SAMPLES = 400\n",
        "\n",
        "    # nuScenes (requires downloaded data + nuscenes-devkit). Used when DATASET_CHOICE == \"motion_nuscenes_mini\"\n",
        "    NUSCENES_ROOT = os.environ.get(\"NUSCENES_ROOT\", \"./nuscenes\")\n",
        "    NUSCENES_VERSION = \"v1.0-mini\"\n",
        "    NUSCENES_CATEGORY_PREFIX = \"vehicle\"\n",
        "\n",
        "\n",
        "    # Aggregators you want to compare\n",
        "    AGG_MODES_TO_RUN = [\n",
        "        #\"linear_weighted\",\n",
        "        \"circular_weighted\",\n",
        "        #\"auto_pick\",\n",
        "    ]\n",
        "\n",
        "    # AutoPick policies to run (only used when agg_mode == \"auto_pick\")\n",
        "    AUTOPICK_POLICIES = [\"loss\", \"geometry\"]\n",
        "\n",
        "    # Partition regimes\n",
        "    PARTITIONS_TO_RUN = [\n",
        "        #(\"label_skew\", None),\n",
        "         #Dirichlet sweeps: include multiple alphas to show regime shifts\n",
        "        #(\"dirichlet\", 5.0),\n",
        "        #(\"dirichlet\", 0.5),\n",
        "        (\"dirichlet\", 0.1),\n",
        "    ]\n",
        "\n",
        "    # Regime amplifiers\n",
        "    LOCAL_INIT_NOISE_LIST = [0.1]  # 0.0 baseline, 0.1 encourages divergence\n",
        "    ANGLE_STRESS_LIST = [False]  # True = controlled mechanism demo\n",
        "\n",
        "    NUM_CLIENTS = 10\n",
        "    ROUNDS = 200\n",
        "    MAXITER_LOCAL = 20\n",
        "    X_SCALE = np.pi\n",
        "\n",
        "    if FAST_CPU_MODE:\n",
        "        # Practical CPU defaults (safe to expand later)\n",
        "        NUM_CLIENTS = 10\n",
        "        ROUNDS = 200\n",
        "        MAXITER_LOCAL = 20\n",
        "        X_SCALE = np.pi  # keep angles in [-pi, pi]\n",
        "\n",
        "    # Validation split inside training\n",
        "    VAL_SIZE = 0.15\n",
        "    VAL_SEED = 42\n",
        "\n",
        "    # Rho threshold for geometry policy\n",
        "    RHO_THRESHOLD = 0.5\n",
        "\n",
        "    # Clinical CSV path\n",
        "    CLINICAL_CSV = \"/content/drive/MyDrive/data/BrEaST-Lesions-USG-Clinical.csv\"\n",
        "\n",
        "    # Optimizer defaults\n",
        "    OPT_BREAST = (\"SPSA\", {\"learning_rate\": 0.05, \"perturbation\": 0.1})\n",
        "    OPT_CLIN   = (\"COBYLA\", None)\n",
        "\n",
        "    # Stream-save options\n",
        "    STREAM_SAVE = True\n",
        "    SAVE_CLIENTS_EACH_ROUND = False\n",
        "    CLEAN_OLD_CSVS = True\n",
        "\n",
        "    def set_all_seeds(seed: int):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        try:\n",
        "            import torch\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Keep dataset split fixed for fair method comparison (recommended).\n",
        "    # If want split to vary across seeds, set DATA_SPLIT_SEED = seed inside the seed loop.\n",
        "    DATA_SPLIT_SEED = 42\n",
        "\n",
        "\n",
        "    # ---------------- INTERNAL HELPERS ----------------\n",
        "    def outdir_for(dataset_key: str, agg_mode: str, policy: str,\n",
        "               partition: str, alpha: Optional[float],\n",
        "               init_noise: float, stress: bool, seed: int) -> Path:\n",
        "\n",
        "      alpha_tag = \"\" if alpha is None else f\"_a{alpha}\"\n",
        "      stress_tag = \"_stress\" if stress else \"\"\n",
        "      noise_tag = f\"_n{init_noise}\"\n",
        "      seed_tag = f\"_seed{seed}\"\n",
        "\n",
        "      name = (\n",
        "        f\"QFL_folder/newQosMotivation/{dataset_key}_\"\n",
        "        f\"{agg_mode}_\"\n",
        "        f\"{policy}_\"\n",
        "        f\"{partition}{alpha_tag}{noise_tag}{stress_tag}{seed_tag}\"\n",
        "      )\n",
        "      return get_outdir_in_drive(name)\n",
        "\n",
        "\n",
        "    def rounds_csv_path(outdir: Path, dataset_key: str) -> Path:\n",
        "        return outdir / f\"round_metrics_{dataset_key}_clients{NUM_CLIENTS}_rounds{ROUNDS}.csv\"\n",
        "\n",
        "    def clients_csv_path(outdir: Path, dataset_key: str) -> Path:\n",
        "        return outdir / f\"client_metrics_{dataset_key}_clients{NUM_CLIENTS}_rounds{ROUNDS}.csv\"\n",
        "\n",
        "    def maybe_clean(*paths: Path):\n",
        "        if not CLEAN_OLD_CSVS:\n",
        "            return\n",
        "        for p in paths:\n",
        "            if p.exists():\n",
        "                p.unlink()\n",
        "\n",
        "    def run_one(dataset_key: str,\n",
        "            X_train, X_test, y_train, y_test,\n",
        "            agg_mode: str,\n",
        "            policy: str,\n",
        "            partition: str,\n",
        "            alpha: Optional[float],\n",
        "            init_noise: float,\n",
        "            stress: bool,\n",
        "            optimizer_name: str,\n",
        "            opt_kwargs: Optional[dict],\n",
        "            seed: int):\n",
        "\n",
        "        outdir = outdir_for(dataset_key, agg_mode, policy, partition, alpha, init_noise, stress, seed)\n",
        "        r_csv = rounds_csv_path(outdir, dataset_key)\n",
        "        c_csv = clients_csv_path(outdir, dataset_key)\n",
        "\n",
        "        maybe_clean(r_csv, c_csv)\n",
        "\n",
        "\n",
        "        cfg = FLConfig(\n",
        "            num_clients=NUM_CLIENTS,\n",
        "            rounds=ROUNDS,\n",
        "            seed=seed,          # <<<< changed\n",
        "            val_seed=42,        # keep fixed for comparable validation split (optional)\n",
        "\n",
        "\n",
        "            partition=partition,\n",
        "            dirichlet_alpha=float(alpha) if alpha is not None else 0.3,\n",
        "\n",
        "            agg_mode=agg_mode,\n",
        "            maxiter_local=MAXITER_LOCAL,\n",
        "            x_scale=X_SCALE,\n",
        "            optimizer=optimizer_name,\n",
        "            opt_kwargs=opt_kwargs,\n",
        "\n",
        "            val_size=VAL_SIZE,\n",
        "\n",
        "\n",
        "            autopick_policy=policy,\n",
        "            rho_threshold=RHO_THRESHOLD,\n",
        "\n",
        "            local_init_noise=init_noise,\n",
        "            angle_stress=stress,\n",
        "            angle_stress_shift=2.8,\n",
        "\n",
        "            min_samples_per_class_per_client=1, # Ensure this is set\n",
        "\n",
        "            stream_save=STREAM_SAVE,\n",
        "            save_clients_each_round=SAVE_CLIENTS_EACH_ROUND,\n",
        "        )\n",
        "\n",
        "        print(f\"\\n=== RUN | {dataset_key} | agg={agg_mode} | policy={policy} | \"\n",
        "              f\"partition={partition} | alpha={alpha} | init_noise={init_noise} | stress={stress} ===\")\n",
        "\n",
        "        out = run_federated_qnn_builtins(\n",
        "            X_train, y_train, X_test, y_test,\n",
        "            cfg,\n",
        "            rounds_csv_path=r_csv,\n",
        "            clients_csv_path=c_csv if SAVE_CLIENTS_EACH_ROUND else None\n",
        "        )\n",
        "\n",
        "        save_meta({\"dataset\": dataset_key, **out[\"config\"]}, outdir / \"run_metadata.json\")\n",
        "        np.save(outdir / \"w_global.npy\", out[\"w_global\"])\n",
        "\n",
        "        print(f\"Saved rounds CSV to: {r_csv}\")\n",
        "        if SAVE_CLIENTS_EACH_ROUND:\n",
        "            print(f\"Saved client CSV to: {c_csv}\")\n",
        "\n",
        "        return out\n",
        "\n",
        "# ---------------- DATASET RUNNERS ----------------\n",
        "\n",
        "def run_breast():\n",
        "    # Keep global split fixed across seeds for fair comparison\n",
        "    X_train, X_test, y_train, y_test = load_breast_cancer(pca_k=PCA_K, test_size=0.30, seed=DATA_SPLIT_SEED)\n",
        "    opt_name, opt_kwargs = OPT_BREAST\n",
        "\n",
        "    for seed in SEEDS:\n",
        "        set_all_seeds(seed)\n",
        "\n",
        "        for partition, alpha in PARTITIONS_TO_RUN:\n",
        "            for init_noise in LOCAL_INIT_NOISE_LIST:\n",
        "                for stress in ANGLE_STRESS_LIST:\n",
        "                    for agg_mode in AGG_MODES_TO_RUN:\n",
        "                        if agg_mode == \"auto_pick\":\n",
        "                            for pol in AUTOPICK_POLICIES:\n",
        "                                run_one(\"breast_cancer\", X_train, X_test, y_train, y_test,\n",
        "                                        agg_mode, pol, partition, alpha, init_noise, stress,\n",
        "                                        opt_name, opt_kwargs, seed=seed)\n",
        "                        else:\n",
        "                            run_one(\"breast_cancer\", X_train, X_test, y_train, y_test,\n",
        "                                    agg_mode, \"fixed\", partition, alpha, init_noise, stress,\n",
        "                                    opt_name, opt_kwargs, seed=seed)\n",
        "\n",
        "\n",
        "def run_clinical():\n",
        "    X_train, X_test, y_train, y_test = load_clinical_csv(\n",
        "        CLINICAL_CSV, test_size=0.45, pca_k=PCA_K, seed=DATA_SPLIT_SEED\n",
        "    )\n",
        "    opt_name, opt_kwargs = OPT_CLIN\n",
        "\n",
        "    for seed in SEEDS:\n",
        "        set_all_seeds(seed)\n",
        "\n",
        "        for partition, alpha in PARTITIONS_TO_RUN:\n",
        "            for init_noise in LOCAL_INIT_NOISE_LIST:\n",
        "                for stress in ANGLE_STRESS_LIST:\n",
        "                    for agg_mode in AGG_MODES_TO_RUN:\n",
        "                        if agg_mode == \"auto_pick\":\n",
        "                            for pol in AUTOPICK_POLICIES:\n",
        "                                run_one(\"clinical\", X_train, X_test, y_train, y_test,\n",
        "                                        agg_mode, pol, partition, alpha, init_noise, stress,\n",
        "                                        opt_name, opt_kwargs, seed=seed)\n",
        "                        else:\n",
        "                            run_one(\"clinical\", X_train, X_test, y_train, y_test,\n",
        "                                    agg_mode, \"fixed\", partition, alpha, init_noise, stress,\n",
        "                                    opt_name, opt_kwargs, seed=seed)\n",
        "\n",
        "\n",
        "def run_biomarker_leukemia():\n",
        "    X_train, X_test, y_train, y_test = load_biomarker_leukemia(\n",
        "        pca_k=PCA_K, test_size=0.30, seed=DATA_SPLIT_SEED, max_samples=MAX_SAMPLES\n",
        "    )\n",
        "    opt_name, opt_kwargs = OPT_BREAST\n",
        "\n",
        "    for seed in SEEDS:\n",
        "        set_all_seeds(seed)\n",
        "\n",
        "        for partition, alpha in PARTITIONS_TO_RUN:\n",
        "            for init_noise in LOCAL_INIT_NOISE_LIST:\n",
        "                for stress in ANGLE_STRESS_LIST:\n",
        "                    for agg_mode in AGG_MODES_TO_RUN:\n",
        "                        if agg_mode == \"auto_pick\":\n",
        "                            for pol in AUTOPICK_POLICIES:\n",
        "                                run_one(\"biomarker_leukemia\", X_train, X_test, y_train, y_test,\n",
        "                                        agg_mode, pol, partition, alpha, init_noise, stress,\n",
        "                                        opt_name, opt_kwargs, seed=seed)\n",
        "                        else:\n",
        "                            run_one(\"biomarker_leukemia\", X_train, X_test, y_train, y_test,\n",
        "                                    agg_mode, \"fixed\", partition, alpha, init_noise, stress,\n",
        "                                    opt_name, opt_kwargs, seed=seed)\n",
        "\n",
        "\n",
        "def run_biomarker_arcene():\n",
        "    X_train, X_test, y_train, y_test = load_biomarker_arcene(\n",
        "        pca_k=PCA_K, test_size=0.30, seed=DATA_SPLIT_SEED, max_samples=MAX_SAMPLES\n",
        "    )\n",
        "    opt_name, opt_kwargs = OPT_BREAST\n",
        "\n",
        "    for seed in SEEDS:\n",
        "        set_all_seeds(seed)\n",
        "\n",
        "        for partition, alpha in PARTITIONS_TO_RUN:\n",
        "            for init_noise in LOCAL_INIT_NOISE_LIST:\n",
        "                for stress in ANGLE_STRESS_LIST:\n",
        "                    for agg_mode in AGG_MODES_TO_RUN:\n",
        "                        if agg_mode == \"auto_pick\":\n",
        "                            for pol in AUTOPICK_POLICIES:\n",
        "                                run_one(\"biomarker_arcene\", X_train, X_test, y_train, y_test,\n",
        "                                        agg_mode, pol, partition, alpha, init_noise, stress,\n",
        "                                        opt_name, opt_kwargs, seed=seed)\n",
        "                        else:\n",
        "                            run_one(\"biomarker_arcene\", X_train, X_test, y_train, y_test,\n",
        "                                    agg_mode, \"fixed\", partition, alpha, init_noise, stress,\n",
        "                                    opt_name, opt_kwargs, seed=seed)\n",
        "\n",
        "\n",
        "def run_motion_nuscenes_mini():\n",
        "    X_train, X_test, y_train, y_test = load_motion_nuscenes_mini(\n",
        "        dataroot=NUSCENES_ROOT,\n",
        "        version=NUSCENES_VERSION,\n",
        "        category_prefix=NUSCENES_CATEGORY_PREFIX,\n",
        "        pca_k=PCA_K,\n",
        "        test_size=0.30,\n",
        "        seed=DATA_SPLIT_SEED,      # keep split fixed\n",
        "        max_samples=MAX_SAMPLES,\n",
        "    )\n",
        "    opt_name, opt_kwargs = OPT_BREAST\n",
        "\n",
        "    for seed in SEEDS:\n",
        "        set_all_seeds(seed)\n",
        "\n",
        "        for partition, alpha in PARTITIONS_TO_RUN:\n",
        "            for init_noise in LOCAL_INIT_NOISE_LIST:\n",
        "                for stress in ANGLE_STRESS_LIST:\n",
        "                    for agg_mode in AGG_MODES_TO_RUN:\n",
        "                        if agg_mode == \"auto_pick\":\n",
        "                            for pol in AUTOPICK_POLICIES:\n",
        "                                run_one(\"motion_nuscenes_mini\", X_train, X_test, y_train, y_test,\n",
        "                                        agg_mode, pol, partition, alpha, init_noise, stress,\n",
        "                                        opt_name, opt_kwargs, seed=seed)\n",
        "                        else:\n",
        "                            run_one(\"motion_nuscenes_mini\", X_train, X_test, y_train, y_test,\n",
        "                                    agg_mode, \"fixed\", partition, alpha, init_noise, stress,\n",
        "                                    opt_name, opt_kwargs, seed=seed)\n",
        "\n",
        "\n",
        "def run_openml_id(openml_id: int):\n",
        "    # Keep split fixed across seeds\n",
        "    X_train, X_test, y_train, y_test = load_openml_binary(\n",
        "        openml_id, pca_k=PCA_K, test_size=0.30, seed=DATA_SPLIT_SEED, max_samples=MAX_SAMPLES\n",
        "    )\n",
        "    opt_name, opt_kwargs = OPT_BREAST\n",
        "    tag = f\"openml_{openml_id}\"\n",
        "\n",
        "    for seed in SEEDS:  # <<<< missing before\n",
        "        set_all_seeds(seed)\n",
        "\n",
        "        for partition, alpha in PARTITIONS_TO_RUN:\n",
        "            for init_noise in LOCAL_INIT_NOISE_LIST:\n",
        "                for stress in ANGLE_STRESS_LIST:\n",
        "                    for agg_mode in AGG_MODES_TO_RUN:\n",
        "                        if agg_mode == \"auto_pick\":\n",
        "                            for pol in AUTOPICK_POLICIES:\n",
        "                                run_one(tag, X_train, X_test, y_train, y_test,\n",
        "                                        agg_mode, pol, partition, alpha, init_noise, stress,\n",
        "                                        opt_name, opt_kwargs, seed=seed)\n",
        "                        else:\n",
        "                            run_one(tag, X_train, X_test, y_train, y_test,\n",
        "                                    agg_mode, \"fixed\", partition, alpha, init_noise, stress,\n",
        "                                    opt_name, opt_kwargs, seed=seed)\n",
        "\n",
        "\n",
        "# ---------------- EXECUTE CHOSEN ----------------\n",
        "if DATASET_CHOICE == \"breast_cancer\":\n",
        "    run_breast()\n",
        "elif DATASET_CHOICE == \"clinical\":\n",
        "    try:\n",
        "        run_clinical()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"[WARN] Clinical CSV not found at: {CLINICAL_CSV}.\")\n",
        "elif DATASET_CHOICE == \"both\":\n",
        "    run_breast()\n",
        "    try:\n",
        "        run_clinical()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"[WARN] Clinical CSV not found at: {CLINICAL_CSV}.\")\n",
        "elif DATASET_CHOICE == \"biomarker_leukemia\":\n",
        "    run_biomarker_leukemia()\n",
        "elif DATASET_CHOICE == \"biomarker_arcene\":\n",
        "    run_biomarker_arcene()\n",
        "elif DATASET_CHOICE == \"motion_nuscenes_mini\":\n",
        "    run_motion_nuscenes_mini()\n",
        "elif isinstance(DATASET_CHOICE, str) and DATASET_CHOICE.startswith(\"openml:\"):\n",
        "    openml_id = int(DATASET_CHOICE.split(\":\", 1)[1])\n",
        "    run_openml_id(openml_id)\n",
        "else:\n",
        "    raise ValueError(\n",
        "        \"DATASET_CHOICE must be: 'breast_cancer', 'clinical', 'both', \"\n",
        "        \"'biomarker_leukemia', 'biomarker_arcene', 'motion_nuscenes_mini', or 'openml:<id>'.\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vK0R7_cinZu",
        "outputId": "6a5a01ba-7230-4766-b50b-12207047f850"
      },
      "id": "0vK0R7_cinZu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "=== RUN | biomarker_leukemia | agg=circular_weighted | policy=fixed | partition=dirichlet | alpha=0.1 | init_noise=0.1 | stress=False ===\n",
            "=== QFL | agg='circular_weighted' | policy='fixed' | partition='dirichlet' (α=0.1) | clients=10 rounds=200 | init_noise=0.1 | stress=False ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Adjust if your Drive base differs\n",
        "BASE = Path(\"/content/drive/MyDrive/AutopickV2/QFL_folder/newQosMotivation/Seeds\")\n",
        "\n",
        "TAIL_K = 20\n",
        "SAFE_Q = 0.25\n",
        "\n",
        "ACC = \"val_acc\"\n",
        "TEST = \"test_acc\"\n",
        "RISK = \"risk_t\"\n",
        "GEOM = \"geom_err_deg\"\n",
        "\n",
        "def auc_norm(df, col):\n",
        "    x = df[\"round\"].to_numpy(float)\n",
        "    y = df[col].to_numpy(float)\n",
        "    m = np.isfinite(x) & np.isfinite(y)\n",
        "    x, y = x[m], y[m]\n",
        "    if len(x) < 2:\n",
        "        return np.nan\n",
        "    idx = np.argsort(x)\n",
        "    x, y = x[idx], y[idx]\n",
        "    auc = np.trapz(y, x)\n",
        "    denom = x.max() - x.min()\n",
        "    return float(auc / denom) if denom > 0 else np.nan\n",
        "\n",
        "def method_label(meta):\n",
        "    # Preferred: use meta flags so it works even if filenames change\n",
        "    if bool(meta.get(\"use_qos_weights\", False)):\n",
        "        return \"QoS\"\n",
        "    mode = meta.get(\"agg_mode\", \"\")\n",
        "    if mode == \"linear_weighted\":\n",
        "        return \"Linear\"\n",
        "    if mode == \"circular_weighted\":\n",
        "        return \"Circular\"\n",
        "    if mode == \"auto_pick\":\n",
        "        return f\"AutoPick:{meta.get('autopick_policy','?')}\"\n",
        "    return str(mode)\n",
        "\n",
        "# ---- load all runs ----\n",
        "runs = []\n",
        "for meta_path in BASE.rglob(\"run_metadata.json\"):\n",
        "    outdir = meta_path.parent\n",
        "    meta = json.loads(meta_path.read_text())\n",
        "\n",
        "    # find the round-metrics csv saved by rounds_csv_path(...)\n",
        "    csvs = sorted(outdir.glob(\"round_metrics_*_clients*_rounds*.csv\"))\n",
        "    if not csvs:\n",
        "        continue\n",
        "    df = pd.read_csv(csvs[0]).sort_values(\"round\")\n",
        "    df = df[df[\"round\"] >= 1].reset_index(drop=True)\n",
        "\n",
        "    runs.append((meta, df))\n",
        "\n",
        "if not runs:\n",
        "    raise RuntimeError(f\"No runs found under: {BASE}\")\n",
        "\n",
        "# ---- compute pooled safe threshold per (dataset, partition, alpha, noise, stress) ----\n",
        "pool_rows = []\n",
        "for meta, df in runs:\n",
        "    if RISK in df.columns:\n",
        "        pool_rows.append({\n",
        "            \"dataset\": meta.get(\"dataset\"),\n",
        "            \"partition\": meta.get(\"partition\"),\n",
        "            \"alpha\": meta.get(\"dirichlet_alpha\", None),\n",
        "            \"noise\": meta.get(\"local_init_noise\", None),\n",
        "            \"stress\": meta.get(\"angle_stress\", None),\n",
        "            \"risk\": df[RISK].to_numpy(float)\n",
        "        })\n",
        "\n",
        "pool_expanded = []\n",
        "for r in pool_rows:\n",
        "    for v in r[\"risk\"]:\n",
        "        if np.isfinite(v):\n",
        "            pool_expanded.append({k: r[k] for k in [\"dataset\",\"partition\",\"alpha\",\"noise\",\"stress\"]} | {\"risk\": v})\n",
        "pool_expanded = pd.DataFrame(pool_expanded)\n",
        "\n",
        "thr_map = (\n",
        "    pool_expanded\n",
        "    .groupby([\"dataset\",\"partition\",\"alpha\",\"noise\",\"stress\"])[\"risk\"]\n",
        "    .quantile(SAFE_Q)\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# ---- per-seed summaries ----\n",
        "rows = []\n",
        "for meta, df in runs:\n",
        "    dataset  = meta.get(\"dataset\")\n",
        "    part     = meta.get(\"partition\")\n",
        "    alpha    = meta.get(\"dirichlet_alpha\", None)\n",
        "    noise    = meta.get(\"local_init_noise\", None)\n",
        "    stress   = meta.get(\"angle_stress\", None)\n",
        "    seed     = meta.get(\"seed\")\n",
        "    method   = method_label(meta)\n",
        "\n",
        "    key = (dataset, part, alpha, noise, stress)\n",
        "    thr = float(thr_map.get(key, np.nan))\n",
        "\n",
        "    row = {\n",
        "        \"dataset\": dataset,\n",
        "        \"partition\": part,\n",
        "        \"alpha\": alpha,\n",
        "        \"noise\": noise,\n",
        "        \"stress\": stress,\n",
        "        \"seed\": seed,\n",
        "        \"method\": method,\n",
        "        \"rounds\": len(df),\n",
        "        \"risk_thr_q\": thr,\n",
        "    }\n",
        "\n",
        "    if ACC in df.columns:\n",
        "        row[\"val_best\"] = float(df[ACC].max())\n",
        "        row[\"val_tail\"] = float(df[ACC].tail(TAIL_K).mean())\n",
        "        row[\"val_auc_norm\"] = auc_norm(df, ACC)\n",
        "\n",
        "    if TEST in df.columns:\n",
        "        row[\"test_best\"] = float(df[TEST].max())\n",
        "        row[\"test_tail\"] = float(df[TEST].tail(TAIL_K).mean())\n",
        "        row[\"test_auc_norm\"] = auc_norm(df, TEST)\n",
        "\n",
        "    if RISK in df.columns:\n",
        "        row[\"risk_mean\"] = float(df[RISK].mean())\n",
        "        row[\"risk_max\"]  = float(df[RISK].max())\n",
        "        row[\"risk_auc_norm\"] = auc_norm(df, RISK)\n",
        "        row[f\"safe_frac_q{SAFE_Q}\"] = float((df[RISK] < thr).mean()) if np.isfinite(thr) else np.nan\n",
        "\n",
        "    if GEOM in df.columns:\n",
        "        row[\"geom_mean\"] = float(df[GEOM].mean())\n",
        "        row[\"geom_max\"]  = float(df[GEOM].max())\n",
        "        row[\"geom_auc_norm\"] = auc_norm(df, GEOM)\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "per_seed = pd.DataFrame(rows)\n",
        "\n",
        "# ---- aggregate across seeds per scenario + method ----\n",
        "def mean_std_worst(g, col):\n",
        "    return pd.Series({\n",
        "        f\"{col}_mean\": g[col].mean(),\n",
        "        f\"{col}_std\":  g[col].std(ddof=1),\n",
        "        f\"{col}_worst\": g[col].min(),   # worst-case robustness (min across seeds)\n",
        "    })\n",
        "\n",
        "agg_cols = [\"val_best\",\"val_tail\",\"test_best\",\"test_tail\",\"risk_mean\",\"geom_mean\",f\"safe_frac_q{SAFE_Q}\"]\n",
        "\n",
        "out = []\n",
        "group_keys = [\"dataset\",\"partition\",\"alpha\",\"noise\",\"stress\",\"method\"]\n",
        "for keys, g in per_seed.groupby(group_keys):\n",
        "    row = dict(zip(group_keys, keys))\n",
        "    row[\"n_seeds\"] = g[\"seed\"].nunique()\n",
        "    for c in agg_cols:\n",
        "        if c in g.columns:\n",
        "            row.update(mean_std_worst(g, c).to_dict())\n",
        "    out.append(row)\n",
        "\n",
        "summary = pd.DataFrame(out).sort_values([\"dataset\",\"partition\",\"alpha\",\"noise\",\"stress\",\"method\"])\n",
        "\n",
        "display(per_seed)\n",
        "display(summary)\n",
        "\n",
        "per_seed.to_csv(\"multiseed_per_seed.csv\", index=False)\n",
        "summary.to_csv(\"multiseed_summary_QoS.csv\", index=False)\n",
        "\n",
        "print(\"Saved: multiseed_per_seed_circ.csv, multiseed_summary_circ.csv\")\n",
        "print(\"\\nLaTeX:\\n\")\n",
        "print(summary.round(4).to_latex(index=False))\n"
      ],
      "metadata": {
        "id": "pGp1PgkelQHI"
      },
      "id": "pGp1PgkelQHI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SHVjrgyrktAx"
      },
      "id": "SHVjrgyrktAx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5109e4b",
      "metadata": {
        "id": "a5109e4b"
      },
      "outputs": [],
      "source": [
        "# After running the main cell:\n",
        "# - set DATASET_CHOICE etc. in the script configuration section (near the bottom),\n",
        "# - then rerun the last \"EXECUTE CHOSEN\" block, or simply rerun the main cell.\n",
        "#\n",
        "# Sanity check:\n",
        "try:\n",
        "    _ = FLConfig().use_qos_weights\n",
        "    print(\"OK: FLConfig has QoS fields.\")\n",
        "except Exception as e:\n",
        "    print(\"QoS fields missing:\", e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "workshopATS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}