{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanikairoshi/QFL_Experiments/blob/main/QFL_FAA_Beta_ICOIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdPhLlT9q3Np"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Us3DV6Sfq6Mw"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "!pip install genomic-benchmarks\n",
        "!pip install qiskit qiskit_machine_learning qiskit_algorithms\n",
        "!pip install qiskit-aer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teleportation Configuration"
      ],
      "metadata": {
        "id": "V_NL7bRexakG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class TeleportCfg:\n",
        "    use_teleportation: bool = True\n",
        "    noise: str = \"med\"     # \"low\" | \"med\" | \"high\"\n",
        "    shots: int = 256       # samples to estimate fidelity\n",
        "    alpha: float = 1.0     # fidelity emphasis\n",
        "    gamma: float = 0.0     # latency penalty exponent\n",
        "    delta: float = 0.0     # instability penalty exponent\n",
        "    beta: float = 1.0      # 1=circular, 0=linear blend (only used for angle params)\n",
        "    seed: int = 2025\n",
        "\n",
        "_NOISE_P = {\"low\": 0.02, \"med\": 0.06, \"high\": 0.12}\n",
        "\n",
        "class TeleportationLink:\n",
        "    \"\"\"Toy fidelity sampler for a Bell-pair teleportation pipeline.\"\"\"\n",
        "    def __init__(self, cfg: TeleportCfg):\n",
        "        self.cfg = cfg\n",
        "        self.rng = np.random.default_rng(cfg.seed)\n",
        "    def sample_fidelity(self, shots=None, noise=None):\n",
        "        if shots is None: shots = self.cfg.shots\n",
        "        if noise is None: noise = self.cfg.noise\n",
        "        p = _NOISE_P.get(noise, 0.06)\n",
        "        u = self.rng.uniform(0.5, 1.0, size=shots)\n",
        "        flips = self.rng.binomial(1, p, size=shots)\n",
        "        return 1.0 - flips * u  # array in [0,1]\n"
      ],
      "metadata": {
        "id": "RIE29YowxdN8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)\n",
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import DemoHumanOrWorm\n",
        "\n",
        "test_set = DemoHumanOrWorm(split='test', version=0)\n",
        "train_set = DemoHumanOrWorm(split='train', version=0)\n",
        "\n",
        "data_set = train_set\n",
        "# data_set = train_set + test_set\n",
        "len(data_set)\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "word_size = 40\n",
        "word_combinations = defaultdict(int)\n",
        "iteration = 1\n",
        "for text, _ in data_set:\n",
        "    for i in range(len(text)):\n",
        "        word = text[i:i+word_size]\n",
        "        if word_combinations.get(word) is None:\n",
        "          word_combinations[word] = iteration\n",
        "          iteration += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"First sample int the data_set variable: \")\n",
        "print(data_set[0])\n",
        "\n",
        "print(\"\\nFirst 5 samples in the word_combinations dict.\")\n",
        "for key, value in list(word_combinations.items())[:5]:\n",
        "    print(key, value)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# Preprocess the training set\n",
        "np_data_set = []\n",
        "for i in range(len(data_set)):\n",
        "    sequence, label = data_set[i]\n",
        "    sequence = sequence.strip()  # Remove any leading/trailing whitespace\n",
        "    words = [sequence[i:i + word_size] for i in range(0, len(sequence), word_size)]  # Split the sequence into 4-letter words\n",
        "    int_sequence = np.array([word_combinations[word] for word in words])\n",
        "    data_point = {'sequence': int_sequence, 'label': label}\n",
        "    np_data_set.append(data_point)\n",
        "\n",
        "\n",
        "print(\"First 5 samples of encoded data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "rng = np.random.default_rng(42)   # fixed seed for reproducibility\n",
        "rng.shuffle(np_data_set)          # shuffle in place, but reproducible\n",
        "print(\"First 5 samples of encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sequences = np.array([item['sequence'] for item in np_data_set])\n",
        "sequences = np.vstack(sequences)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "sequences_scaled = scaler.fit_transform(sequences)\n",
        "\n",
        "for i, item in enumerate(np_data_set):\n",
        "    item['sequence'] = sequences_scaled[i]\n",
        "\n",
        "print(\"First 5 samples of scaled encoded shuffled data:\")\n",
        "np_data_set[:5]\n",
        "\n",
        "\n",
        "np_train_data = np_data_set[:70000]\n",
        "np_test_data = np_data_set[-5000:]\n",
        "\n",
        "print(f\"Length of np_train_data: {len(np_train_data)}\")\n",
        "print(f\"Length of np_test_data: {len(np_test_data)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np_train_data = np_data_set[:70000]\n",
        "np_test_data = np_data_set[-5000:]\n",
        "\n",
        "print(f\"Length of np_train_data: {len(np_train_data)}\")\n",
        "print(f\"Length of np_test_data: {len(np_test_data)}\")\n",
        "\n",
        "test_sequences = [data_point[\"sequence\"] for data_point in np_test_data]\n",
        "test_labels = [data_point[\"label\"] for data_point in np_test_data]\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_labels = np.array(test_labels)\n",
        "print(f\"Length of Test data: {len(test_sequences)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4S1g9FC5nH",
        "outputId": "c1545d5f-d830-4bd7-abd1-9b449a221630"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First sample int the data_set variable: \n",
            "('TACATCATTGGTTCGGGACGCTGGTGGAAATAGTGAATCCGGCATGTCAGTGAAAGTAAATGAACTTCTTCGATTATACTCGGCAAATGAGAAGTACGGGAATGGAATTATGCTCCTTAACGCCTATAAATCTGTTATTCATAACTTTTCCGGTTTTCCCAAAACCTACCCATTTTTGAGCAAAATTGCCAACGTAGGCA', 0)\n",
            "\n",
            "First 5 samples in the word_combinations dict.\n",
            "TACATCATTGGTTCGGGACGCTGGTGGAAATAGTGAATCC 1\n",
            "ACATCATTGGTTCGGGACGCTGGTGGAAATAGTGAATCCG 2\n",
            "CATCATTGGTTCGGGACGCTGGTGGAAATAGTGAATCCGG 3\n",
            "ATCATTGGTTCGGGACGCTGGTGGAAATAGTGAATCCGGC 4\n",
            "TCATTGGTTCGGGACGCTGGTGGAAATAGTGAATCCGGCA 5\n",
            "First 5 samples of encoded data:\n",
            "First 5 samples of encoded shuffled data:\n",
            "First 5 samples of scaled encoded shuffled data:\n",
            "Length of np_train_data: 70000\n",
            "Length of np_test_data: 5000\n",
            "Length of np_train_data: 70000\n",
            "Length of np_test_data: 5000\n",
            "Length of Test data: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcRZf8-bLOQH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the structure of np_test_data\n",
        "for i, data_point in enumerate(np_test_data[:5]):  # Print the first 5 test data points\n",
        "    print(f\"Test data point {i}: {data_point}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTLOJOQeLRg7",
        "outputId": "38f84102-65b6-4ad3-bf84-9a7ecbcd4b5d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data point 0: {'sequence': array([0.74898835, 0.74898835, 0.74898835, 0.74898835, 0.74898835]), 'label': 1}\n",
            "Test data point 1: {'sequence': array([0.270534, 0.270534, 0.270534, 0.270534, 0.270534]), 'label': 0}\n",
            "Test data point 2: {'sequence': array([0.55422793, 0.55422793, 0.55422793, 0.55422793, 0.55422793]), 'label': 1}\n",
            "Test data point 3: {'sequence': array([0.52057059, 0.52057059, 0.52057059, 0.52057059, 0.52057059]), 'label': 1}\n",
            "Test data point 4: {'sequence': array([0.20668716, 0.20668716, 0.20668716, 0.20668716, 0.20668716]), 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vM9FXroZu1bK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
        "from qiskit.primitives import BackendSampler\n",
        "from functools import partial\n",
        "from qiskit_aer import Aer\n",
        "\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit_algorithms.optimizers import COBYLA\n",
        "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
        "from qiskit.primitives import BackendSampler\n",
        "from functools import partial\n",
        "from qiskit_aer import Aer\n",
        "\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "num_clients = 5\n",
        "num_epochs = 20\n",
        "max_train_iterations = 50\n",
        "samples_per_epoch=100\n",
        "backend = Aer.get_backend('aer_simulator')\n",
        "\n",
        "class Client:\n",
        "   def __init__(self, train_data):  # Add test_data to __init__\n",
        "        self.client_train_data = train_data\n",
        "        #self.test_data = test_data  # Store test_data as an attribute\n",
        "        self.models = []\n",
        "        self.train_scores = []\n",
        "        self.test_scores = []\n",
        "        self.primary_model = None\n",
        "'''\n",
        "def split_dataset(num_clients, num_epochs, samples_per_epoch):\n",
        "  clients = []\n",
        "  for i in range(num_clients):\n",
        "    client_data = []\n",
        "    for j in range(num_epochs):\n",
        "      start_idx = (i*num_epochs*samples_per_epoch)+(j*samples_per_epoch)\n",
        "      end_idx = (i*num_epochs*samples_per_epoch)+((j+1)*samples_per_epoch)\n",
        "      client_data.append(np_train_data[start_idx:end_idx])\n",
        "    # Pass test_data when creating Client instances\n",
        "    clients.append(Client(client_data, np_test_data))\n",
        "  return clients\n",
        "\n",
        "clients = split_dataset(num_clients, num_epochs, samples_per_epoch)\n",
        "'''\n",
        "def split_dataset(num_clients, num_epochs, samples_per_epoch):\n",
        "    clients = []\n",
        "    # Split test data across clients\n",
        "    #test_samples_per_client = len(test_sample_sequences) // num_clients\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_train_data = []\n",
        "        for j in range(num_epochs):\n",
        "            start_idx = (i * num_epochs * samples_per_epoch) + (j * samples_per_epoch)\n",
        "            end_idx = (i * num_epochs * samples_per_epoch) + ((j + 1) * samples_per_epoch)\n",
        "            client_train_data.append(np_train_data[start_idx:end_idx])\n",
        "            #print(f\"Client {i+1} training data size: {len(np_train_data[start_idx:end_idx])}\")\n",
        "        # Assign a subset of the test data to each client\n",
        "        #test_start_idx = i * test_samples_per_client\n",
        "        #test_end_idx = (i + 1) * test_samples_per_client\n",
        "        #client_test_data = test_sample_sequences[test_start_idx:test_end_idx]\n",
        "\n",
        "        # Create Client instance with both train and test data\n",
        "        clients.append(Client(client_train_data))\n",
        "\n",
        "    return clients\n",
        "\n",
        "clients = split_dataset(num_clients, num_epochs, samples_per_epoch)\n",
        "\n",
        "itr = 0\n",
        "def training_callback(weights, obj_func_eval):\n",
        "        global itr\n",
        "        itr += 1\n",
        "        print(f\"{itr}\", end=' | ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_AV8hy8zY_R"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vMGbsnFLzZhB"
      },
      "outputs": [],
      "source": [
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Callback function to visualize training progress\n",
        "objective_func_vals = []\n",
        "def callback_graph(weights, obj_func_eval):\n",
        "    clear_output(wait=True)\n",
        "    objective_func_vals.append(obj_func_eval)\n",
        "    plt.title(\"Objective function value against iteration\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective function value\")\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SE9YYRwPJDau"
      },
      "outputs": [],
      "source": [
        "# Function to initialize a new QNN model (same architecture as clients' models)\n",
        "def initialize_model(num_features):\n",
        "    # Create the same quantum neural network (QNN) architecture as clients\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "\n",
        "    # Combine the feature map and ansatz into a single quantum circuit\n",
        "    qc = feature_map.compose(ansatz)\n",
        "\n",
        "    # Use parity as the interpretation function\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2  # Binary classification\n",
        "\n",
        "    # Explicitly define input_params and weight_params\n",
        "    input_params = feature_map.parameters  # Input parameters (for encoding)\n",
        "    weight_params = ansatz.parameters      # Trainable parameters (for optimization)\n",
        "\n",
        "    # Define the QNN model using a sampler\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,  # Binary classification\n",
        "        input_params=input_params,\n",
        "        weight_params=weight_params\n",
        "    )\n",
        "\n",
        "    # Create a classifier using the neural network\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "        neural_network=sampler_qnn,\n",
        "        optimizer=SPSA(maxiter=50)  # Use SPSA optimizer\n",
        "    )\n",
        "\n",
        "    return qnn_classifier\n",
        "\n",
        "# Function to create a model with averaged weights\n",
        "def create_model_with_weights(average_weights, num_features):\n",
        "    # Initialize a new QNN model with the same architecture\n",
        "    model = initialize_model(num_features)\n",
        "    # Assign the averaged weights to the model's trainable parameters (ansatz weights)\n",
        "    weight_params = model.neural_network.weight_params  # Get the trainable parameters\n",
        "\n",
        "    # Check if the lengths match, and truncate if necessary\n",
        "    num_weights = min(len(average_weights), len(weight_params))\n",
        "\n",
        "    # Create a dictionary mapping parameters to averaged weights\n",
        "    param_dict = {param: average_weights[i] for i, param in enumerate(weight_params[:num_weights])}\n",
        "\n",
        "\n",
        "    # Assign the averaged weights to the circuit parameters\n",
        "    model.neural_network.circuit.assign_parameters(param_dict)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "j-HD-sL2zgxh"
      },
      "outputs": [],
      "source": [
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
        "from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes\n",
        "from qiskit.primitives import BackendSampler\n",
        "from qiskit_algorithms.optimizers import SPSA\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Callback function to visualize training progress\n",
        "objective_func_vals = []\n",
        "def callback_graph(weights, obj_func_eval):\n",
        "    clear_output(wait=True)\n",
        "    objective_func_vals.append(obj_func_eval)\n",
        "    plt.title(\"Objective function value against iteration\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective function value\")\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
        "    plt.show()\n",
        "\n",
        "# Function to create the QNN model\n",
        "def create_qnn_model(num_features):\n",
        "    # num_features = data_train[0][\"sequence\"].shape[0]  # Remove this line as data_train is not defined\n",
        "    # Define the quantum feature map and ansatz\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)  # Use num_features passed as argument\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "\n",
        "    # Construct the quantum neural network using a sampler\n",
        "    qc = feature_map.compose(ansatz)  # Build the QNN circuit\n",
        "    print(f\"Number of features (input dimension): {num_features}\")\n",
        "    print(f\"Number of circuit parameters: {qc.num_parameters}\")\n",
        "\n",
        "    # Use parity as the interpretation function\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2  # Binary classification\n",
        "\n",
        "    # Explicitly define input_params and weight_params\n",
        "    input_params = feature_map.parameters  # Parameters for the feature map (inputs)\n",
        "    weight_params = ansatz.parameters      # Parameters for the ansatz (weights)\n",
        "    #print(f\"Input Parameters: {input_params}\")\n",
        "    #print(f\"Weight Parameters: {weight_params}\")\n",
        "\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,  # Output dimension for binary classification\n",
        "        input_params=input_params,       # Pass input parameters\n",
        "        weight_params=weight_params     # Pass weight parameters\n",
        "    )\n",
        "\n",
        "    # Define a classifier using the QNN\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "        neural_network=sampler_qnn,\n",
        "        optimizer=SPSA(maxiter=50),  # Example with SPSA optimizer\n",
        "        #callback=callback_graph\n",
        "    )\n",
        "\n",
        "    return qnn_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IkECBNFLazNA"
      },
      "outputs": [],
      "source": [
        "def getAccuracy(weights, num_features, test_sequences, test_labels):\n",
        "    # Rebuild the QNN model with the given weights\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "\n",
        "    # Replace bind_parameters with assign_parameters\n",
        "    # Create a parameter dictionary for assignment\n",
        "    param_dict = {param: weight for param, weight in zip(ansatz.parameters, weights)}\n",
        "    ansatz = ansatz.assign_parameters(param_dict)\n",
        "\n",
        "    # Rebuild the QNN using the updated ansatz\n",
        "    qc = feature_map.compose(ansatz)\n",
        "\n",
        "    # Define the parity function for binary classification\n",
        "    def parity(x):\n",
        "        return \"{:b}\".format(x).count(\"1\") % 2\n",
        "\n",
        "    # Build the SamplerQNN with the updated circuit\n",
        "    sampler_qnn = SamplerQNN(\n",
        "        circuit=qc,\n",
        "        interpret=parity,\n",
        "        output_shape=2,  # Binary classification\n",
        "        input_params=feature_map.parameters,  # Input parameters (from the feature map)\n",
        "        weight_params=ansatz.parameters  # Weight parameters (from the ansatz)\n",
        "    )\n",
        "\n",
        "    # Build the NeuralNetworkClassifier with the QNN\n",
        "    qnn_classifier = NeuralNetworkClassifier(\n",
        "        neural_network=sampler_qnn,\n",
        "        optimizer=COBYLA(maxiter=0)  # No need for further optimization\n",
        "    )\n",
        "\n",
        "    # Train the classifier on a subset of test data (or use full test data if preferred)\n",
        "    qnn_classifier.fit(test_sequences, test_labels)\n",
        "\n",
        "    # Return the accuracy on a larger test set\n",
        "    return qnn_classifier.score(test_sequences, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CSV_PATH = '/content/drive/MyDrive/ICOIN_2025/federated_qnn_metrics_genome.csv'  # change if you like\n",
        "os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "def init_metrics_csv(csv_path: str, num_clients: int):\n",
        "    \"\"\"Create/overwrite CSV with a header.\"\"\"\n",
        "    header = ['Epoch', 'GlobalAccuracy']\n",
        "    for i in range(num_clients):\n",
        "        header += [f'Client{i}_TrainAcc', f'Client{i}_TestAcc']\n",
        "    with open(csv_path, 'w', newline='') as f:\n",
        "        csv.writer(f).writerow(header)\n",
        "\n",
        "def append_metrics_row(csv_path: str, epoch: int, global_acc: float,\n",
        "                       train_accs: list[float], test_accs: list[float]):\n",
        "    \"\"\"Append one epoch’s metrics.\"\"\"\n",
        "    row = [epoch, global_acc] + [\n",
        "        x for pair in zip(train_accs, test_accs) for x in pair\n",
        "    ]\n",
        "    with open(csv_path, 'a', newline='') as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "def compute_global_accuracy(model, clients: list[dict]) -> float:\n",
        "    \"\"\"Evaluate the current global model on the union of all client test sets.\"\"\"\n",
        "    X = np.concatenate([c[\"test_data\"][\"sequence\"] for c in clients], axis=0)\n",
        "    y = np.concatenate([c[\"test_data\"][\"label\"]   for c in clients],   axis=0)\n",
        "    return float(model.score(X, y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTktmHkiuWhn",
        "outputId": "7e7d6f9d-4449-4943-da55-2c837d92928e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trust-weighted angle aggregation (drop-in function)"
      ],
      "metadata": {
        "id": "4cOu5jwzzvk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def agg_faaa_beta_with_qos(\n",
        "    local_ws,             # List[np.ndarray], per-client parameter vectors\n",
        "    shard_sizes,          # List[int], data counts per client (|D_n|)\n",
        "    fidelities,           # np.ndarray shape (K,)\n",
        "    latencies,            # np.ndarray shape (K,)\n",
        "    instabilities,        # np.ndarray shape (K,)\n",
        "    tele_cfg: TeleportCfg,\n",
        "    angle_mask=None,      # optional boolean mask for angular params\n",
        "):\n",
        "    # base client weights ~ data size\n",
        "    Wn = np.asarray(shard_sizes, float)\n",
        "    Wn = Wn / (Wn.sum() + 1e-12)\n",
        "\n",
        "    F = np.clip(np.asarray(fidelities, float), 0.0, 1.0)\n",
        "    T = np.maximum(np.asarray(latencies, float), 1e-6)\n",
        "    V = np.maximum(np.asarray(instabilities, float), 0.0)\n",
        "\n",
        "    trust = (F ** tele_cfg.alpha) / ((T ** tele_cfg.gamma) * ((V + 1e-8) ** tele_cfg.delta))\n",
        "    W = trust * Wn\n",
        "    W = W / (W.sum() + 1e-12)\n",
        "\n",
        "    # Temporarily rescale shard_sizes by trust for your existing aggregator\n",
        "    shard_sizes_eff = (W * 1000).astype(int).tolist()  # only ratios matter\n",
        "\n",
        "    # Use your angle-aware aggregator (‘auto’ chooses circular vs linear)\n",
        "    avg_w, diag = aggregate_weights_angle_aware(\n",
        "        local_ws, shard_sizes=shard_sizes_eff, mode=\"auto\", angle_mask=angle_mask\n",
        "    )\n",
        "\n",
        "    # Optional β-blend: if you want to force a circular/linear blend explicitly,\n",
        "    # set tele_cfg.beta in aggregate_weights_angle_aware (not needed if mode='auto').\n",
        "    diag[\"trust_weights_mean\"] = float(W.mean())\n",
        "    return avg_w, diag\n",
        "'''\n",
        "def agg_faaa_beta_with_qos(\n",
        "    local_ws, shard_sizes, fidelities, latencies, instabilities, tele_cfg, angle_mask=None\n",
        "):\n",
        "    Wn = np.asarray(shard_sizes, float); Wn /= (Wn.sum() + 1e-12)\n",
        "    F = np.clip(np.asarray(fidelities, float), 0.0, 1.0)\n",
        "    T = np.maximum(np.asarray(latencies, float), 1e-6)\n",
        "    V = np.maximum(np.asarray(instabilities, float), 0.0)\n",
        "\n",
        "    trust = (F ** tele_cfg.alpha) / ((T ** tele_cfg.gamma) * ((V + 1e-8) ** tele_cfg.delta))\n",
        "    W = trust * Wn\n",
        "    W = W / (W.sum() + 1e-12)  # normalized trust weights actually applied\n",
        "\n",
        "    # use W as effective shard sizes for angle-aware aggregation\n",
        "    shard_sizes_eff = (W * 1000).astype(int).tolist()\n",
        "    avg_w, diag = aggregate_weights_angle_aware(local_ws, shard_sizes=shard_sizes_eff,\n",
        "                                                mode=\"auto\", angle_mask=angle_mask)\n",
        "    diag[\"used_mode\"] = diag.get(\"used_mode\", \"auto\")\n",
        "    return avg_w, diag, W  # <-- return W\n",
        "\n",
        "\n",
        "tele_cfg = TeleportCfg(use_teleportation=True, noise=\"med\", shots=256,\n",
        "                       alpha=1.0, gamma=0.0, delta=0.0, beta=1.0, seed=2025)\n",
        "tele_link = TeleportationLink(tele_cfg)\n",
        "\n",
        "def measure_instability(weights_vec: np.ndarray) -> float:\n",
        "    return float(np.var(weights_vec))\n",
        "\n",
        "def extract_client_weights(model) -> np.ndarray:\n",
        "    # Works for NeuralNetworkClassifier(SamplerQNN) and VQC: get the *trainable* weights\n",
        "    if hasattr(model, \"weights\") and model.weights is not None:\n",
        "        return np.asarray(model.weights, float)\n",
        "    # Fallback: try ansatz parameters (bound values) if available\n",
        "    params = model.neural_network.weight_params if hasattr(model, \"neural_network\") else []\n",
        "    # If you can’t read values directly, keep the model’s own .weights path\n",
        "    return np.asarray([float(p._symbol_expr) if hasattr(p, \"_symbol_expr\") else 0.0 for p in params], float)\n"
      ],
      "metadata": {
        "id": "DIjK9bwRztJX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "def append_round_telemetry(path, epoch, global_acc, fidelities, latencies, instabilities, weights, diag):\n",
        "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    header = [\"epoch\",\"global_acc\",\"fid_mean\",\"fid_std\",\"lat_mean\",\"lat_p90\",\"instab_mean\",\n",
        "              \"weight_entropy\",\"used_mode\",\"R_mean\",\"straddle_frac\",\"sse_geo_gap\"]\n",
        "    write_header = not path.exists()\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        w = csv.writer(f)\n",
        "        if write_header: w.writerow(header)\n",
        "        # simple weight entropy (higher ⇒ more uniform)\n",
        "        eps = 1e-12\n",
        "        ent = float(-np.sum(weights * np.log(weights + eps)))\n",
        "        row = [\n",
        "            int(epoch), float(global_acc),\n",
        "            float(np.mean(fidelities)), float(np.std(fidelities)),\n",
        "            float(np.mean(latencies)), float(np.percentile(latencies,90)),\n",
        "            float(np.mean(instabilities)),\n",
        "            ent,\n",
        "            str(diag.get(\"used_mode\",\"auto\")), float(diag[\"R_mean\"]), float(diag[\"straddle_frac\"]),\n",
        "            float(diag[\"sse_geo_gap\"]),\n",
        "        ]\n",
        "        w.writerow(row)\n",
        "\n",
        "def append_client_telemetry(path, epoch, per_client_rows):\n",
        "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    header = [\"epoch\",\"client_id\",\"shard_size\",\n",
        "              \"train_acc\",\"test_acc\",\"train_loss\",\"test_loss\",\n",
        "              \"latency_sec\",\"fidelity_mean\",\"instability_var\",\"trust_weight\"]\n",
        "    write_header = not path.exists()\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        w = csv.writer(f)\n",
        "        if write_header: w.writerow(header)\n",
        "        for r in per_client_rows:\n",
        "            w.writerow([\n",
        "                int(epoch), int(r[\"client_id\"]), int(r[\"shard_size\"]),\n",
        "                float(r[\"train_acc_local\"]), float(r[\"test_acc_local\"]),\n",
        "                float(r[\"train_loss_local\"]), float(r[\"test_loss_local\"]),\n",
        "                float(r[\"time_sec_local\"]), float(r[\"fidelity_mean\"]),\n",
        "                float(r[\"instability_var\"]), float(r[\"trust_weight\"]),\n",
        "            ])\n"
      ],
      "metadata": {
        "id": "etoCNTsOatOb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Optional, Tuple, Dict\n",
        "\n",
        "# ---------- Angle utilities ----------\n",
        "def _wrap_to_pi(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Map real numbers to (-π, π].\"\"\"\n",
        "    return (x + np.pi) % (2*np.pi) - np.pi\n",
        "\n",
        "def _unwrap_to_ref(A: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Unwrap rows to be closest to the first row (reference).\n",
        "    A: (K, D) angles in (-π, π].\n",
        "    \"\"\"\n",
        "    K, D = A.shape\n",
        "    out = A.copy()\n",
        "    ref = out[0]\n",
        "    for i in range(1, K):\n",
        "        delta = out[i] - ref\n",
        "        out[i] = ref + ((delta + np.pi) % (2*np.pi) - np.pi)\n",
        "    return out\n",
        "\n",
        "def _weighted_circular_mean(A: np.ndarray, W: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Coordinate-wise weighted circular mean.\n",
        "    A: (K, D) in (-π, π]; W: (K,), sum=1.\n",
        "    \"\"\"\n",
        "    C = np.sum(W[:, None] * np.cos(A), axis=0)\n",
        "    S = np.sum(W[:, None] * np.sin(A), axis=0)\n",
        "    return np.arctan2(S, C)\n",
        "\n",
        "def _geodesic_sse(angles: np.ndarray, center: np.ndarray, W: np.ndarray) -> float:\n",
        "    \"\"\"Weighted sum of squared geodesic distances (torus).\"\"\"\n",
        "    diff = _wrap_to_pi(angles - center)    # (K, D)\n",
        "    sse_per_client = np.sum(diff**2, axis=1)\n",
        "    return float(np.sum(W * sse_per_client))\n",
        "\n",
        "def _min_covering_arc_length(angles_1d: np.ndarray) -> float:\n",
        "    \"\"\"Minimal arc length covering all angles in [0, 2π] metric.\"\"\"\n",
        "    a = np.sort(angles_1d)\n",
        "    gaps = np.diff(a, append=a[0] + 2*np.pi)\n",
        "    return float(2*np.pi - np.max(gaps))\n",
        "\n",
        "def angle_diagnostics(local_ws: List[np.ndarray], shard_sizes: List[int]) -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Interpret local_ws as angles in (-π, π]; return summary stats for auto-pick.\n",
        "    \"\"\"\n",
        "    A = np.stack(local_ws, axis=0)                 # (K, D)\n",
        "    W = np.asarray(shard_sizes, float)\n",
        "    W /= W.sum()\n",
        "\n",
        "    C = np.sum(W[:, None] * np.cos(A), axis=0)\n",
        "    S = np.sum(W[:, None] * np.sin(A), axis=0)\n",
        "    R = np.sqrt(C**2 + S**2)\n",
        "    R_mean = float(np.mean(R))\n",
        "    R_min  = float(np.min(R))\n",
        "\n",
        "    mu_circ = np.arctan2(S, C)\n",
        "    A_unwrap = _unwrap_to_ref(A)\n",
        "    mu_lin_unwrapped = np.sum(W[:, None] * A_unwrap, axis=0)\n",
        "    mu_lin = _wrap_to_pi(mu_lin_unwrapped)\n",
        "\n",
        "    sse_circ = _geodesic_sse(A, mu_circ, W)\n",
        "    sse_lin  = _geodesic_sse(A, mu_lin,  W)\n",
        "    sse_gap  = float(sse_lin - sse_circ)  # > 0 ⇒ circular better intrinsically\n",
        "\n",
        "    cover = np.array([_min_covering_arc_length(A[:, j]) for j in range(A.shape[1])])\n",
        "    straddle_frac = float(1.0 - np.mean(cover <= np.pi))\n",
        "\n",
        "    return {\n",
        "        \"R_mean\": R_mean,\n",
        "        \"R_min\": R_min,\n",
        "        \"straddle_frac\": straddle_frac,\n",
        "        \"sse_geo_gap\": sse_gap,\n",
        "        \"mu_circ\": mu_circ,\n",
        "        \"mu_lin\":  mu_lin,\n",
        "    }\n",
        "\n",
        "# ---------- Aggregators ----------\n",
        "def fedavg_linear(local_ws: List[np.ndarray], shard_sizes: List[int]) -> np.ndarray:\n",
        "    W = np.asarray(shard_sizes, float); W /= W.sum()\n",
        "    A = np.stack(local_ws, axis=0)                 # (K, D)\n",
        "    return np.sum(W[:, None] * A, axis=0)\n",
        "\n",
        "def fedavg_circular(local_ws: List[np.ndarray], shard_sizes: List[int]) -> np.ndarray:\n",
        "    W = np.asarray(shard_sizes, float); W /= W.sum()\n",
        "    A = np.stack(local_ws, axis=0)                 # (K, D) angles\n",
        "    return _weighted_circular_mean(A, W)\n",
        "\n",
        "def _autopick_mode(diag: Dict[str, object],\n",
        "                   r_mean_thresh: float = 0.85,\n",
        "                   straddle_thresh: float = 0.05) -> str:\n",
        "    \"\"\"\n",
        "    Rule: prefer circular if geodesic SSE gap > 0, or straddling occurs, or concentration is low.\n",
        "    \"\"\"\n",
        "    if (diag[\"sse_geo_gap\"] > 0.0) or (diag[\"straddle_frac\"] > straddle_thresh) or (diag[\"R_mean\"] < r_mean_thresh):\n",
        "        return \"circular\"\n",
        "    return \"linear\"\n",
        "\n",
        "def aggregate_weights_angle_aware(\n",
        "    local_ws: List[np.ndarray],\n",
        "    shard_sizes: List[int],\n",
        "    mode: str = \"auto\",\n",
        "    angle_mask: Optional[np.ndarray] = None\n",
        ") -> Tuple[np.ndarray, Dict[str, object]]:\n",
        "    \"\"\"\n",
        "    Angle-aware aggregation on a mixed parameter vector.\n",
        "    - local_ws: list of client vectors (length D)\n",
        "    - shard_sizes: client weights (e.g., data counts)\n",
        "    - mode ∈ {'linear','circular','auto'}\n",
        "    - angle_mask: boolean mask of length D; True ⇒ treat as angle in (-π, π]\n",
        "    Returns: (avg_weights, diagnostics)\n",
        "    \"\"\"\n",
        "    A = np.stack(local_ws, axis=0)                 # (K, D)\n",
        "    D = A.shape[1]\n",
        "    W = np.asarray(shard_sizes, float); W /= W.sum()\n",
        "\n",
        "    if angle_mask is None:\n",
        "        # If you only have circuit angles in 'weights', set all True.\n",
        "        angle_mask = np.ones(D, dtype=bool)\n",
        "\n",
        "    ang_idx = angle_mask\n",
        "    non_idx = ~angle_mask\n",
        "\n",
        "    diag = {\"used_mode\": None}\n",
        "    if np.any(ang_idx):\n",
        "        diag_angles = angle_diagnostics([w[ang_idx] for w in local_ws], shard_sizes)\n",
        "    else:\n",
        "        # No angular coordinates ⇒ reduce to linear FedAvg\n",
        "        return fedavg_linear(local_ws, shard_sizes), {\"used_mode\": \"linear\"}\n",
        "\n",
        "    pick = mode if mode != \"auto\" else _autopick_mode(diag_angles)\n",
        "\n",
        "    out = np.zeros(D, dtype=float)\n",
        "    if pick == \"linear\":\n",
        "        out[:] = fedavg_linear(local_ws, shard_sizes)\n",
        "    elif pick == \"circular\":\n",
        "        circ = fedavg_circular([w[ang_idx] for w in local_ws], shard_sizes)\n",
        "        out[ang_idx] = _wrap_to_pi(circ)\n",
        "        if np.any(non_idx):\n",
        "            out[non_idx] = fedavg_linear([w[non_idx] for w in local_ws], shard_sizes)\n",
        "    else:\n",
        "        raise ValueError(\"mode must be one of {'linear','circular','auto'}\")\n",
        "\n",
        "    diag.update(diag_angles)\n",
        "    diag[\"used_mode\"] = pick\n",
        "    return out, diag\n"
      ],
      "metadata": {
        "id": "1m114jkZ7t9Y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ucS5zEozmN5",
        "outputId": "00fd116e-3d58-47ea-ec06-c9e377124c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Training Client 0\n",
            "Number of features (input dimension): 5\n",
            "Number of circuit parameters: 25\n",
            "Training started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3714545703.py:44: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
            "  sampler_qnn = SamplerQNN(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 134.59790992736816 seconds.\n",
            "Client 0 Train Score: 0.87\n",
            "Client 0 Test Score: 0.8248\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Number of features (input dimension): 5\n",
            "Number of circuit parameters: 25\n",
            "Training started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3714545703.py:44: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
            "  sampler_qnn = SamplerQNN(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 132.02119040489197 seconds.\n",
            "Client 1 Train Score: 0.83\n",
            "Client 1 Test Score: 0.7928\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Number of features (input dimension): 5\n",
            "Number of circuit parameters: 25\n",
            "Training started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3714545703.py:44: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
            "  sampler_qnn = SamplerQNN(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 133.75613260269165 seconds.\n",
            "Client 2 Train Score: 0.87\n",
            "Client 2 Test Score: 0.8168\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Number of features (input dimension): 5\n",
            "Number of circuit parameters: 25\n",
            "Training started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3714545703.py:44: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
            "  sampler_qnn = SamplerQNN(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 132.90233755111694 seconds.\n",
            "Client 3 Train Score: 0.9\n",
            "Client 3 Test Score: 0.88\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Number of features (input dimension): 5\n",
            "Number of circuit parameters: 25\n",
            "Training started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3714545703.py:44: DeprecationWarning: V1 Primitives are deprecated as of qiskit-machine-learning 0.8.0 and will be removed no sooner than 4 months after the release date. Use V2 primitives for continued compatibility and support.\n",
            "  sampler_qnn = SamplerQNN(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 133.34577918052673 seconds.\n",
            "Client 4 Train Score: 0.99\n",
            "Client 4 Test Score: 0.9576\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.459 | straddle=0.600 | sse_gap=8.0877 | F̄=0.951\n",
            "Global model updated\n",
            "[Epoch 0] global_acc=0.5740\n",
            "----------------------------------------------------------\n",
            "[Epoch 0] global_acc=0.5740 | train=[0.87, 0.83, 0.87, 0.9, 0.99] | test=[0.8248, 0.7928, 0.8168, 0.88, 0.9576]\n",
            "Epoch: 1\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 132.77351927757263 seconds.\n",
            "Client 0 Train Score: 0.78\n",
            "Client 0 Test Score: 0.706\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 132.78806591033936 seconds.\n",
            "Client 1 Train Score: 0.78\n",
            "Client 1 Test Score: 0.7154\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 134.34469318389893 seconds.\n",
            "Client 2 Train Score: 0.77\n",
            "Client 2 Test Score: 0.7538\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 131.87289118766785 seconds.\n",
            "Client 3 Train Score: 0.75\n",
            "Client 3 Test Score: 0.7256\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 127.02525663375854 seconds.\n",
            "Client 4 Train Score: 0.78\n",
            "Client 4 Test Score: 0.7104\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.597 | straddle=0.400 | sse_gap=0.4186 | F̄=0.957\n",
            "Global model updated\n",
            "[Epoch 1] global_acc=0.6380\n",
            "----------------------------------------------------------\n",
            "[Epoch 1] global_acc=0.6380 | train=[0.78, 0.78, 0.77, 0.75, 0.78] | test=[0.706, 0.7154, 0.7538, 0.7256, 0.7104]\n",
            "Epoch: 2\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 126.45176124572754 seconds.\n",
            "Client 0 Train Score: 0.76\n",
            "Client 0 Test Score: 0.6868\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 128.57663989067078 seconds.\n",
            "Client 1 Train Score: 0.76\n",
            "Client 1 Test Score: 0.7354\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 127.73226976394653 seconds.\n",
            "Client 2 Train Score: 0.71\n",
            "Client 2 Test Score: 0.674\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 127.08516216278076 seconds.\n",
            "Client 3 Train Score: 0.87\n",
            "Client 3 Test Score: 0.8786\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 128.9764223098755 seconds.\n",
            "Client 4 Train Score: 0.89\n",
            "Client 4 Test Score: 0.7998\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.556 | straddle=0.450 | sse_gap=4.9952 | F̄=0.947\n",
            "Global model updated\n",
            "[Epoch 2] global_acc=0.5200\n",
            "----------------------------------------------------------\n",
            "[Epoch 2] global_acc=0.5200 | train=[0.76, 0.76, 0.71, 0.87, 0.89] | test=[0.6868, 0.7354, 0.674, 0.8786, 0.7998]\n",
            "Epoch: 3\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 127.88525414466858 seconds.\n",
            "Client 0 Train Score: 0.74\n",
            "Client 0 Test Score: 0.6142\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 126.19912958145142 seconds.\n",
            "Client 1 Train Score: 0.75\n",
            "Client 1 Test Score: 0.6872\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 126.01775431632996 seconds.\n",
            "Client 2 Train Score: 0.9\n",
            "Client 2 Test Score: 0.82\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 129.72666549682617 seconds.\n",
            "Client 3 Train Score: 0.77\n",
            "Client 3 Test Score: 0.6962\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 127.9940824508667 seconds.\n",
            "Client 4 Train Score: 0.94\n",
            "Client 4 Test Score: 0.9252\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.634 | straddle=0.350 | sse_gap=5.4342 | F̄=0.953\n",
            "Global model updated\n",
            "[Epoch 3] global_acc=0.4580\n",
            "----------------------------------------------------------\n",
            "[Epoch 3] global_acc=0.4580 | train=[0.74, 0.75, 0.9, 0.77, 0.94] | test=[0.6142, 0.6872, 0.82, 0.6962, 0.9252]\n",
            "Epoch: 4\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 127.78259325027466 seconds.\n",
            "Client 0 Train Score: 0.88\n",
            "Client 0 Test Score: 0.7974\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 127.62440466880798 seconds.\n",
            "Client 1 Train Score: 0.84\n",
            "Client 1 Test Score: 0.7756\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 128.01465821266174 seconds.\n",
            "Client 2 Train Score: 0.79\n",
            "Client 2 Test Score: 0.7522\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 127.92972254753113 seconds.\n",
            "Client 3 Train Score: 0.83\n",
            "Client 3 Test Score: 0.7858\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 128.68953585624695 seconds.\n",
            "Client 4 Train Score: 0.8\n",
            "Client 4 Test Score: 0.7282\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.545 | straddle=0.450 | sse_gap=2.4586 | F̄=0.956\n",
            "Global model updated\n",
            "[Epoch 4] global_acc=0.5040\n",
            "----------------------------------------------------------\n",
            "[Epoch 4] global_acc=0.5040 | train=[0.88, 0.84, 0.79, 0.83, 0.8] | test=[0.7974, 0.7756, 0.7522, 0.7858, 0.7282]\n",
            "Epoch: 5\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 126.83564972877502 seconds.\n",
            "Client 0 Train Score: 0.87\n",
            "Client 0 Test Score: 0.8592\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 126.03995275497437 seconds.\n",
            "Client 1 Train Score: 0.76\n",
            "Client 1 Test Score: 0.7998\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 127.3072259426117 seconds.\n",
            "Client 2 Train Score: 0.72\n",
            "Client 2 Test Score: 0.6902\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 128.78651475906372 seconds.\n",
            "Client 3 Train Score: 0.91\n",
            "Client 3 Test Score: 0.8864\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 128.2700309753418 seconds.\n",
            "Client 4 Train Score: 0.74\n",
            "Client 4 Test Score: 0.6722\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.484 | straddle=0.650 | sse_gap=3.6808 | F̄=0.961\n",
            "Global model updated\n",
            "[Epoch 5] global_acc=0.2340\n",
            "----------------------------------------------------------\n",
            "[Epoch 5] global_acc=0.2340 | train=[0.87, 0.76, 0.72, 0.91, 0.74] | test=[0.8592, 0.7998, 0.6902, 0.8864, 0.6722]\n",
            "Epoch: 6\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 127.58057880401611 seconds.\n",
            "Client 0 Train Score: 0.83\n",
            "Client 0 Test Score: 0.8372\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 127.71009373664856 seconds.\n",
            "Client 1 Train Score: 0.83\n",
            "Client 1 Test Score: 0.7386\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 125.57849979400635 seconds.\n",
            "Client 2 Train Score: 0.61\n",
            "Client 2 Test Score: 0.593\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 124.76863598823547 seconds.\n",
            "Client 3 Train Score: 0.81\n",
            "Client 3 Test Score: 0.774\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 127.52403497695923 seconds.\n",
            "Client 4 Train Score: 0.84\n",
            "Client 4 Test Score: 0.7464\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.451 | straddle=0.600 | sse_gap=23.2911 | F̄=0.956\n",
            "Global model updated\n",
            "[Epoch 6] global_acc=0.5560\n",
            "----------------------------------------------------------\n",
            "[Epoch 6] global_acc=0.5560 | train=[0.83, 0.83, 0.61, 0.81, 0.84] | test=[0.8372, 0.7386, 0.593, 0.774, 0.7464]\n",
            "Epoch: 7\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 126.37410235404968 seconds.\n",
            "Client 0 Train Score: 0.81\n",
            "Client 0 Test Score: 0.7664\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 125.6535050868988 seconds.\n",
            "Client 1 Train Score: 0.72\n",
            "Client 1 Test Score: 0.7036\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 127.80480098724365 seconds.\n",
            "Client 2 Train Score: 0.81\n",
            "Client 2 Test Score: 0.7398\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 126.95762991905212 seconds.\n",
            "Client 3 Train Score: 0.77\n",
            "Client 3 Test Score: 0.8\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 125.8544225692749 seconds.\n",
            "Client 4 Train Score: 0.78\n",
            "Client 4 Test Score: 0.7494\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.487 | straddle=0.600 | sse_gap=6.6696 | F̄=0.962\n",
            "Global model updated\n",
            "[Epoch 7] global_acc=0.5700\n",
            "----------------------------------------------------------\n",
            "[Epoch 7] global_acc=0.5700 | train=[0.81, 0.72, 0.81, 0.77, 0.78] | test=[0.7664, 0.7036, 0.7398, 0.8, 0.7494]\n",
            "Epoch: 8\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 126.25078296661377 seconds.\n",
            "Client 0 Train Score: 0.87\n",
            "Client 0 Test Score: 0.8472\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 126.18515872955322 seconds.\n",
            "Client 1 Train Score: 0.72\n",
            "Client 1 Test Score: 0.6244\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 127.66939163208008 seconds.\n",
            "Client 2 Train Score: 0.82\n",
            "Client 2 Test Score: 0.7072\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 126.77008819580078 seconds.\n",
            "Client 3 Train Score: 0.83\n",
            "Client 3 Test Score: 0.7864\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 130.36552715301514 seconds.\n",
            "Client 4 Train Score: 0.77\n",
            "Client 4 Test Score: 0.7076\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.382 | straddle=0.650 | sse_gap=5.0279 | F̄=0.956\n",
            "Global model updated\n",
            "[Epoch 8] global_acc=0.5220\n",
            "----------------------------------------------------------\n",
            "[Epoch 8] global_acc=0.5220 | train=[0.87, 0.72, 0.82, 0.83, 0.77] | test=[0.8472, 0.6244, 0.7072, 0.7864, 0.7076]\n",
            "Epoch: 9\n",
            "Training Client 0\n",
            "Training started...\n",
            "Training completed in 129.19919776916504 seconds.\n",
            "Client 0 Train Score: 0.85\n",
            "Client 0 Test Score: 0.7838\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 1\n",
            "Training started...\n",
            "Training completed in 125.90959358215332 seconds.\n",
            "Client 1 Train Score: 0.8\n",
            "Client 1 Test Score: 0.7644\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 2\n",
            "Training started...\n",
            "Training completed in 124.95742297172546 seconds.\n",
            "Client 2 Train Score: 0.84\n",
            "Client 2 Test Score: 0.8596\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 3\n",
            "Training started...\n",
            "Training completed in 125.43837141990662 seconds.\n",
            "Client 3 Train Score: 0.89\n",
            "Client 3 Test Score: 0.7938\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "Training Client 4\n",
            "Training started...\n",
            "Training completed in 131.1409330368042 seconds.\n",
            "Client 4 Train Score: 0.76\n",
            "Client 4 Test Score: 0.7856\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "[AUTO-FAAA] mode=circular | R_mean=0.495 | straddle=0.400 | sse_gap=2.7358 | F̄=0.958\n",
            "Global model updated\n",
            "[Epoch 9] global_acc=0.6420\n",
            "----------------------------------------------------------\n",
            "[Epoch 9] global_acc=0.6420 | train=[0.85, 0.8, 0.84, 0.89, 0.76] | test=[0.7838, 0.7644, 0.8596, 0.7938, 0.7856]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Lists to store accuracies over epochs\n",
        "global_model_weights = {}\n",
        "global_model_accuracy = []\n",
        "clients_train_accuracies = []  # List to store train accuracies per epoch per client\n",
        "clients_test_accuracies = []   # List to store test accuracies per epoch per client\n",
        "\n",
        "# Function to train the QNN model for one client\n",
        "def train_qnn_model(client_data, client_test_data, model=None):\n",
        "    if model is None:\n",
        "        # Create a new QNN model if one doesn't exist\n",
        "        num_features = client_data[0][\"sequence\"].shape[0]\n",
        "        model = create_qnn_model(num_features)  # Pass num_features\n",
        "\n",
        "    # Extract sequences and labels for training\n",
        "    train_sequences = [data_point[\"sequence\"] for data_point in client_data]\n",
        "    train_labels = [data_point[\"label\"] for data_point in client_data]\n",
        "\n",
        "    # Extract sequences and labels for testing\n",
        "    test_sequences = [data_point[\"sequence\"] for data_point in client_test_data]\n",
        "    test_labels = [data_point[\"label\"] for data_point in client_test_data]\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    train_sequences = np.array(train_sequences)\n",
        "    train_labels = np.array(train_labels)\n",
        "    test_sequences = np.array(test_sequences)\n",
        "    test_labels = np.array(test_labels)\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the QNN model\n",
        "    model.fit(train_sequences, train_labels)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Training completed in {elapsed_time} seconds.\")\n",
        "\n",
        "    # Evaluate the model on training and test data\n",
        "    train_score_q = model.score(train_sequences, train_labels)\n",
        "    test_score_q = model.score(test_sequences, test_labels)\n",
        "\n",
        "    return model, train_score_q, test_score_q, elapsed_time\n",
        "\n",
        "# Function to manually average the numerical values of the parameters across clients\n",
        "def manual_average_weights(epoch_weights):\n",
        "    # Initialize a list to store the summed weights (initialize with zeros)\n",
        "    num_weights = len(epoch_weights[0])  # Number of weights in the model\n",
        "    num_clients = len(epoch_weights)  # Number of clients\n",
        "\n",
        "    # Initialize sum of weights to zero (assuming NumPy array or list of weights)\n",
        "    summed_weights = np.zeros(num_weights)\n",
        "\n",
        "    # Sum the weights from all clients\n",
        "    for client_weights in epoch_weights:\n",
        "        summed_weights += np.array(client_weights)\n",
        "\n",
        "    # Compute the average by dividing the summed weights by the number of clients\n",
        "    average_weights = summed_weights / num_clients\n",
        "\n",
        "    return average_weights\n",
        "\n",
        "# Function to save accuracies to CSV\n",
        "def save_accuracies_to_csv(global_accuracies, clients_train_accuracies, clients_test_accuracies, filename='accuracies.csv'):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the header row\n",
        "        header = ['Epoch', 'Global Accuracy']\n",
        "        for i in range(len(clients_train_accuracies[0])):  # Assuming all clients have the same number of records\n",
        "            header.append(f'Client {i} Train Accuracy')\n",
        "            header.append(f'Client {i} Test Accuracy')\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # Write the accuracy data for each epoch\n",
        "        for epoch in range(len(global_accuracies)):\n",
        "            row = [epoch, global_accuracies[epoch]]  # Start with epoch and global accuracy\n",
        "            for client_index in range(len(clients_train_accuracies[epoch])):\n",
        "                row.append(clients_train_accuracies[epoch][client_index])  # Add train accuracy for client\n",
        "                row.append(clients_test_accuracies[epoch][client_index])   # Add test accuracy for client\n",
        "            writer.writerow(row)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _logits_to_labels(y_raw: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Robust conversion from network outputs to class labels.\"\"\"\n",
        "    y_raw = np.asarray(y_raw)\n",
        "    if y_raw.ndim == 1:\n",
        "        # Binary, single logit (e.g., expectation). Threshold at 0.\n",
        "        return (y_raw >= 0).astype(int)\n",
        "    elif y_raw.ndim == 2:\n",
        "        if y_raw.shape[1] == 1:\n",
        "            return (y_raw[:, 0] >= 0).astype(int)\n",
        "        # Multi-class or 2-class with 2 outputs\n",
        "        return np.argmax(y_raw, axis=1)\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected network output shape: {}\".format(y_raw.shape))\n",
        "\n",
        "\n",
        "def compute_global_accuracy_from_weights(prototype_model, avg_weights: np.ndarray, clients: list) -> float:\n",
        "    \"\"\"\n",
        "    Use a *fitted* prototype_model to access its underlying QNN and\n",
        "    forward the averaged parameter vector 'avg_weights' over the\n",
        "    concatenated global test set, returning accuracy.\n",
        "    \"\"\"\n",
        "    # 1) Build the global test pool\n",
        "    Xg = np.concatenate([np.array(item[\"sequence\"]).reshape(1, -1) for c in clients for item in c.client_train_data[0]], axis=0)\n",
        "    yg = np.concatenate([np.array(item[\"label\"]).reshape(1,) for c in clients for item in c.client_train_data[0]], axis=0)\n",
        "\n",
        "\n",
        "    # 2) Extract the underlying QNN (works for NeuralNetworkClassifier built on {Sampler,Estimator}QNN)\n",
        "    qnn = getattr(prototype_model, \"neural_network\", None)\n",
        "    if qnn is None:\n",
        "        # Some versions use a protected attribute\n",
        "        qnn = getattr(prototype_model, \"_neural_network\", None)\n",
        "    if qnn is None:\n",
        "        raise RuntimeError(\"Cannot access underlying QNN from the classifier.\")\n",
        "\n",
        "    # 3) Forward pass with the *averaged* parameter vector (no fit needed)\n",
        "    y_raw = qnn.forward(Xg, np.asarray(avg_weights))\n",
        "\n",
        "    # 4) Convert to labels and compute accuracy\n",
        "    y_pred = _logits_to_labels(y_raw)\n",
        "    return float(np.mean(y_pred == yg))\n",
        "\n",
        "\n",
        "# Federated learning loop\n",
        "num_features = 5\n",
        "num_epochs = 10\n",
        "global_model_weights = {}\n",
        "\n",
        "num_clients = len(clients)\n",
        "init_metrics_csv(CSV_PATH, num_clients)\n",
        "global_model_accuracy = []\n",
        "# Initialize 'primary_model' for each client\n",
        "for client in clients:  # Assuming 'clients' is your list of client dictionaries\n",
        "    client.primary_model = None  # Initialize 'primary_model' to None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_weights = []\n",
        "\n",
        "    epoch_train_accuracies = []  # Store train accuracies for this epoch\n",
        "    epoch_test_accuracies = []   # Store test accuracies for this epoch\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    #===NEW===\n",
        "    # Collect per-client stats for teleportation-aware aggregation\n",
        "    epoch_weights = []\n",
        "    shard_sizes = []\n",
        "    latencies = []\n",
        "    fidelities = []\n",
        "    instabilities = []\n",
        "    #===NEW===\n",
        "    # Train each client\n",
        "    for index, client in enumerate(clients):\n",
        "        t0 = time.time()\n",
        "        print(f\"Training Client {index}\")\n",
        "        # Use dictionary key access instead of dot notation\n",
        "        if client.primary_model is None:\n",
        "            # Pass both training and test data to the training function\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.client_train_data[epoch], np_test_data) # Use client.client_train_data and np_test_data\n",
        "            client.primary_model = model\n",
        "        else:\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.client_train_data[epoch], np_test_data, model=client.primary_model) # Use client.client_train_data and np_test_data\n",
        "\n",
        "        client.models.append(model)\n",
        "        client.train_scores.append(train_score_q)\n",
        "        client.test_scores.append(test_score_q)\n",
        "\n",
        "        w_local = extract_client_weights(model)\n",
        "        epoch_weights.append(w_local)\n",
        "        shard_sizes.append(len(client.client_train_data[epoch]))\n",
        "\n",
        "        # latency (seconds) for this client's local step\n",
        "        latencies.append(max(train_time, 1e-6))\n",
        "\n",
        "        # instability proxy\n",
        "        instabilities.append(measure_instability(w_local))\n",
        "\n",
        "        # teleportation fidelity (mean over samples) — or 1.0 if disabled\n",
        "        if tele_cfg.use_teleportation:\n",
        "            F_mean = float(np.mean(tele_link.sample_fidelity()))\n",
        "        else:\n",
        "            F_mean = 1.0\n",
        "        fidelities.append(F_mean)\n",
        "\n",
        "        print(f\"Client {index} Train Score: {train_score_q}\")\n",
        "        print(f\"Client {index} Test Score: {test_score_q}\")\n",
        "        print(\"\\n\\n\")\n",
        "        print(\"----------------------------------------------------------\")\n",
        "\n",
        "        # Append client accuracies for this epoch\n",
        "        epoch_train_accuracies.append(train_score_q)\n",
        "        epoch_test_accuracies.append(test_score_q)\n",
        "\n",
        "        # Collect model weights (assuming model has a \"weights\" attribute that can be averaged)\n",
        "        # REMOVE THIS DUPLICATE APPEND: epoch_weights.append(model.weights)\n",
        "\n",
        "   # Manually compute the average weights\n",
        "    #average_weights = manual_average_weights(epoch_weights)\n",
        "    # Weighted sizes per client (use your own definition of |D_n| as needed)\n",
        "    shard_sizes = [len(client.client_train_data[epoch]) for client in clients]\n",
        "\n",
        "    # If you have a known angle_mask per parameter, pass it here; else None ⇒ all angular.\n",
        "    #average_weights, diag = aggregate_weights_angle_aware(\n",
        "        #epoch_weights,\n",
        "        #shard_sizes=shard_sizes,\n",
        "        #mode=\"auto\",           # 'linear' | 'circular' | 'auto'\n",
        "        #angle_mask=None        # or a boolean array of length len(epoch_weights[0])\n",
        "    #)\n",
        "    # === Teleportation-aware, angle-aware aggregation ===\n",
        "    average_weights, diag,weights = agg_faaa_beta_with_qos(\n",
        "        local_ws=epoch_weights,\n",
        "        shard_sizes=shard_sizes,\n",
        "        fidelities=np.asarray(fidelities),\n",
        "        latencies=np.asarray(latencies),\n",
        "        instabilities=np.asarray(instabilities),\n",
        "        tele_cfg=tele_cfg,\n",
        "        angle_mask=None  # set to boolean mask if only some coords are angles\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"[AUTO-FAAA] mode={diag['used_mode']} | R_mean={diag['R_mean']:.3f} | \"\n",
        "          f\"straddle={diag['straddle_frac']:.3f} | sse_gap={diag['sse_geo_gap']:.4f} | \"\n",
        "          f\"F̄={np.mean(fidelities):.3f}\")\n",
        "\n",
        "\n",
        "        # Update the global model weights\n",
        "    print(\"Global model updated\")\n",
        "    # Push global weights back to all clients for warm-start next epoch\n",
        "    global_model_weights[epoch] = average_weights\n",
        "\n",
        "    for c in clients:\n",
        "        if hasattr(c.primary_model, \"initial_point\"):\n",
        "            c.primary_model.initial_point = np.asarray(average_weights)\n",
        "        # Some learners also let you set .weights directly for warm-start\n",
        "        # REMOVE THIS LINE: if hasattr(c.primary_model, \"weights\"):\n",
        "            # REMOVE THIS LINE: c.primary_model.weights = np.asarray(average_weights)\n",
        "\n",
        "    # --- insert this instead ---\n",
        "    #avg_w = global_model_weights[epoch]\n",
        "    # Evaluate “global” accuracy using your existing helper\n",
        "    prototype = clients[0].primary_model\n",
        "    global_acc = compute_global_accuracy_from_weights(prototype, average_weights, clients)\n",
        "    global_model_accuracy.append(global_acc)\n",
        "\n",
        "    # ---- save round-level telemetry\n",
        "    append_round_telemetry(\n",
        "        path=CSV_PATH.replace(\".csv\", \"ICOIN_2025/Client/_round_telemetry.csv\"),\n",
        "        epoch=epoch,\n",
        "        global_acc=global_acc,\n",
        "        fidelities=np.asarray(fidelities),\n",
        "        latencies=np.asarray(latencies),\n",
        "        instabilities=np.asarray(instabilities),\n",
        "        weights=weights,\n",
        "        diag=diag\n",
        "    )\n",
        "\n",
        "    # ---- save client-level telemetry (attach trust weights to per-client rows)\n",
        "    per_client_rows = []\n",
        "    for cid, (tr, te, dt, F, V, sz, wloc) in enumerate(\n",
        "            zip(epoch_train_accuracies, epoch_test_accuracies, latencies, fidelities, instabilities, shard_sizes, epoch_weights)):\n",
        "        per_client_rows.append({\n",
        "            \"client_id\": cid,\n",
        "            \"shard_size\": sz,\n",
        "            \"train_acc_local\": tr,\n",
        "            \"test_acc_local\": te,\n",
        "            \"train_loss_local\": 0.0,   # fill if you have them here, else 0.0\n",
        "            \"test_loss_local\":  0.0,\n",
        "            \"time_sec_local\": dt,\n",
        "            \"fidelity_mean\": F,\n",
        "            \"instability_var\": V,\n",
        "            \"trust_weight\": float(weights[cid]),\n",
        "        })\n",
        "\n",
        "    append_client_telemetry(\n",
        "        path=CSV_PATH.replace(\".csv\", \"ICOIN_2025/CLient/_client_telemetry.csv\"),\n",
        "        epoch=epoch,\n",
        "        per_client_rows=per_client_rows\n",
        "    )\n",
        "\n",
        "\n",
        "    # set starting point for the NEXT epoch’s training\n",
        "    #for c in clients:\n",
        "       # c.next_init = avg_w.copy() # Use dot notation\n",
        "        #c.primary_model.initial_point = np.asarray(avg_w) # Use dot notation\n",
        "\n",
        "\n",
        "    # # Calculate global accuracy using the test data\n",
        "    # test_sequences = clients[0][\"test_data\"][\"sequence\"]  # Access test_sequences\n",
        "    # test_labels = clients[0][\"test_data\"][\"label\"]  # Access test_labels\n",
        "    # global_accuracy = getAccuracy(global_model_weights[epoch], num_features, test_sequences, test_labels)\n",
        "    # global_model_accuracy.append(global_accuracy)\n",
        "\n",
        "    # Save the clients' train/test accuracies for this epoch\n",
        "    clients_train_accuracies.append(epoch_train_accuracies)\n",
        "    clients_test_accuracies.append(epoch_test_accuracies)\n",
        "\n",
        "    # compute global accuracy on the combined test set\n",
        "    # global_acc = compute_global_accuracy(new_model_with_global_weights, clients) # This line was removed in the original cell\n",
        "    # global_model_accuracy.append(global_acc) # This line was removed in the original cell\n",
        "    # Example: append to a separate file\n",
        "    with open(\"angle_diag.csv\", \"a\", newline=\"\") as f:\n",
        "        w = csv.writer(f)\n",
        "        if epoch == 0:\n",
        "            w.writerow([\"epoch\",\"used_mode\",\"R_mean\",\"straddle_frac\",\"sse_geo_gap\"])\n",
        "        w.writerow([epoch, diag[\"used_mode\"], diag[\"R_mean\"], diag[\"straddle_frac\"], diag[\"sse_geo_gap\"]])\n",
        "\n",
        "    # APPEND one row to Drive for this epoch\n",
        "    append_metrics_row(\n",
        "        CSV_PATH, epoch, global_acc,\n",
        "        epoch_train_accuracies, # Use the lists containing accuracies for the current epoch\n",
        "        epoch_test_accuracies   # Use the lists containing accuracies for the current epoch\n",
        "    )\n",
        "    print(f\"[Epoch {epoch}] global_acc={global_acc:.4f}\")\n",
        "    # print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    print(f\"[Epoch {epoch}] global_acc={global_acc:.4f} \"\n",
        "          f\"| train={epoch_train_accuracies} | test={epoch_test_accuracies}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fxtq4-9VQsF"
      },
      "source": [
        "Visualizing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV9T-8JBzwMo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "filename = 'accuracies.csv'\n",
        "data = pd.read_csv(filename)\n",
        "\n",
        "# Extract the relevant columns for plotting\n",
        "epochs = data['Epoch']\n",
        "global_accuracy = data['Global Accuracy']\n",
        "client_train_accuracies = data.filter(like='Train Accuracy').values\n",
        "client_test_accuracies = data.filter(like='Test Accuracy').values\n",
        "\n",
        "# Plot Global Accuracy over Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, global_accuracy, label='Global Accuracy', color='blue', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Global Accuracy')\n",
        "plt.title('Global Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Train Accuracies for all clients over Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(client_train_accuracies.shape[1]):\n",
        "    plt.plot(epochs, client_train_accuracies[:, i], label=f'Client {i} Train Accuracy', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Train Accuracy')\n",
        "plt.title('Train Accuracies for All Clients Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracies for all clients over Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(client_test_accuracies.shape[1]):\n",
        "    plt.plot(epochs, client_test_accuracies[:, i], label=f'Client {i} Test Accuracy', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Test Accuracies for All Clients Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a_q7UqBTcju"
      },
      "source": [
        "new ways to average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd3Im7PfTkl9"
      },
      "outputs": [],
      "source": [
        "global_model_weights = {}\n",
        "global_model_accuracy = []\n",
        "\n",
        "# Federated learning loop\n",
        "for epoch in range(num_epochs):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_weights = []\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    # Train each client\n",
        "    for index, client in enumerate(clients):\n",
        "        print(f\"Training Client {index}\")\n",
        "\n",
        "        if client.primary_model is None:\n",
        "            # First time training: no existing model\n",
        "            train_score_q, test_score_q, model = train(data=client.data[epoch])\n",
        "            client.primary_model = model\n",
        "        else:\n",
        "            # Continue training with the existing model\n",
        "            train_score_q, test_score_q, model = train(data=client.data[epoch], model=client.primary_model)\n",
        "\n",
        "        # Save model and scores\n",
        "        client.models.append(model)\n",
        "        client.train_scores.append(train_score_q)\n",
        "        client.test_scores.append(test_score_q)\n",
        "\n",
        "        # Extract and collect model weights\n",
        "        print(f\"Train Score: {train_score_q}\")\n",
        "        print(f\"Test Score: {test_score_q}\")\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "        # Assuming model.weights returns a NumPy array or list of weights\n",
        "        epoch_weights.append(model.weights)\n",
        "\n",
        "    # Average the weights across all clients\n",
        "    average_weights = sum(epoch_weights) / len(epoch_weights)\n",
        "\n",
        "    # Create a new model with the averaged global weights\n",
        "    global_model_weights[epoch] = average_weights\n",
        "    new_model_with_global_weights = create_model_with_weights(global_model_weights[epoch])\n",
        "\n",
        "    # Update each client's primary model with the global averaged weights\n",
        "    for index, client in enumerate(clients):\n",
        "        client.primary_model = new_model_with_global_weights\n",
        "\n",
        "    # Optionally calculate global accuracy (if applicable in your case)\n",
        "    global_accuracy = getAccuracy(global_model_weights[epoch])  # Assuming getAccuracy() works with global weights\n",
        "    global_model_accuracy.append(global_accuracy)\n",
        "\n",
        "    print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skaTQsAsTbbr"
      },
      "outputs": [],
      "source": [
        "# Function to extract numerical values of parameters\n",
        "def extract_param_values(model):\n",
        "    param_values = []\n",
        "    # Loop through each parameter in the circuit and get its bound value\n",
        "    for param in model.neural_network.circuit.parameters:\n",
        "        # Extract the numerical value (assuming they are already bound with values)\n",
        "        bound_value = model.neural_network.circuit._parameters[param]\n",
        "        param_values.append(bound_value)\n",
        "    return np.array(param_values)\n",
        "\n",
        "# Function to set numerical values of parameters back into the circuit\n",
        "def set_param_values(model, param_values):\n",
        "    # Assign the averaged parameter values back to the circuit\n",
        "    parameter_dict = {param: value for param, value in zip(model.neural_network.circuit.parameters, param_values)}\n",
        "    model.neural_network.circuit.assign_parameters(parameter_dict)\n",
        "\n",
        "# Federated learning loop\n",
        "for epoch in range(num_epochs):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_weights = []\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    # Train each client\n",
        "    for index, client in enumerate(clients):\n",
        "        print(f\"Training Client {index}\")\n",
        "\n",
        "        if client.primary_model is None:\n",
        "            # Pass both training and test data to the training function\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data[epoch])\n",
        "            client.primary_model = model\n",
        "        else:\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data[epoch], model=client.primary_model)\n",
        "\n",
        "        client.models.append(model)\n",
        "        client.train_scores.append(train_score_q)\n",
        "        client.test_scores.append(test_score_q)\n",
        "\n",
        "        print(f\"Client {index} Train Score: {train_score_q}\")\n",
        "        print(f\"Client {index} Test Score: {test_score_q}\")\n",
        "        print(\"  \")\n",
        "        print(\"----------------------------------------------------------\")\n",
        "\n",
        "        # Collect model weights (extract numerical parameter values)\n",
        "        param_values = extract_param_values(client.primary_model)\n",
        "        epoch_weights.append(param_values)\n",
        "\n",
        "    # Average the numerical values of the parameters across clients\n",
        "    average_weights = np.mean(epoch_weights, axis=0)\n",
        "\n",
        "    # Manually update each client's model parameters with the averaged weights\n",
        "    global_model_weights[epoch] = average_weights\n",
        "\n",
        "    for client in clients:\n",
        "        # Manually set the global averaged weights to the model's parameters\n",
        "        set_param_values(client.primary_model, global_model_weights[epoch])\n",
        "\n",
        "    # Calculate global accuracy\n",
        "    global_accuracy = getAccuracy(global_model_weights[epoch])\n",
        "\n",
        "    # Print global accuracy for the epoch\n",
        "    print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8swGngN3RP"
      },
      "source": [
        "33333333333333333333333333333333333333333333"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUX7LWLUIvFu"
      },
      "outputs": [],
      "source": [
        "global_model_weights = {}\n",
        "global_model_accuracy = []\n",
        "\n",
        "# Function to train the QNN model for one client\n",
        "def train_qnn_model(client_data,client_test_data, model=None):\n",
        "    #num_features = client_data[0][\"sequence\"].shape[0]  # Get the feature dimension\n",
        "    # Debug: Print the client data structure\n",
        "    print(\"Client Test Data Structure:\", client_test_data)\n",
        "    if model is None:\n",
        "        # Create a new QNN model if one doesn't exist\n",
        "        model = create_qnn_model(client_data)\n",
        "\n",
        "    # Extract sequences and labels for training\n",
        "    train_sequences = [data_point[\"sequence\"] for data_point in client_data]\n",
        "    train_labels = [data_point[\"label\"] for data_point in client_data]\n",
        "\n",
        "    # Extract sequences and labels for testing\n",
        "    # Handle test data\n",
        "    if isinstance(client_test_data, dict):\n",
        "        # Single data point (dictionary format)\n",
        "        test_sequences = np.array([client_test_data[\"sequence\"]])\n",
        "        test_labels = np.array([client_test_data[\"label\"]])\n",
        "    else:\n",
        "        # List of dictionaries (multiple data points)\n",
        "        test_sequences = [data_point[\"sequence\"] for data_point in client_test_data]\n",
        "        test_labels = [data_point[\"label\"] for data_point in client_test_data]\n",
        "        test_sequences = np.array(test_sequences)\n",
        "        test_labels = np.array(test_labels)\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    train_sequences = np.array(train_sequences)\n",
        "    train_labels = np.array(train_labels)\n",
        "\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the QNN model\n",
        "    model.fit(train_sequences, train_labels)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Training completed in {elapsed_time} seconds.\")\n",
        "\n",
        "    # Evaluate the model on training and test data\n",
        "    train_score_q = model.score(train_sequences, train_labels)\n",
        "    test_score_q = model.score(test_sequences, test_labels)\n",
        "\n",
        "    return model, train_score_q, test_score_q, elapsed_time\n",
        "\n",
        "# Federated learning loop\n",
        "for epoch in range(num_epochs):\n",
        "    global_model_weights[epoch] = []\n",
        "    epoch_weights = []\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "\n",
        "    # Train each client\n",
        "    for index, client in enumerate(clients):\n",
        "        print(f\"Training Client {index}\")\n",
        "\n",
        "        if client.primary_model is None:\n",
        "            # Pass both training and test data to the training function\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data[epoch])\n",
        "            client.primary_model = model\n",
        "        else:\n",
        "            model, train_score_q, test_score_q, train_time = train_qnn_model(client.data[epoch], client.test_data[epoch], model=client.primary_model)\n",
        "\n",
        "        client.models.append(model)\n",
        "        client.train_scores.append(train_score_q)\n",
        "        client.test_scores.append(test_score_q)\n",
        "\n",
        "        print(f\"Client {index} Train Score: {train_score_q}\")\n",
        "        print(f\"Client {index} Test Score: {test_score_q}\")\n",
        "        print(\"  \")\n",
        "        print(\"----------------------------------------------------------\")\n",
        "\n",
        "        for model in client.models:\n",
        "        # Extract numerical values of parameters (as a NumPy array)\n",
        "            param_values = [p.value for p in model.neural_network.circuit.parameters] # Extract the values\n",
        "            epoch_weights.append(param_values)\n",
        "\n",
        "    # Average the numerical values of the parameters across clients\n",
        "    average_weights = np.mean(epoch_weights, axis=0)\n",
        "\n",
        "    # Manually update each client's model parameters with the averaged weights\n",
        "    global_model_weights[epoch] = average_weights\n",
        "\n",
        "    for client in clients:\n",
        "        # Manually set the global averaged weights to the model's parameters\n",
        "        for i, param in enumerate(client.primary_model.neural_network.circuit.parameters):\n",
        "            client.primary_model.neural_network.circuit._parameters[param] = average_weights[i] # Set the value directly\n",
        "\n",
        "\n",
        "    # Calculate global accuracy\n",
        "    global_accuracy = getAccuracy(global_model_weights[epoch])\n",
        "\n",
        "    # Print global accuracy for the epoch\n",
        "    print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy:.2f}\")\n",
        "    print(\"----------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqzOZRiKN2FE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "\n",
        "itr = 0\n",
        "def training_callback(weights, obj_func_eval):\n",
        "        global itr\n",
        "        itr += 1\n",
        "        print(f\"{itr}\", end=' | ')\n",
        "def train(data, model = None):\n",
        "  if model is None:\n",
        "    num_features = len(data[0][\"sequence\"])\n",
        "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "    optimizer = COBYLA(maxiter=max_train_iterations)\n",
        "    vqc_model = VQC(\n",
        "        feature_map=feature_map,\n",
        "        ansatz=ansatz,\n",
        "        optimizer=optimizer,\n",
        "        callback=partial(training_callback),\n",
        "        sampler=BackendSampler(backend=backend),\n",
        "        warm_start=True\n",
        "    )\n",
        "    model = vqc_model\n",
        "\n",
        "  train_sequences = [data_point[\"sequence\"] for data_point in data]\n",
        "  train_labels = [data_point[\"label\"] for data_point in data]\n",
        "\n",
        "  # Convert the lists to NumPy arrays\n",
        "  train_sequences = np.array(train_sequences)\n",
        "  train_labels = np.array(train_labels)\n",
        "\n",
        "  # Print the shapes\n",
        "  print(\"Train Sequences Shape:\", train_sequences.shape)\n",
        "  print(\"Train Labels Shape:\", train_labels.shape)\n",
        "\n",
        "  print(\"Training Started\")\n",
        "  start_time = time.time()\n",
        "  model.fit(train_sequences, train_labels)\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nTraining complete. Time taken: {elapsed_time} seconds.\")\n",
        "\n",
        "  print(f\"SCORING MODEL\")\n",
        "  train_score_q = model.score(train_sequences, train_labels)\n",
        "  test_score_q = model.score(test_sequences[:200], test_labels[:200])\n",
        "  return train_score_q, test_score_q, model\n",
        "\n",
        "def getAccuracy(weights):\n",
        "        num_features = len(test_sequences[0])\n",
        "        feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "        ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "        ansatz = ansatz.bind_parameters(weights)\n",
        "        optimizer = COBYLA(maxiter=0)\n",
        "        vqc = VQC(\n",
        "            feature_map=feature_map,\n",
        "            ansatz=ansatz,\n",
        "            optimizer=optimizer,\n",
        "            sampler=BackendSampler(backend=backend)\n",
        "        )\n",
        "        vqc.fit(test_sequences[:25], test_labels[:25])\n",
        "        return vqc.score(test_sequences[:200], test_labels[:200])\n",
        "\n",
        "def create_model_with_weights(weights):\n",
        "  num_features = len(test_sequences[0])\n",
        "  feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
        "  ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
        "  optimizer = COBYLA(maxiter=max_train_iterations)\n",
        "  vqc = VQC(\n",
        "      feature_map=feature_map,\n",
        "      ansatz=ansatz,\n",
        "      optimizer=optimizer,\n",
        "      sampler=BackendSampler(backend=backend),\n",
        "      warm_start = True,\n",
        "      initial_point  = weights,\n",
        "      callback=partial(training_callback)\n",
        "  )\n",
        "  return vqc\n",
        "\n",
        "\n",
        "global_model_weights = {}\n",
        "global_model_accuracy = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  global_model_weights[epoch] = []\n",
        "  epoch_weights = []\n",
        "  print(f\"epoch: {epoch}\")\n",
        "  for index, client in enumerate(clients):\n",
        "    print(f\"Index: {index}, Client: {client}\")\n",
        "\n",
        "    if client.primary_model is None:\n",
        "      train_score_q, test_score_q, model = train(data = client.data[epoch])\n",
        "      client.models.append(model)\n",
        "      client.test_scores.append(test_score_q)\n",
        "      client.train_scores.append(train_score_q)\n",
        "      # Print the values\n",
        "      print(\"Train Score:\", train_score_q)\n",
        "      print(\"Test Score:\", test_score_q)\n",
        "      print(\"\\n\\n\")\n",
        "      epoch_weights.append(model.weights)\n",
        "\n",
        "    else:\n",
        "      train_score_q, test_score_q, model = train(data = client.data[epoch], model = client.primary_model)\n",
        "      client.models.append(model)\n",
        "      client.test_scores.append(test_score_q)\n",
        "      client.train_scores.append(train_score_q)\n",
        "      print(\"Train Score:\", train_score_q)\n",
        "      print(\"Test Score:\", test_score_q)\n",
        "      print(\"\\n\\n\")\n",
        "      epoch_weights.append(model.weights)\n",
        "\n",
        "\n",
        "if(epoch != 0):\n",
        "    epoch_weights.append(global_model_weights[epoch-1])\n",
        "average_weights = sum(epoch_weights) / len(epoch_weights)\n",
        "\n",
        "global_model_weights[epoch] = average_weights\n",
        "new_model_with_global_weights = create_model_with_weights(global_model_weights[epoch])\n",
        "for index, client in enumerate(clients):\n",
        "  client.primary_model = new_model_with_global_weights\n",
        "\n",
        "global_accuracy = getAccuracy(global_model_weights[epoch])\n",
        "print(f\"Global Model Accuracy In Epoch {epoch}: {global_accuracy}\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "global_model_accuracy.append(global_accuracy)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}